#!/usr/bin/env python3
"""
è¯„åˆ†ç®—æ³•å®ç°éªŒè¯å·¥å…·
éªŒè¯è¯„åˆ†ç®—æ³•çš„å®ç°æ­£ç¡®æ€§å’Œè¾¹ç•Œæ¡ä»¶å¤„ç†
"""

import math
from typing import Dict, List, Any, Tuple

class ScoringAlgorithmValidator:
    """è¯„åˆ†ç®—æ³•éªŒè¯å™¨"""
    
    def __init__(self):
        # æ–‡æ¡£ä¸­å®šä¹‰çš„æƒé‡
        self.documented_weights = {
            'modification_count': 0.3,
            'change_complexity': 0.25,
            'risk_level': 0.35,
            'time_recency': 0.1
        }
        
        # é£é™©ç­‰çº§åˆ†æ•°æ˜ å°„
        self.risk_level_scores = {
            'L1': 1.0,    # L1ä¿®æ”¹å½±å“æœ€å¤§
            'L2': 0.6,    # L2ä¿®æ”¹ä¸­ç­‰å½±å“
            'L3': 0.2     # L3ä¿®æ”¹å½±å“æœ€å°
        }
        
        # åˆ†æ•°èŒƒå›´å®šä¹‰
        self.score_ranges = {
            'L1': (0.9, 1.0),
            'L2': (0.3, 0.8),
            'L3': (0.1, 0.3)
        }
        
    def validate_scoring_formula(self) -> Dict[str, Any]:
        """éªŒè¯è¯„åˆ†å…¬å¼çš„æ­£ç¡®æ€§"""
        print("ğŸ§® éªŒè¯è¯„åˆ†å…¬å¼å®ç°...")
        print("=" * 50)
        
        issues = []
        test_results = []
        
        # æµ‹è¯•ç”¨ä¾‹è®¾è®¡
        test_cases = [
            {
                "name": "é«˜é£é™©å…¸å‹åœºæ™¯",
                "data": {
                    "modification_count": 1.0,  # å½’ä¸€åŒ–åˆ°0-1
                    "change_complexity": 1.0,
                    "risk_level": "L1",
                    "time_recency": 1.0
                },
                "expected_range": (0.9, 1.0),
                "expected_level": "L1"
            },
            {
                "name": "ä¸­é£é™©å…¸å‹åœºæ™¯", 
                "data": {
                    "modification_count": 0.6,
                    "change_complexity": 0.6,
                    "risk_level": "L2",
                    "time_recency": 0.5
                },
                "expected_range": (0.3, 0.8),
                "expected_level": "L2"
            },
            {
                "name": "ä½é£é™©å…¸å‹åœºæ™¯",
                "data": {
                    "modification_count": 0.2,
                    "change_complexity": 0.2,
                    "risk_level": "L3",
                    "time_recency": 0.1
                },
                "expected_range": (0.1, 0.3),
                "expected_level": "L3"
            },
            {
                "name": "è¾¹ç•Œæ¡ä»¶-å…¨é›¶å€¼",
                "data": {
                    "modification_count": 0.0,
                    "change_complexity": 0.0,
                    "risk_level": "L3",
                    "time_recency": 0.0
                },
                "expected_range": (0.0, 0.3),
                "expected_level": "L3"
            },
            {
                "name": "è¾¹ç•Œæ¡ä»¶-å…¨æœ€å¤§å€¼",
                "data": {
                    "modification_count": 1.0,
                    "change_complexity": 1.0,
                    "risk_level": "L1",
                    "time_recency": 1.0
                },
                "expected_range": (0.9, 1.0),
                "expected_level": "L1"
            }
        ]
        
        for test_case in test_cases:
            print(f"\nğŸ§ª æµ‹è¯•: {test_case['name']}")
            
            # è®¡ç®—åˆ†æ•°
            calculated_score = self._calculate_risk_score(test_case['data'])
            expected_min, expected_max = test_case['expected_range']
            
            print(f"   è¾“å…¥æ•°æ®: {test_case['data']}")
            print(f"   è®¡ç®—åˆ†æ•°: {calculated_score:.3f}")
            print(f"   æœŸæœ›èŒƒå›´: {expected_min}-{expected_max}")
            
            # æ£€æŸ¥åˆ†æ•°æ˜¯å¦åœ¨æœŸæœ›èŒƒå›´å†…
            in_range = expected_min <= calculated_score <= expected_max
            
            if in_range:
                print(f"   âœ… åˆ†æ•°åœ¨æœŸæœ›èŒƒå›´å†…")
            else:
                print(f"   âŒ åˆ†æ•°è¶…å‡ºæœŸæœ›èŒƒå›´")
                issues.append({
                    "type": "score_out_of_range",
                    "test_case": test_case['name'],
                    "calculated": calculated_score,
                    "expected_range": test_case['expected_range'],
                    "severity": "high"
                })
            
            # æ£€æŸ¥æ¨å¯¼çš„é£é™©ç­‰çº§
            derived_level = self._derive_risk_level_from_score(calculated_score)
            expected_level = test_case['expected_level']
            
            print(f"   æ¨å¯¼ç­‰çº§: {derived_level}")
            print(f"   æœŸæœ›ç­‰çº§: {expected_level}")
            
            if derived_level == expected_level:
                print(f"   âœ… é£é™©ç­‰çº§æ¨å¯¼æ­£ç¡®")
            else:
                print(f"   âŒ é£é™©ç­‰çº§æ¨å¯¼é”™è¯¯")
                issues.append({
                    "type": "risk_level_mismatch",
                    "test_case": test_case['name'],
                    "derived_level": derived_level,
                    "expected_level": expected_level,
                    "severity": "medium"
                })
            
            test_results.append({
                "test_case": test_case['name'],
                "calculated_score": calculated_score,
                "in_expected_range": in_range,
                "derived_level": derived_level,
                "level_correct": derived_level == expected_level
            })
        
        return {
            "issues": issues,
            "test_results": test_results,
            "total_tests": len(test_cases),
            "passed_tests": len([r for r in test_results if r["in_expected_range"] and r["level_correct"]]),
            "success_rate": len([r for r in test_results if r["in_expected_range"] and r["level_correct"]]) / len(test_cases)
        }
    
    def test_edge_cases(self) -> Dict[str, Any]:
        """æµ‹è¯•è¾¹ç•Œæ¡ä»¶å’Œå¼‚å¸¸å€¼å¤„ç†"""
        print(f"\nğŸ”¬ æµ‹è¯•è¾¹ç•Œæ¡ä»¶å’Œå¼‚å¸¸å€¼å¤„ç†...")
        print("=" * 50)
        
        edge_cases = [
            {
                "name": "è´Ÿæ•°è¾“å…¥",
                "data": {
                    "modification_count": -0.5,
                    "change_complexity": 0.5,
                    "risk_level": "L2",
                    "time_recency": 0.3
                },
                "should_handle": True
            },
            {
                "name": "è¶…å¤§æ•°å€¼è¾“å…¥",
                "data": {
                    "modification_count": 100.0,
                    "change_complexity": 50.0,
                    "risk_level": "L1",
                    "time_recency": 10.0
                },
                "should_handle": True
            },
            {
                "name": "NaNè¾“å…¥",
                "data": {
                    "modification_count": float('nan'),
                    "change_complexity": 0.5,
                    "risk_level": "L2", 
                    "time_recency": 0.3
                },
                "should_handle": True
            },
            {
                "name": "æ— ç©·å¤§è¾“å…¥",
                "data": {
                    "modification_count": float('inf'),
                    "change_complexity": 0.5,
                    "risk_level": "L2",
                    "time_recency": 0.3
                },
                "should_handle": True
            },
            {
                "name": "æ— æ•ˆé£é™©ç­‰çº§",
                "data": {
                    "modification_count": 0.5,
                    "change_complexity": 0.5,
                    "risk_level": "L4",  # ä¸å­˜åœ¨çš„ç­‰çº§
                    "time_recency": 0.3
                },
                "should_handle": True
            }
        ]
        
        issues = []
        
        for edge_case in edge_cases:
            print(f"\nğŸ§ª è¾¹ç•Œæµ‹è¯•: {edge_case['name']}")
            
            try:
                score = self._calculate_risk_score(edge_case['data'])
                
                # æ£€æŸ¥è¿”å›çš„åˆ†æ•°æ˜¯å¦åˆç†
                if math.isnan(score) or math.isinf(score):
                    print(f"   âŒ è¿”å›äº†æ— æ•ˆåˆ†æ•°: {score}")
                    issues.append({
                        "type": "invalid_score_returned",
                        "test_case": edge_case['name'],
                        "returned_score": str(score),
                        "severity": "high"
                    })
                elif score < 0 or score > 1:
                    print(f"   âŒ åˆ†æ•°è¶…å‡ºæœ‰æ•ˆèŒƒå›´[0,1]: {score}")
                    issues.append({
                        "type": "score_out_of_bounds",
                        "test_case": edge_case['name'],
                        "returned_score": score,
                        "severity": "high"
                    })
                else:
                    print(f"   âœ… æ­£ç¡®å¤„ç†å¼‚å¸¸è¾“å…¥ï¼Œè¿”å›åˆ†æ•°: {score:.3f}")
                    
            except Exception as e:
                print(f"   âŒ å¤„ç†å¼‚å¸¸è¾“å…¥æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                issues.append({
                    "type": "exception_on_edge_case",
                    "test_case": edge_case['name'],
                    "error": str(e),
                    "severity": "high"
                })
        
        return {
            "issues": issues,
            "edge_cases_tested": len(edge_cases),
            "edge_cases_passed": len(edge_cases) - len(issues)
        }
    
    def validate_score_consistency(self) -> Dict[str, Any]:
        """éªŒè¯è¯„åˆ†ä¸€è‡´æ€§"""
        print(f"\nğŸ”„ éªŒè¯è¯„åˆ†ä¸€è‡´æ€§...")
        print("=" * 50)
        
        # æµ‹è¯•ç›¸åŒè¾“å…¥æ˜¯å¦äº§ç”Ÿç›¸åŒè¾“å‡º
        test_data = {
            "modification_count": 0.7,
            "change_complexity": 0.6,
            "risk_level": "L2",
            "time_recency": 0.4
        }
        
        scores = []
        for i in range(10):
            score = self._calculate_risk_score(test_data)
            scores.append(score)
        
        # æ£€æŸ¥ä¸€è‡´æ€§
        all_same = all(abs(score - scores[0]) < 1e-10 for score in scores)
        
        print(f"   æµ‹è¯•æ•°æ®: {test_data}")
        print(f"   10æ¬¡è®¡ç®—ç»“æœ: {[f'{s:.6f}' for s in scores[:3]]}..." if len(scores) > 3 else scores)
        print(f"   ä¸€è‡´æ€§æ£€æŸ¥: {'âœ… é€šè¿‡' if all_same else 'âŒ å¤±è´¥'}")
        
        return {
            "consistency_passed": all_same,
            "scores": scores,
            "variance": max(scores) - min(scores) if scores else 0
        }
    
    def _calculate_risk_score(self, data: Dict[str, Any]) -> float:
        """è®¡ç®—é£é™©åˆ†æ•°çš„å‚è€ƒå®ç°"""
        try:
            # å¤„ç†å¼‚å¸¸è¾“å…¥
            mod_count = self._normalize_value(data.get('modification_count', 0))
            complexity = self._normalize_value(data.get('change_complexity', 0))
            time_recency = self._normalize_value(data.get('time_recency', 0))
            
            # è·å–é£é™©ç­‰çº§åˆ†æ•°
            risk_level = data.get('risk_level', 'L3')
            risk_score = self.risk_level_scores.get(risk_level, 0.2)  # é»˜è®¤L3
            
            # è®¡ç®—åŠ æƒåˆ†æ•°
            weighted_score = (
                self.documented_weights['modification_count'] * mod_count +
                self.documented_weights['change_complexity'] * complexity +
                self.documented_weights['risk_level'] * risk_score +
                self.documented_weights['time_recency'] * time_recency
            )
            
            # ç¡®ä¿åˆ†æ•°åœ¨[0,1]èŒƒå›´å†…
            return max(0.0, min(1.0, weighted_score))
            
        except Exception as e:
            # å‡ºé”™æ—¶è¿”å›é»˜è®¤çš„ä¸­ç­‰é£é™©åˆ†æ•°
            return 0.5
    
    def _normalize_value(self, value: Any) -> float:
        """æ ‡å‡†åŒ–æ•°å€¼åˆ°[0,1]èŒƒå›´"""
        try:
            val = float(value)
            if math.isnan(val) or math.isinf(val):
                return 0.5  # é»˜è®¤ä¸­ç­‰å€¼
            return max(0.0, min(1.0, val))  # é™åˆ¶åœ¨[0,1]èŒƒå›´
        except (ValueError, TypeError):
            return 0.5  # æ— æ³•è½¬æ¢æ—¶è¿”å›é»˜è®¤å€¼
    
    def _derive_risk_level_from_score(self, score: float) -> str:
        """ä»åˆ†æ•°æ¨å¯¼é£é™©ç­‰çº§"""
        if 0.9 <= score <= 1.0:
            return "L1"
        elif 0.3 <= score <= 0.8:
            return "L2"
        elif 0.1 <= score <= 0.3:
            return "L3"
        else:
            # åˆ†æ•°åœ¨é‡å åŒºåŸŸæˆ–å¼‚å¸¸èŒƒå›´ï¼Œæ ¹æ®æœ€æ¥è¿‘çš„åŒºé—´åˆ¤æ–­
            if score > 0.8:
                return "L1"
            elif score > 0.3:
                return "L2"
            else:
                return "L3"
    
    def generate_validation_report(self, formula_results: Dict, edge_results: Dict, 
                                 consistency_results: Dict) -> str:
        """ç”ŸæˆéªŒè¯æŠ¥å‘Š"""
        report = []
        report.append("# è¯„åˆ†ç®—æ³•éªŒè¯æŠ¥å‘Š")
        report.append("")
        
        # å…¬å¼éªŒè¯ç»“æœ
        report.append("## ğŸ“Š è¯„åˆ†å…¬å¼éªŒè¯")
        success_rate = formula_results['success_rate']
        report.append(f"- **æµ‹è¯•é€šè¿‡ç‡**: {success_rate:.1%}")
        report.append(f"- **æµ‹è¯•ç”¨ä¾‹**: {formula_results['passed_tests']}/{formula_results['total_tests']}")
        
        if formula_results['issues']:
            report.append("\n### å‘ç°çš„é—®é¢˜")
            for issue in formula_results['issues']:
                severity_icon = "ğŸ”´" if issue['severity'] == 'high' else "ğŸŸ¡"
                report.append(f"- {severity_icon} **{issue['type']}**: {issue.get('test_case', '')}")
        
        # è¾¹ç•Œæ¡ä»¶æµ‹è¯•ç»“æœ
        report.append("\n## ğŸ”¬ è¾¹ç•Œæ¡ä»¶æµ‹è¯•")
        edge_pass_rate = edge_results['edge_cases_passed'] / edge_results['edge_cases_tested']
        report.append(f"- **è¾¹ç•Œæµ‹è¯•é€šè¿‡ç‡**: {edge_pass_rate:.1%}")
        report.append(f"- **æµ‹è¯•ç”¨ä¾‹**: {edge_results['edge_cases_passed']}/{edge_results['edge_cases_tested']}")
        
        if edge_results['issues']:
            report.append("\n### è¾¹ç•Œæ¡ä»¶é—®é¢˜")
            for issue in edge_results['issues']:
                report.append(f"- ğŸ”´ **{issue['type']}**: {issue.get('test_case', '')}")
        
        # ä¸€è‡´æ€§æµ‹è¯•ç»“æœ
        report.append("\n## ğŸ”„ ä¸€è‡´æ€§æµ‹è¯•")
        consistency_status = "âœ… é€šè¿‡" if consistency_results['consistency_passed'] else "âŒ å¤±è´¥"
        report.append(f"- **ä¸€è‡´æ€§æ£€æŸ¥**: {consistency_status}")
        report.append(f"- **åˆ†æ•°æ–¹å·®**: {consistency_results['variance']:.10f}")
        
        # æ€»ä½“è¯„ä»·
        overall_issues = len(formula_results['issues']) + len(edge_results['issues'])
        if not consistency_results['consistency_passed']:
            overall_issues += 1
            
        report.append("\n## ğŸ¯ æ€»ä½“è¯„ä»·")
        if overall_issues == 0:
            report.append("âœ… **è¯„åˆ†ç®—æ³•å®ç°æ­£ç¡®ï¼Œæ— é‡å¤§é—®é¢˜**")
        elif overall_issues <= 2:
            report.append("ğŸŸ¡ **è¯„åˆ†ç®—æ³•åŸºæœ¬æ­£ç¡®ï¼Œæœ‰å°‘é‡é—®é¢˜éœ€è¦ä¿®å¤**")
        else:
            report.append("âŒ **è¯„åˆ†ç®—æ³•å­˜åœ¨å¤šä¸ªé—®é¢˜ï¼Œéœ€è¦é‡ç‚¹ä¿®å¤**")
        
        report.append(f"\n- **å‘ç°é—®é¢˜æ€»æ•°**: {overall_issues}")
        report.append(f"- **ç®—æ³•å¯é æ€§**: {'é«˜' if overall_issues <= 1 else 'ä¸­' if overall_issues <= 3 else 'ä½'}")
        
        return "\n".join(report)

def main():
    """ä¸»å‡½æ•°"""
    validator = ScoringAlgorithmValidator()
    
    print("ğŸš€ å¼€å§‹è¯„åˆ†ç®—æ³•å®ç°éªŒè¯")
    print("=" * 60)
    
    # éªŒè¯è¯„åˆ†å…¬å¼
    formula_results = validator.validate_scoring_formula()
    
    # æµ‹è¯•è¾¹ç•Œæ¡ä»¶
    edge_results = validator.test_edge_cases()
    
    # éªŒè¯ä¸€è‡´æ€§
    consistency_results = validator.validate_score_consistency()
    
    # ç”ŸæˆæŠ¥å‘Š
    report = validator.generate_validation_report(
        formula_results, edge_results, consistency_results
    )
    
    # ä¿å­˜æŠ¥å‘Š
    report_filename = "scoring_algorithm_validation_report.md"
    with open(report_filename, 'w', encoding='utf-8') as f:
        f.write(report)
    
    print(f"\nğŸ“Š ç®—æ³•éªŒè¯å®Œæˆ!")
    print(f"ğŸ“„ éªŒè¯æŠ¥å‘Šå·²ä¿å­˜: {report_filename}")
    
    # æ±‡æ€»ç»“æœ
    total_issues = len(formula_results['issues']) + len(edge_results['issues'])
    if not consistency_results['consistency_passed']:
        total_issues += 1
    
    print(f"\nğŸ¯ éªŒè¯æ±‡æ€»:")
    print(f"   å…¬å¼æµ‹è¯•é€šè¿‡ç‡: {formula_results['success_rate']:.1%}")
    print(f"   è¾¹ç•Œæµ‹è¯•é€šè¿‡ç‡: {edge_results['edge_cases_passed']/edge_results['edge_cases_tested']:.1%}")
    print(f"   ä¸€è‡´æ€§æµ‹è¯•: {'é€šè¿‡' if consistency_results['consistency_passed'] else 'å¤±è´¥'}")
    print(f"   å‘ç°é—®é¢˜æ€»æ•°: {total_issues}")
    print(f"   ç®—æ³•çŠ¶æ€: {'âœ… å¯é ' if total_issues <= 1 else 'ğŸŸ¡ åŸºæœ¬å¯ç”¨' if total_issues <= 3 else 'âŒ éœ€è¦ä¿®å¤'}")

if __name__ == "__main__":
    main()