#!/usr/bin/env python3
"""
æ•°æ®æµç¨‹å®Œæ•´æ€§æ·±åº¦å®¡è®¡å™¨
å‘ç°å¹¶åˆ†ææ•°æ®ä¼ é€’è¿‡ç¨‹ä¸­çš„ä¸¢å¤±ã€å‰²è£‚å’Œä¸ä¸€è‡´é—®é¢˜
"""

import json
import os
from typing import Dict, List, Any

class DataFlowIntegrityAuditor:
    """æ•°æ®æµç¨‹å®Œæ•´æ€§å®¡è®¡å™¨"""
    
    def __init__(self):
        self.base_dir = "/root/projects/tencent-doc-manager"
        self.audit_results = {
            "timestamp": "2025-08-20T20:30:00",
            "audit_type": "deep_data_integrity_check",
            "critical_issues": [],
            "data_loss_points": [],
            "inconsistencies": [],
            "recommendations": []
        }
    
    def perform_comprehensive_audit(self) -> Dict[str, Any]:
        """æ‰§è¡Œå…¨é¢çš„æ•°æ®æµç¨‹å®¡è®¡"""
        
        print("ğŸ” å¼€å§‹æ•°æ®æµç¨‹å®Œæ•´æ€§æ·±åº¦å®¡è®¡...")
        
        # æ­¥éª¤1: å®¡è®¡CSVå¯¹æ¯”æ•°æ®
        csv_audit = self._audit_csv_comparison_stage()
        
        # æ­¥éª¤2: å®¡è®¡æ™ºèƒ½æ˜ å°„é˜¶æ®µ  
        mapping_audit = self._audit_intelligent_mapping_stage()
        
        # æ­¥éª¤3: å®¡è®¡æœ€ç»ˆçƒ­åŠ›å›¾æ•°æ®
        heatmap_audit = self._audit_final_heatmap_stage()
        
        # æ­¥éª¤4: æ•°æ®ä¸€è‡´æ€§äº¤å‰éªŒè¯
        consistency_audit = self._perform_cross_stage_validation(
            csv_audit, mapping_audit, heatmap_audit
        )
        
        # æ­¥éª¤5: ç”Ÿæˆé—®é¢˜æŠ¥å‘Šå’Œä¿®å¤å»ºè®®
        self._generate_audit_report(csv_audit, mapping_audit, heatmap_audit, consistency_audit)
        
        return self.audit_results
    
    def _audit_csv_comparison_stage(self) -> Dict[str, Any]:
        """å®¡è®¡CSVå¯¹æ¯”é˜¶æ®µ"""
        
        print("   ğŸ“‹ å®¡è®¡é˜¶æ®µ1: CSVå¯¹æ¯”æ•°æ®...")
        
        csv_file = os.path.join(self.base_dir, "csv_security_results/real_test_comparison.json")
        
        if not os.path.exists(csv_file):
            return {"error": "CSVå¯¹æ¯”ç»“æœæ–‡ä»¶ä¸å­˜åœ¨"}
        
        with open(csv_file, 'r', encoding='utf-8') as f:
            csv_data = json.load(f)
        
        audit_result = {
            "stage": "csv_comparison",
            "total_differences": len(csv_data.get("differences", [])),
            "column_info": csv_data.get("file_info", {}).get("metadata", {}).get("column_mapping", {}),
            "differences_detail": []
        }
        
        # è¯¦ç»†åˆ†ææ¯ä¸ªå·®å¼‚
        for i, diff in enumerate(csv_data.get("differences", [])):
            diff_info = {
                "index": i + 1,
                "location": f"è¡Œ{diff['è¡Œå·']}åˆ—{diff['åˆ—ç´¢å¼•']}",
                "column_name": diff["åˆ—å"],
                "change": f"{diff['åŸå€¼']} â†’ {diff['æ–°å€¼']}",
                "risk_level": diff["risk_level"],
                "risk_score": diff["risk_score"],
                "mappable": self._check_column_mappability(diff["åˆ—å"])
            }
            audit_result["differences_detail"].append(diff_info)
        
        print(f"      âœ“ å‘ç° {audit_result['total_differences']} ä¸ªå·®å¼‚")
        return audit_result
    
    def _audit_intelligent_mapping_stage(self) -> Dict[str, Any]:
        """å®¡è®¡æ™ºèƒ½æ˜ å°„é˜¶æ®µ"""
        
        print("   ğŸ§  å®¡è®¡é˜¶æ®µ2: æ™ºèƒ½æ˜ å°„å¤„ç†...")
        
        mapping_file = os.path.join(self.base_dir, "intelligent_mapping_result.json")
        
        if not os.path.exists(mapping_file):
            return {"error": "æ™ºèƒ½æ˜ å°„ç»“æœæ–‡ä»¶ä¸å­˜åœ¨"}
        
        with open(mapping_file, 'r', encoding='utf-8') as f:
            mapping_data = json.load(f)
        
        audit_result = {
            "stage": "intelligent_mapping",
            "claimed_source_differences": mapping_data["processing_info"]["source_differences"],
            "column_mapping": mapping_data["column_mapping"]["mappings"],
            "unmapped_columns": mapping_data["column_mapping"]["unmapped_columns"],
            "coverage_rate": mapping_data["column_mapping"]["coverage_rate"],
            "row_mapping": mapping_data["row_mapping"]["mappings"],
            "mapping_issues": []
        }
        
        # æ£€æŸ¥æ˜ å°„é—®é¢˜
        for unmapped_col in audit_result["unmapped_columns"]:
            audit_result["mapping_issues"].append({
                "type": "unmapped_column",
                "column": unmapped_col,
                "impact": "è¯¥åˆ—çš„æ‰€æœ‰å·®å¼‚å°†è¢«ä¸¢å¤±",
                "severity": "HIGH"
            })
        
        print(f"      âœ“ åˆ—æ˜ å°„è¦†ç›–ç‡: {audit_result['coverage_rate']:.1%}")
        print(f"      âš  æœªæ˜ å°„åˆ—æ•°: {len(audit_result['unmapped_columns'])}")
        
        return audit_result
    
    def _audit_final_heatmap_stage(self) -> Dict[str, Any]:
        """å®¡è®¡æœ€ç»ˆçƒ­åŠ›å›¾é˜¶æ®µ"""
        
        print("   ğŸ”¥ å®¡è®¡é˜¶æ®µ3: æœ€ç»ˆçƒ­åŠ›å›¾æ•°æ®...")
        
        heatmap_file = os.path.join(self.base_dir, "production/servers/real_time_heatmap.json")
        
        if not os.path.exists(heatmap_file):
            return {"error": "çƒ­åŠ›å›¾æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨"}
        
        with open(heatmap_file, 'r', encoding='utf-8') as f:
            heatmap_data = json.load(f)
        
        # åˆ†æçƒ­åŠ›å›¾çŸ©é˜µ
        matrix = heatmap_data["heatmap_data"]
        significant_hotspots = []
        
        for i, row in enumerate(matrix):
            for j, value in enumerate(row):
                if value > 0.1:  # æ˜¾è‘—çƒ­ç‚¹é˜ˆå€¼
                    significant_hotspots.append({
                        "position": f"è¡Œ{i}åˆ—{j}",
                        "intensity": round(value, 4)
                    })
        
        audit_result = {
            "stage": "final_heatmap",
            "claimed_changes_applied": heatmap_data["changes_applied"],
            "data_source": heatmap_data["data_source"],
            "matrix_size": f"{len(matrix)}x{len(matrix[0])}",
            "total_hotspots": len([v for row in matrix for v in row if v > 0.06]),
            "significant_hotspots": len(significant_hotspots),
            "max_intensity": max(max(row) for row in matrix),
            "hotspots_detail": significant_hotspots
        }
        
        print(f"      âœ“ å£°ç§°åº”ç”¨å˜æ›´: {audit_result['claimed_changes_applied']}ä¸ª")
        print(f"      âœ“ æ˜¾è‘—çƒ­ç‚¹æ•°: {audit_result['significant_hotspots']}ä¸ª")
        
        return audit_result
    
    def _perform_cross_stage_validation(self, csv_audit, mapping_audit, heatmap_audit) -> Dict[str, Any]:
        """æ‰§è¡Œè·¨é˜¶æ®µæ•°æ®ä¸€è‡´æ€§éªŒè¯"""
        
        print("   ğŸ”— æ‰§è¡Œè·¨é˜¶æ®µä¸€è‡´æ€§éªŒè¯...")
        
        validation_result = {
            "data_quantity_consistency": {},
            "data_quality_consistency": {},
            "processing_chain_integrity": {}
        }
        
        # éªŒè¯1: æ•°æ®æ•°é‡ä¸€è‡´æ€§
        csv_differences = csv_audit.get("total_differences", 0)
        mapping_claimed = mapping_audit.get("claimed_source_differences", 0)  
        heatmap_claimed = heatmap_audit.get("claimed_changes_applied", 0)
        
        validation_result["data_quantity_consistency"] = {
            "csv_differences": csv_differences,
            "mapping_claimed": mapping_claimed,
            "heatmap_claimed": heatmap_claimed,
            "consistent": csv_differences == mapping_claimed == heatmap_claimed,
            "discrepancy": abs(csv_differences - heatmap_claimed)
        }
        
        # éªŒè¯2: å®é™…å¤„ç†æ•ˆæœ
        unmapped_columns = mapping_audit.get("unmapped_columns", [])
        coverage_rate = mapping_audit.get("coverage_rate", 0)
        significant_hotspots = heatmap_audit.get("significant_hotspots", 0)
        
        # è®¡ç®—é¢„æœŸå¯å¤„ç†çš„å·®å¼‚æ•°é‡
        processable_differences = 0
        for diff in csv_audit.get("differences_detail", []):
            if diff["column_name"] not in unmapped_columns:
                processable_differences += 1
        
        validation_result["data_quality_consistency"] = {
            "processable_differences": processable_differences,
            "actual_significant_hotspots": significant_hotspots,
            "expected_vs_actual_ratio": significant_hotspots / processable_differences if processable_differences > 0 else 0,
            "coverage_impact": len(unmapped_columns) / 5  # å‡è®¾æ€»åˆ—æ•°ä¸º5
        }
        
        # éªŒè¯3: å¤„ç†é“¾è·¯å®Œæ•´æ€§
        data_loss_points = []
        
        if csv_differences != heatmap_claimed:
            data_loss_points.append("æ•°é‡å£°æ˜ä¸ä¸€è‡´")
        
        if len(unmapped_columns) > 0:
            data_loss_points.append(f"{len(unmapped_columns)}ä¸ªåˆ—æ— æ³•æ˜ å°„")
        
        if significant_hotspots < processable_differences:
            data_loss_points.append("çƒ­ç‚¹ç”Ÿæˆä¸è¶³")
        
        validation_result["processing_chain_integrity"] = {
            "integrity_status": "BROKEN" if data_loss_points else "INTACT",
            "data_loss_points": data_loss_points,
            "overall_efficiency": (significant_hotspots / csv_differences) if csv_differences > 0 else 0
        }
        
        return validation_result
    
    def _check_column_mappability(self, column_name: str) -> bool:
        """æ£€æŸ¥åˆ—åæ˜¯å¦å¯ä»¥æ˜ å°„åˆ°19ä¸ªæ ‡å‡†åˆ—"""
        
        # æ¨¡æ‹Ÿæ™ºèƒ½æ˜ å°„ç®—æ³•çš„å…³é”®è¯åŒ¹é…
        standard_keywords = [
            ["id", "åºå·", "ç¼–å·"], ["è´Ÿè´£äºº", "owner", "responsible"],
            ["çŠ¶æ€", "status", "è¿›åº¦"], ["ç±»å‹", "type", "category"],
            ["æ—¶é—´", "date", "time"], ["éƒ¨é—¨", "department", "dept"],
            # ... å…¶ä»–æ ‡å‡†åˆ—å…³é”®è¯
        ]
        
        column_lower = column_name.lower()
        
        for keywords in standard_keywords:
            for keyword in keywords:
                if keyword.lower() in column_lower or column_lower in keyword.lower():
                    return True
        
        return False
    
    def _generate_audit_report(self, csv_audit, mapping_audit, heatmap_audit, consistency_audit):
        """ç”Ÿæˆå®¡è®¡æŠ¥å‘Š"""
        
        print("   ğŸ“„ ç”Ÿæˆå®¡è®¡æŠ¥å‘Š...")
        
        # è¯†åˆ«å…³é”®é—®é¢˜
        consistency = consistency_audit["data_quantity_consistency"]
        quality = consistency_audit["data_quality_consistency"]
        integrity = consistency_audit["processing_chain_integrity"]
        
        # ä¸¥é‡é—®é¢˜è¯†åˆ«
        if not consistency["consistent"]:
            self.audit_results["critical_issues"].append({
                "type": "DATA_QUANTITY_INCONSISTENCY",
                "description": f"æ•°æ®æ•°é‡å£°æ˜ä¸ä¸€è‡´: CSV({consistency['csv_differences']}) vs çƒ­åŠ›å›¾({consistency['heatmap_claimed']})",
                "severity": "CRITICAL",
                "impact": "æ•°æ®å®Œæ•´æ€§å—æŸ"
            })
        
        if len(mapping_audit["unmapped_columns"]) > 0:
            lost_differences = 0
            for diff in csv_audit.get("differences_detail", []):
                if diff["column_name"] in mapping_audit["unmapped_columns"]:
                    lost_differences += 1
            
            self.audit_results["critical_issues"].append({
                "type": "COLUMN_MAPPING_DATA_LOSS",
                "description": f"{len(mapping_audit['unmapped_columns'])}ä¸ªåˆ—æ— æ³•æ˜ å°„ï¼Œå¯¼è‡´{lost_differences}ä¸ªå·®å¼‚ä¸¢å¤±",
                "severity": "HIGH",
                "impact": f"æ•°æ®ä¸¢å¤±ç‡: {lost_differences/csv_audit['total_differences']:.1%}",
                "lost_columns": mapping_audit["unmapped_columns"]
            })
        
        if quality["actual_significant_hotspots"] < quality["processable_differences"]:
            self.audit_results["critical_issues"].append({
                "type": "HEATMAP_GENERATION_INEFFICIENCY", 
                "description": f"å¯å¤„ç†{quality['processable_differences']}ä¸ªå·®å¼‚ï¼Œä½†åªç”Ÿæˆ{quality['actual_significant_hotspots']}ä¸ªæ˜¾è‘—çƒ­ç‚¹",
                "severity": "MEDIUM",
                "impact": "çƒ­åŠ›å›¾æ•ˆæœä¸ä½³"
            })
        
        # æ•°æ®ä¸¢å¤±ç‚¹åˆ†æ
        self.audit_results["data_loss_points"] = [
            {
                "stage": "intelligent_mapping",
                "type": "column_unmappable",
                "description": "éƒ¨åˆ†åˆ—åæ— æ³•æ˜ å°„åˆ°æ ‡å‡†19åˆ—",
                "affected_data": mapping_audit["unmapped_columns"],
                "loss_rate": 1 - mapping_audit["coverage_rate"]
            },
            {
                "stage": "heatmap_generation", 
                "type": "insufficient_intensity",
                "description": "éƒ¨åˆ†æ˜ å°„æˆåŠŸçš„å·®å¼‚æœªäº§ç”Ÿæ˜¾è‘—çƒ­ç‚¹",
                "affected_data": "ä½å¼ºåº¦çƒ­ç‚¹",
                "loss_rate": 1 - quality["expected_vs_actual_ratio"]
            }
        ]
        
        # ä¿®å¤å»ºè®®
        self.audit_results["recommendations"] = [
            {
                "priority": "HIGH",
                "category": "ç®—æ³•æ”¹è¿›",
                "action": "æ‰©å±•æ™ºèƒ½æ˜ å°„ç®—æ³•çš„è¯­ä¹‰è¯å…¸",
                "description": "ä¸º'å·¥èµ„'ã€'éƒ¨é—¨'ç­‰å¸¸è§åˆ—åæ·»åŠ æ˜ å°„è§„åˆ™",
                "expected_impact": "æé«˜åˆ—æ˜ å°„è¦†ç›–ç‡åˆ°90%+"
            },
            {
                "priority": "HIGH", 
                "category": "æ•°æ®å®Œæ•´æ€§",
                "action": "å®ç°æ•°æ®å®ˆæ’éªŒè¯æœºåˆ¶",
                "description": "ç¡®ä¿æ¯ä¸ªé˜¶æ®µå¤„ç†çš„æ•°æ®é‡ä¸è¾“å…¥ä¸€è‡´",
                "expected_impact": "æ¶ˆé™¤æ•°æ®æ— å£°ä¸¢å¤±é—®é¢˜"
            },
            {
                "priority": "MEDIUM",
                "category": "çƒ­åŠ›å›¾ç®—æ³•",
                "action": "è°ƒæ•´çƒ­åŠ›å¼ºåº¦è®¡ç®—å…¬å¼",
                "description": "ç¡®ä¿æ‰€æœ‰æœ‰æ•ˆå·®å¼‚éƒ½äº§ç”Ÿå¯è§çƒ­ç‚¹",
                "expected_impact": "æé«˜çƒ­ç‚¹ç”Ÿæˆæ•ˆç‡"
            },
            {
                "priority": "MEDIUM",
                "category": "ç›‘æ§å‘Šè­¦",
                "action": "æ·»åŠ å®æ—¶æ•°æ®æµç¨‹ç›‘æ§",
                "description": "åœ¨æ¯ä¸ªå¤„ç†é˜¶æ®µéªŒè¯æ•°æ®å®Œæ•´æ€§",
                "expected_impact": "åŠæ—¶å‘ç°æ•°æ®å¼‚å¸¸"
            }
        ]

def main():
    """ä¸»å‡½æ•°"""
    auditor = DataFlowIntegrityAuditor()
    
    print("ğŸ¯ æ•°æ®æµç¨‹å®Œæ•´æ€§æ·±åº¦å®¡è®¡")
    print("=" * 60)
    
    audit_results = auditor.perform_comprehensive_audit()
    
    # ä¿å­˜å®¡è®¡æŠ¥å‘Š
    report_file = "/root/projects/tencent-doc-manager/data_integrity_audit_report.json"
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(audit_results, f, indent=2, ensure_ascii=False)
    
    print("\n" + "=" * 60)
    print("ğŸ“Š å®¡è®¡ç»“æœæ‘˜è¦:")
    print(f"   å…³é”®é—®é¢˜æ•°: {len(audit_results['critical_issues'])}")
    print(f"   æ•°æ®ä¸¢å¤±ç‚¹: {len(audit_results['data_loss_points'])}")
    print(f"   ä¿®å¤å»ºè®®: {len(audit_results['recommendations'])}")
    
    print("\nğŸš¨ å…³é”®é—®é¢˜:")
    for issue in audit_results["critical_issues"]:
        print(f"   âŒ {issue['type']}: {issue['description']}")
    
    print("\nğŸ’¡ ä¼˜å…ˆä¿®å¤å»ºè®®:")
    for rec in audit_results["recommendations"][:3]:
        print(f"   ğŸ”§ {rec['priority']}: {rec['action']}")
    
    print(f"\nğŸ“„ å®Œæ•´å®¡è®¡æŠ¥å‘Šå·²ä¿å­˜è‡³: {report_file}")
    
    return audit_results

if __name__ == "__main__":
    main()