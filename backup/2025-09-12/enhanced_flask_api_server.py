#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è…¾è®¯æ–‡æ¡£ç®¡ç†ç³»ç»Ÿ - å¢å¼ºç‰ˆFlask APIæœåŠ¡å™¨
é›†æˆæ‰€æœ‰æ ¸å¿ƒç»„ä»¶ï¼šè‡ªé€‚åº”è¡¨æ ¼å¯¹æ¯”ã€AIè¯­ä¹‰åˆ†æã€Excel MCPå¯è§†åŒ–ã€è…¾è®¯æ–‡æ¡£ä¸Šä¼ 
ç‰ˆæœ¬: 2.0.0
ä½œè€…: Claude Code System Integration Team
åˆ›å»ºæ—¶é—´: 2025-08-12

Excel MCP ä½¿ç”¨è¯´æ˜:
è¯·åœ¨ä½¿ç”¨Excel MCPç›¸å…³åŠŸèƒ½å‰ï¼ŒåŠ¡å¿…é˜…è¯» docs/Excel-MCP-AI-ä½¿ç”¨æŒ‡å—.md
ç¡®ä¿æ­£ç¡®ä½¿ç”¨ mcp__excel-optimized__* ç³»åˆ—å‡½æ•°ï¼Œé¿å…å†…å­˜æº¢å‡ºå’Œæ ¼å¼é”™è¯¯ã€‚
"""

from flask import Flask, request, jsonify, send_file, render_template, stream_template
from flask_cors import CORS
from werkzeug.utils import secure_filename
import os
import json
import asyncio
import pandas as pd
from datetime import datetime, timedelta
import hashlib
import sqlite3
from pathlib import Path
import secrets
import logging
import threading
import time
from functools import wraps
from typing import Dict, List, Any, Optional
import traceback

# å¯¼å…¥ç°æœ‰çš„è‡ªåŠ¨åŒ–å·¥å…·
try:
    from tencent_export_automation import TencentDocAutoExporter
    from tencent_upload_automation import TencentDocUploader
    from concurrent_processor import get_processor, TaskType
except ImportError:
    print("âš ï¸ éƒ¨åˆ†è…¾è®¯æ–‡æ¡£è‡ªåŠ¨åŒ–å·¥å…·æœªæ‰¾åˆ°ï¼Œå°†è·³è¿‡ç›¸å…³åŠŸèƒ½")

# å¯¼å…¥é›†æˆçš„æ–°ç»„ä»¶
from document_change_analyzer import DocumentChangeAnalyzer
from adaptive_table_comparator import AdaptiveTableComparator, IntelligentColumnMatcher
# ä½¿ç”¨æ–°çš„Claudeå°è£…ç¨‹åºé›†æˆ
from claude_wrapper_integration import AISemanticAnalysisOrchestrator
from excel_mcp_visualizer import ExcelMCPVisualizationClient, VisualizationConfig

# å…¼å®¹æ€§å¯¼å…¥
try:
    from claude_semantic_analysis import ModificationAnalysisRequest
except ImportError:
    # å¦‚æœåŸæ¨¡å—ä¸å­˜åœ¨ï¼Œåˆ›å»ºåŸºæœ¬æ•°æ®ç»“æ„
    from dataclasses import dataclass
    from typing import Optional
    
    @dataclass
    class ModificationAnalysisRequest:
        modification_id: str
        column_name: str
        original_value: str
        new_value: str
        table_name: str
        row_index: int
        change_context: Optional[str] = None
        modification_time: Optional[str] = None
        modifier: Optional[str] = None

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('tencent_doc_manager.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

app = Flask(__name__)
CORS(app, resources={
    r"/api/*": {
        "origins": ["http://localhost:3000", "http://127.0.0.1:3000"],
        "methods": ["GET", "POST", "PUT", "DELETE"],
        "allow_headers": ["Content-Type", "Authorization"]
    }
})

# é…ç½®
app.config.update({
    'SECRET_KEY': secrets.token_hex(32),
    'UPLOAD_FOLDER': 'uploads',
    'DOWNLOAD_FOLDER': 'downloads',
    'ANALYSIS_FOLDER': 'analysis_results',
    'VISUALIZATION_FOLDER': 'visualizations',
    'MAX_CONTENT_LENGTH': 500 * 1024 * 1024,  # 500MBé™åˆ¶
    'DATABASE': 'tencent_docs_enhanced.db',
    'REDIS_URL': 'redis://localhost:6379/0',
    'CLAUDE_API_KEY': os.getenv('CLAUDE_API_KEY', ''),
    'CLAUDE_WRAPPER_URL': os.getenv('CLAUDE_WRAPPER_URL', 'http://localhost:8081'),  # æ–°å¢
    'PROCESSING_TIMEOUT': 300,  # 5åˆ†é’Ÿå¤„ç†è¶…æ—¶
    'MAX_CONCURRENT_JOBS': 3,
    'BATCH_SIZE': 30
})

# ç¡®ä¿æ‰€æœ‰ç›®å½•å­˜åœ¨
for folder in ['UPLOAD_FOLDER', 'DOWNLOAD_FOLDER', 'ANALYSIS_FOLDER', 'VISUALIZATION_FOLDER']:
    os.makedirs(app.config[folder], exist_ok=True)

os.makedirs('static', exist_ok=True)
os.makedirs('templates', exist_ok=True)
os.makedirs('logs', exist_ok=True)

# ===================== æ ¸å¿ƒç»„ä»¶åˆå§‹åŒ– =====================

class EnhancedDocumentManager:
    """å¢å¼ºç‰ˆæ–‡æ¡£ç®¡ç†å™¨ - ç»Ÿä¸€ç®¡ç†æ‰€æœ‰ç»„ä»¶"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.logger = logging.getLogger(self.__class__.__name__)
        
        # åˆå§‹åŒ–æ ¸å¿ƒç»„ä»¶
        self.document_analyzer = DocumentChangeAnalyzer()
        self.adaptive_comparator = AdaptiveTableComparator()
        self.column_matcher = IntelligentColumnMatcher()
        
        # åˆå§‹åŒ–AIè¯­ä¹‰åˆ†æå™¨ï¼ˆä½¿ç”¨Claudeå°è£…ç¨‹åºï¼‰
        self.ai_analyzer = None
        claude_wrapper_url = config.get('CLAUDE_WRAPPER_URL', 'http://localhost:8081')
        try:
            self.ai_analyzer = AISemanticAnalysisOrchestrator(claude_wrapper_url)
            self.logger.info("âœ… AIè¯­ä¹‰åˆ†æå™¨åˆå§‹åŒ–æˆåŠŸ (ä½¿ç”¨Claudeå°è£…ç¨‹åº)")
        except Exception as e:
            self.logger.warning(f"âš ï¸ AIè¯­ä¹‰åˆ†æå™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            # ä¿æŒå…¼å®¹æ€§ï¼Œå¦‚æœæœ‰ç›´æ¥APIå¯†é’¥ä¹Ÿå¯ä»¥ç”¨
            if config.get('CLAUDE_API_KEY'):
                try:
                    from claude_semantic_analysis import AISemanticAnalysisOrchestrator as DirectAI
                    self.ai_analyzer = DirectAI(config['CLAUDE_API_KEY'])
                    self.logger.info("âœ… ä½¿ç”¨ç›´æ¥APIä½œä¸ºå¤‡é€‰æ–¹æ¡ˆ")
                except Exception:
                    pass
        
        # åˆå§‹åŒ–Excelå¯è§†åŒ–å™¨
        self.visualization_config = VisualizationConfig(
            enable_diagonal_pattern=True,
            enable_detailed_comments=True,
            enable_risk_charts=True,
            enable_interactive_dashboard=True,
            color_scheme="professional"
        )
        self.excel_visualizer = ExcelMCPVisualizationClient(self.visualization_config)
        
        # å¤„ç†ç»Ÿè®¡
        self.processing_stats = {
            "total_analyses": 0,
            "successful_analyses": 0,
            "failed_analyses": 0,
            "ai_analyses_performed": 0,
            "visualizations_created": 0,
            "start_time": datetime.now()
        }
    
    async def comprehensive_document_analysis(self, file_paths: List[str], 
                                            reference_file: str = None,
                                            enable_ai_analysis: bool = True,
                                            enable_visualization: bool = True,
                                            job_id: str = None) -> Dict[str, Any]:
        """
        å…¨é¢çš„æ–‡æ¡£åˆ†ææµç¨‹
        
        Args:
            file_paths: è¦åˆ†æçš„æ–‡ä»¶è·¯å¾„åˆ—è¡¨
            reference_file: å‚è€ƒæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
            enable_ai_analysis: æ˜¯å¦å¯ç”¨AIåˆ†æ
            enable_visualization: æ˜¯å¦å¯ç”¨å¯è§†åŒ–
            job_id: ä½œä¸šIDï¼ˆç”¨äºè¿›åº¦è·Ÿè¸ªï¼‰
            
        Returns:
            å®Œæ•´çš„åˆ†æç»“æœ
        """
        
        analysis_start_time = time.time()
        self.processing_stats["total_analyses"] += 1
        
        try:
            self.logger.info(f"ğŸš€ å¼€å§‹å…¨é¢æ–‡æ¡£åˆ†æï¼Œæ–‡ä»¶æ•°é‡: {len(file_paths)}")
            
            analysis_result = {
                "job_id": job_id,
                "analysis_timestamp": datetime.now().isoformat(),
                "file_paths": file_paths,
                "reference_file": reference_file,
                "processing_summary": {},
                "adaptive_comparison_results": [],
                "ai_analysis_results": {},
                "visualization_results": {},
                "risk_summary": {},
                "recommendations": [],
                "processing_stats": {}
            }
            
            # ç¬¬1é˜¶æ®µï¼šè‡ªé€‚åº”è¡¨æ ¼å¯¹æ¯”åˆ†æ
            self.logger.info("ğŸ“Š ç¬¬1é˜¶æ®µï¼šæ‰§è¡Œè‡ªé€‚åº”è¡¨æ ¼å¯¹æ¯”åˆ†æ...")
            
            # æ„å»ºè¡¨æ ¼æ•°æ®ç»“æ„
            current_tables = []
            for i, file_path in enumerate(file_paths):
                try:
                    # æ ¹æ®æ–‡ä»¶ç±»å‹é€‰æ‹©è§£ææ–¹æ³•
                    if file_path.endswith('.csv'):
                        df = pd.read_csv(file_path, encoding='utf-8-sig')
                    elif file_path.endswith(('.xlsx', '.xls')):
                        df = pd.read_excel(file_path)
                    else:
                        self.logger.warning(f"âš ï¸ ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_path}")
                        continue
                    
                    table_data = {
                        "name": os.path.basename(file_path),
                        "data": [df.columns.tolist()] + df.values.tolist()
                    }
                    current_tables.append(table_data)
                    
                except Exception as e:
                    self.logger.error(f"âŒ æ–‡ä»¶è§£æå¤±è´¥ {file_path}: {e}")
                    continue
            
            if not current_tables:
                raise ValueError("æ²¡æœ‰æˆåŠŸè§£æçš„æ–‡ä»¶")
            
            # æ‰§è¡Œè‡ªé€‚åº”å¯¹æ¯”
            reference_tables = None
            if reference_file:
                try:
                    if reference_file.endswith('.csv'):
                        ref_df = pd.read_csv(reference_file, encoding='utf-8-sig')
                    elif reference_file.endswith(('.xlsx', '.xls')):
                        ref_df = pd.read_excel(reference_file)
                    
                    reference_tables = [{
                        "name": os.path.basename(reference_file),
                        "data": [ref_df.columns.tolist()] + ref_df.values.tolist()
                    }]
                except Exception as e:
                    self.logger.warning(f"âš ï¸ å‚è€ƒæ–‡ä»¶è§£æå¤±è´¥: {e}")
            
            adaptive_result = self.adaptive_comparator.adaptive_compare_tables(
                current_tables, reference_tables
            )
            analysis_result["adaptive_comparison_results"] = adaptive_result
            
            # ç¬¬2é˜¶æ®µï¼šAIè¯­ä¹‰åˆ†æï¼ˆå¦‚æœå¯ç”¨ä¸”æ£€æµ‹åˆ°L2çº§åˆ«å˜æ›´ï¼‰
            if enable_ai_analysis and self.ai_analyzer:
                self.logger.info("ğŸ¤– ç¬¬2é˜¶æ®µï¼šæ‰§è¡ŒAIè¯­ä¹‰åˆ†æ...")
                
                # æå–éœ€è¦AIåˆ†æçš„L2çº§åˆ«ä¿®æ”¹
                l2_modifications = []
                for comparison in adaptive_result.get("comparison_results", []):
                    change_result = comparison.get("change_detection_result")
                    if change_result and "changes" in change_result:
                        for change in change_result["changes"]:
                            if change.get("risk_level") == "L2":
                                mod_request = ModificationAnalysisRequest(
                                    modification_id=f"{comparison.get('table_id', 'unknown')}_{change.get('row_index', 0)}_{change.get('column_name', '')}",
                                    column_name=change.get("column_name", ""),
                                    original_value=str(change.get("original_value", "")),
                                    new_value=str(change.get("new_value", "")),
                                    table_name=comparison.get("table_name", ""),
                                    row_index=change.get("row_index", 0),
                                    change_context="è‡ªåŠ¨æ£€æµ‹çš„è¡¨æ ¼ä¿®æ”¹",
                                    modification_time=datetime.now().isoformat(),
                                    modifier="system"
                                )
                                l2_modifications.append(mod_request)
                
                if l2_modifications:
                    ai_results = await self.ai_analyzer.analyze_modifications_with_caching(
                        l2_modifications, use_cache=True
                    )
                    
                    # ç”ŸæˆAIåˆ†ææ‘˜è¦
                    ai_summary = self.ai_analyzer.generate_analysis_summary(ai_results)
                    analysis_result["ai_analysis_results"] = {
                        "individual_analyses": {r.modification_id: {
                            "recommendation": r.recommendation,
                            "confidence": r.confidence,
                            "reasoning": r.reasoning,
                            "business_impact": r.business_impact,
                            "suggested_action": r.suggested_action,
                            "risk_factors": r.risk_factors
                        } for r in ai_results},
                        "summary": ai_summary
                    }
                    
                    self.processing_stats["ai_analyses_performed"] += len(ai_results)
                    self.logger.info(f"âœ… AIåˆ†æå®Œæˆï¼Œå¤„ç†äº†{len(ai_results)}ä¸ªL2çº§åˆ«ä¿®æ”¹")
                else:
                    analysis_result["ai_analysis_results"] = {"message": "æœªæ£€æµ‹åˆ°éœ€è¦AIåˆ†æçš„L2çº§åˆ«ä¿®æ”¹"}
            
            # ç¬¬3é˜¶æ®µï¼šExcel MCPå¯è§†åŒ–ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if enable_visualization:
                self.logger.info("ğŸ¨ ç¬¬3é˜¶æ®µï¼šç”ŸæˆExcelå¯è§†åŒ–æŠ¥å‘Š...")
                
                try:
                    # æ„å»ºå¯è§†åŒ–æ•°æ®
                    visualization_data = {
                        "comparison_results": adaptive_result.get("comparison_results", []),
                        "ai_analysis_results": analysis_result.get("ai_analysis_results", {}).get("individual_analyses", {}),
                        "table_metadata": {"generated_by": "Enhanced Document Manager"}
                    }
                    
                    # ç”ŸæˆExcelå¯è§†åŒ–æŠ¥å‘Š
                    output_path = os.path.join(
                        app.config['VISUALIZATION_FOLDER'], 
                        f"analysis_report_{job_id or 'manual'}_{int(time.time())}.xlsx"
                    )
                    
                    visualization_result = await self.excel_visualizer.create_comprehensive_risk_report(
                        visualization_data, output_path
                    )
                    
                    analysis_result["visualization_results"] = visualization_result
                    self.processing_stats["visualizations_created"] += 1
                    self.logger.info(f"âœ… å¯è§†åŒ–æŠ¥å‘Šç”Ÿæˆå®Œæˆ: {output_path}")
                    
                except Exception as e:
                    self.logger.error(f"âŒ å¯è§†åŒ–ç”Ÿæˆå¤±è´¥: {e}")
                    analysis_result["visualization_results"] = {"error": str(e)}
            
            # ç¬¬4é˜¶æ®µï¼šé£é™©æ±‡æ€»å’Œå»ºè®®
            self.logger.info("ğŸ“‹ ç¬¬4é˜¶æ®µï¼šç”Ÿæˆé£é™©æ±‡æ€»å’Œå»ºè®®...")
            analysis_result["risk_summary"] = self._generate_comprehensive_risk_summary(analysis_result)
            analysis_result["recommendations"] = self._generate_actionable_recommendations(analysis_result)
            
            # å¤„ç†ç»Ÿè®¡
            processing_time = time.time() - analysis_start_time
            analysis_result["processing_stats"] = {
                "total_processing_time": processing_time,
                "files_processed": len(current_tables),
                "phases_completed": 4,
                "ai_analysis_enabled": enable_ai_analysis and bool(self.ai_analyzer),
                "visualization_enabled": enable_visualization,
                "success": True
            }
            
            self.processing_stats["successful_analyses"] += 1
            self.logger.info(f"ğŸ‰ å…¨é¢æ–‡æ¡£åˆ†æå®Œæˆï¼è€—æ—¶: {processing_time:.2f}ç§’")
            
            return analysis_result
            
        except Exception as e:
            self.processing_stats["failed_analyses"] += 1
            self.logger.error(f"âŒ å…¨é¢æ–‡æ¡£åˆ†æå¤±è´¥: {e}")
            self.logger.error(traceback.format_exc())
            
            return {
                "job_id": job_id,
                "error": str(e),
                "analysis_timestamp": datetime.now().isoformat(),
                "processing_stats": {
                    "success": False,
                    "error_message": str(e),
                    "processing_time": time.time() - analysis_start_time
                }
            }
    
    def _generate_comprehensive_risk_summary(self, analysis_result: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆç»¼åˆé£é™©æ±‡æ€»"""
        
        risk_summary = {
            "overall_risk_level": "LOW",
            "critical_issues_count": 0,
            "high_risk_issues_count": 0,
            "medium_risk_issues_count": 0,
            "l1_violations": 0,
            "l2_modifications": 0,
            "l3_modifications": 0,
            "ai_high_confidence_rejections": 0,
            "data_quality_concerns": [],
            "processing_warnings": []
        }
        
        # åˆ†æè‡ªé€‚åº”å¯¹æ¯”ç»“æœ
        for comparison in analysis_result.get("adaptive_comparison_results", {}).get("comparison_results", []):
            change_result = comparison.get("change_detection_result")
            if change_result:
                risk_dist = change_result.get("risk_distribution", {})
                risk_summary["l1_violations"] += risk_dist.get("L1", 0)
                risk_summary["l2_modifications"] += risk_dist.get("L2", 0)
                risk_summary["l3_modifications"] += risk_dist.get("L3", 0)
            
            # æ£€æŸ¥æ•°æ®è´¨é‡
            std_result = comparison.get("standardization_result", {})
            quality_score = std_result.get("data_quality_score", 1.0)
            if quality_score < 0.7:
                risk_summary["data_quality_concerns"].append({
                    "table_name": comparison.get("table_name", "unknown"),
                    "quality_score": quality_score,
                    "issues": std_result.get("issues", [])[:3]  # å‰3ä¸ªé—®é¢˜
                })
        
        # åˆ†æAIç»“æœ
        ai_results = analysis_result.get("ai_analysis_results", {})
        if "individual_analyses" in ai_results:
            for mod_id, ai_result in ai_results["individual_analyses"].items():
                if ai_result["recommendation"] == "REJECT" and ai_result["confidence"] > 0.8:
                    risk_summary["ai_high_confidence_rejections"] += 1
        
        # ç¡®å®šæ•´ä½“é£é™©ç­‰çº§
        if risk_summary["l1_violations"] > 0:
            risk_summary["overall_risk_level"] = "CRITICAL"
            risk_summary["critical_issues_count"] = risk_summary["l1_violations"]
        elif risk_summary["ai_high_confidence_rejections"] > 0 or risk_summary["l2_modifications"] > 5:
            risk_summary["overall_risk_level"] = "HIGH"
            risk_summary["high_risk_issues_count"] = risk_summary["ai_high_confidence_rejections"] + min(risk_summary["l2_modifications"], 5)
        elif risk_summary["l2_modifications"] > 0 or len(risk_summary["data_quality_concerns"]) > 0:
            risk_summary["overall_risk_level"] = "MEDIUM"
            risk_summary["medium_risk_issues_count"] = risk_summary["l2_modifications"] + len(risk_summary["data_quality_concerns"])
        
        return risk_summary
    
    def _generate_actionable_recommendations(self, analysis_result: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ç”Ÿæˆå¯æ“ä½œçš„å»ºè®®"""
        
        recommendations = []
        risk_summary = analysis_result.get("risk_summary", {})
        
        # L1è¿è§„å»ºè®®
        if risk_summary.get("l1_violations", 0) > 0:
            recommendations.append({
                "priority": "critical",
                "category": "compliance",
                "title": "ä¸¥é‡è¿è§„éœ€è¦ç«‹å³å¤„ç†",
                "description": f"æ£€æµ‹åˆ°{risk_summary['l1_violations']}ä¸ªL1çº§åˆ«è¿è§„ä¿®æ”¹ï¼Œè¿™äº›å­—æ®µç»å¯¹ä¸èƒ½ä¿®æ”¹",
                "action_items": [
                    "ç«‹å³å›æ»šæ‰€æœ‰L1çº§åˆ«çš„ä¿®æ”¹",
                    "è”ç³»ç›¸å…³è´Ÿè´£äººç¡®è®¤ä¿®æ”¹æ„å›¾",
                    "æ£€æŸ¥æƒé™è®¾ç½®é˜²æ­¢æœªæˆæƒä¿®æ”¹"
                ],
                "estimated_effort": "é«˜ä¼˜å…ˆçº§ï¼Œç«‹å³å¤„ç†"
            })
        
        # AIé«˜ç½®ä¿¡åº¦æ‹’ç»å»ºè®®
        if risk_summary.get("ai_high_confidence_rejections", 0) > 0:
            recommendations.append({
                "priority": "high",
                "category": "ai_review", 
                "title": "AIå¼ºçƒˆå»ºè®®æ‹’ç»çš„ä¿®æ”¹",
                "description": f"AIç³»ç»Ÿé«˜ç½®ä¿¡åº¦å»ºè®®æ‹’ç»{risk_summary['ai_high_confidence_rejections']}ä¸ªä¿®æ”¹",
                "action_items": [
                    "äººå·¥å¤æŸ¥AIæ ‡è®°çš„é«˜é£é™©ä¿®æ”¹",
                    "åˆ†æä¿®æ”¹çš„ä¸šåŠ¡åˆç†æ€§",
                    "è€ƒè™‘å›æ»šä¸åˆç†çš„ä¿®æ”¹"
                ],
                "estimated_effort": "24å°æ—¶å†…å®Œæˆå®¡æ ¸"
            })
        
        # æ•°æ®è´¨é‡å»ºè®®
        if risk_summary.get("data_quality_concerns"):
            recommendations.append({
                "priority": "medium",
                "category": "data_quality",
                "title": "æ•°æ®è´¨é‡éœ€è¦æ”¹è¿›",
                "description": f"å‘ç°{len(risk_summary['data_quality_concerns'])}ä¸ªè¡¨æ ¼å­˜åœ¨æ•°æ®è´¨é‡é—®é¢˜",
                "action_items": [
                    "æ£€æŸ¥åŸå§‹æ•°æ®æ¥æºçš„å‡†ç¡®æ€§",
                    "æ ‡å‡†åŒ–æ•°æ®æ ¼å¼å’Œå‘½åè§„èŒƒ",
                    "å»ºç«‹æ•°æ®è´¨é‡ç›‘æ§æœºåˆ¶"
                ],
                "estimated_effort": "1-2ä¸ªå·¥ä½œæ—¥"
            })
        
        # L2ä¿®æ”¹å»ºè®®
        if risk_summary.get("l2_modifications", 0) > 0:
            recommendations.append({
                "priority": "medium",
                "category": "review",
                "title": "éœ€è¦è¯­ä¹‰å®¡æ ¸çš„ä¿®æ”¹",
                "description": f"{risk_summary['l2_modifications']}ä¸ªL2çº§åˆ«ä¿®æ”¹éœ€è¦äººå·¥ç¡®è®¤",
                "action_items": [
                    "å®‰æ’ä¸šåŠ¡ä¸“å®¶å®¡æ ¸L2çº§åˆ«ä¿®æ”¹",
                    "ç¡®è®¤ä¿®æ”¹ç¬¦åˆä¸šåŠ¡è§„åˆ™",
                    "å»ºç«‹å®šæœŸå®¡æ ¸æµç¨‹"
                ],
                "estimated_effort": "3-5ä¸ªå·¥ä½œæ—¥"
            })
        
        # ç³»ç»Ÿä¼˜åŒ–å»ºè®®
        adaptive_results = analysis_result.get("adaptive_comparison_results", {})
        avg_confidence = adaptive_results.get("processing_stats", {}).get("average_match_confidence", 1.0)
        if avg_confidence < 0.8:
            recommendations.append({
                "priority": "low",
                "category": "system_optimization",
                "title": "æå‡åˆ—åŒ¹é…å‡†ç¡®ç‡",
                "description": f"å¹³å‡åˆ—åŒ¹é…ç½®ä¿¡åº¦ä¸º{avg_confidence:.1%}ï¼Œå»ºè®®ä¼˜åŒ–",
                "action_items": [
                    "æ ‡å‡†åŒ–è¡¨æ ¼åˆ—åå‘½åè§„èŒƒ",
                    "åˆ›å»ºåˆ—åæ˜ å°„è¯å…¸",
                    "è®­ç»ƒæ™ºèƒ½åŒ¹é…ç®—æ³•"
                ],
                "estimated_effort": "é•¿æœŸä¼˜åŒ–é¡¹ç›®"
            })
        
        return recommendations


# å…¨å±€æ–‡æ¡£ç®¡ç†å™¨å®ä¾‹
doc_manager = EnhancedDocumentManager(app.config)

# ===================== æ•°æ®åº“åˆå§‹åŒ– =====================

def init_enhanced_db():
    """åˆå§‹åŒ–å¢å¼ºç‰ˆæ•°æ®åº“"""
    conn = sqlite3.connect(app.config['DATABASE'])
    c = conn.cursor()
    
    # ç”¨æˆ·è¡¨
    c.execute('''CREATE TABLE IF NOT EXISTS users
                 (id INTEGER PRIMARY KEY AUTOINCREMENT,
                  username TEXT UNIQUE NOT NULL,
                  cookie_hash TEXT NOT NULL,
                  encrypted_cookies TEXT NOT NULL,
                  created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                  last_used TIMESTAMP,
                  preferences TEXT)''')
    
    # æ–‡æ¡£è¡¨
    c.execute('''CREATE TABLE IF NOT EXISTS documents
                 (id INTEGER PRIMARY KEY AUTOINCREMENT,
                  user_id INTEGER,
                  doc_url TEXT,
                  doc_name TEXT,
                  file_path TEXT,
                  operation TEXT,
                  status TEXT DEFAULT 'processing',
                  created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                  updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                  metadata TEXT,
                  FOREIGN KEY (user_id) REFERENCES users (id))''')
    
    # åˆ†æä½œä¸šè¡¨
    c.execute('''CREATE TABLE IF NOT EXISTS analysis_jobs
                 (id INTEGER PRIMARY KEY AUTOINCREMENT,
                  job_id TEXT UNIQUE NOT NULL,
                  user_id INTEGER,
                  job_name TEXT,
                  job_type TEXT,
                  status TEXT DEFAULT 'created',
                  progress REAL DEFAULT 0.0,
                  result_path TEXT,
                  error_message TEXT,
                  created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                  started_at TIMESTAMP,
                  completed_at TIMESTAMP,
                  config TEXT,
                  FOREIGN KEY (user_id) REFERENCES users (id))''')
    
    # æ–‡ä»¶å¤„ç†è®°å½•è¡¨
    c.execute('''CREATE TABLE IF NOT EXISTS file_processing
                 (id INTEGER PRIMARY KEY AUTOINCREMENT,
                  job_id TEXT,
                  file_path TEXT,
                  file_name TEXT,
                  processing_status TEXT DEFAULT 'pending',
                  analysis_result TEXT,
                  error_message TEXT,
                  created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                  processed_at TIMESTAMP,
                  FOREIGN KEY (job_id) REFERENCES analysis_jobs(job_id))''')
    
    conn.commit()
    conn.close()

# åˆå§‹åŒ–æ•°æ®åº“
init_enhanced_db()

# ===================== å·¥å…·å‡½æ•° =====================

def require_auth(f):
    """ç®€å•çš„è®¤è¯è£…é¥°å™¨"""
    @wraps(f)
    def decorated_function(*args, **kwargs):
        username = request.json.get('username') if request.is_json else request.form.get('username')
        if not username:
            return jsonify({'error': 'éœ€è¦ç”¨æˆ·åè¿›è¡Œè®¤è¯'}), 401
        return f(*args, **kwargs)
    return decorated_function

def get_user_by_username(username: str) -> Optional[Dict[str, Any]]:
    """æ ¹æ®ç”¨æˆ·åè·å–ç”¨æˆ·ä¿¡æ¯"""
    conn = sqlite3.connect(app.config['DATABASE'])
    c = conn.cursor()
    c.execute("SELECT id, username, created_at, last_used FROM users WHERE username = ?", (username,))
    user = c.fetchone()
    conn.close()
    
    if user:
        return {
            'id': user[0],
            'username': user[1],
            'created_at': user[2],
            'last_used': user[3]
        }
    return None

def create_analysis_job(user_id: int, job_name: str, job_type: str, config: Dict[str, Any]) -> str:
    """åˆ›å»ºåˆ†æä½œä¸šè®°å½•"""
    job_id = f"{job_type}_{int(time.time())}_{secrets.token_hex(8)}"
    
    conn = sqlite3.connect(app.config['DATABASE'])
    c = conn.cursor()
    c.execute("""INSERT INTO analysis_jobs 
                 (job_id, user_id, job_name, job_type, config) 
                 VALUES (?, ?, ?, ?, ?)""",
              (job_id, user_id, job_name, job_type, json.dumps(config)))
    conn.commit()
    conn.close()
    
    return job_id

def update_job_status(job_id: str, status: str, progress: float = None, 
                     result_path: str = None, error_message: str = None):
    """æ›´æ–°ä½œä¸šçŠ¶æ€"""
    conn = sqlite3.connect(app.config['DATABASE'])
    c = conn.cursor()
    
    update_fields = ["status = ?", "updated_at = CURRENT_TIMESTAMP"]
    values = [status]
    
    if progress is not None:
        update_fields.append("progress = ?")
        values.append(progress)
    
    if result_path:
        update_fields.append("result_path = ?")
        values.append(result_path)
    
    if error_message:
        update_fields.append("error_message = ?")
        values.append(error_message)
    
    if status == 'running' and progress == 0.0:
        update_fields.append("started_at = CURRENT_TIMESTAMP")
    elif status == 'completed':
        update_fields.append("completed_at = CURRENT_TIMESTAMP")
    
    values.append(job_id)
    
    c.execute(f"UPDATE analysis_jobs SET {', '.join(update_fields)} WHERE job_id = ?", values)
    conn.commit()
    conn.close()

# ===================== APIè·¯ç”± =====================

@app.route('/')
def index():
    """ä¸»é¡µ"""
    return jsonify({
        'service': 'è…¾è®¯æ–‡æ¡£ç®¡ç†ç³»ç»Ÿ - å¢å¼ºç‰ˆAPI',
        'version': '2.0.0',
        'status': 'running',
        'features': [
            'è‡ªé€‚åº”è¡¨æ ¼å¯¹æ¯”åˆ†æ',
            'AIè¯­ä¹‰åˆ†æé›†æˆ',
            'Excel MCPå¯è§†åŒ–',
            'æ‰¹é‡æ–‡æ¡£å¤„ç†',
            'è…¾è®¯æ–‡æ¡£è‡ªåŠ¨åŒ–',
            'é£é™©è¯„ä¼°å’Œå»ºè®®'
        ],
        'endpoints': {
            'health': '/api/health',
            'analysis': '/api/v2/analysis',
            'batch_analysis': '/api/v2/batch/analysis',
            'jobs': '/api/v2/jobs',
            'upload': '/api/v2/upload',
            'download': '/api/v2/download'
        }
    })

@app.route('/api/health', methods=['GET'])
def health_check():
    """å¢å¼ºç‰ˆå¥åº·æ£€æŸ¥"""
    
    # æ£€æŸ¥æ ¸å¿ƒç»„ä»¶çŠ¶æ€
    component_status = {
        'document_analyzer': True,
        'adaptive_comparator': True,
        'column_matcher': True,
        'ai_analyzer': bool(doc_manager.ai_analyzer),
        'excel_visualizer': True,
        'database': True
    }
    
    # æ£€æŸ¥æ•°æ®åº“è¿æ¥
    try:
        conn = sqlite3.connect(app.config['DATABASE'])
        conn.execute("SELECT 1")
        conn.close()
    except Exception:
        component_status['database'] = False
    
    # ç³»ç»Ÿç»Ÿè®¡
    system_stats = doc_manager.processing_stats.copy()
    system_stats['uptime_seconds'] = (datetime.now() - system_stats['start_time']).total_seconds()
    
    overall_health = all(component_status.values())
    
    return jsonify({
        'status': 'healthy' if overall_health else 'degraded',
        'service': 'è…¾è®¯æ–‡æ¡£ç®¡ç†ç³»ç»Ÿ - å¢å¼ºç‰ˆ',
        'version': '2.0.0',
        'timestamp': datetime.now().isoformat(),
        'component_status': component_status,
        'system_stats': {
            'total_analyses': system_stats['total_analyses'],
            'successful_analyses': system_stats['successful_analyses'],
            'success_rate': system_stats['successful_analyses'] / max(1, system_stats['total_analyses']),
            'ai_analyses_performed': system_stats['ai_analyses_performed'],
            'visualizations_created': system_stats['visualizations_created'],
            'uptime_hours': round(system_stats['uptime_seconds'] / 3600, 2)
        }
    }), 200 if overall_health else 503

@app.route('/api/v2/analysis/single', methods=['POST'])
@require_auth
def single_file_analysis():
    """å•æ–‡ä»¶åˆ†æAPI"""
    try:
        # æ£€æŸ¥æ˜¯å¦æœ‰ä¸Šä¼ æ–‡ä»¶
        if 'file' not in request.files:
            return jsonify({'error': 'æ²¡æœ‰ä¸Šä¼ æ–‡ä»¶'}), 400
        
        file = request.files['file']
        username = request.form.get('username')
        enable_ai = request.form.get('enable_ai', 'true').lower() == 'true'
        enable_viz = request.form.get('enable_visualization', 'true').lower() == 'true'
        
        if file.filename == '':
            return jsonify({'error': 'æ²¡æœ‰é€‰æ‹©æ–‡ä»¶'}), 400
        
        # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶
        filename = secure_filename(file.filename)
        file_path = os.path.join(app.config['UPLOAD_FOLDER'], f"{username}_{int(time.time())}_{filename}")
        file.save(file_path)
        
        # è·å–ç”¨æˆ·ä¿¡æ¯
        user = get_user_by_username(username)
        if not user:
            return jsonify({'error': 'ç”¨æˆ·ä¸å­˜åœ¨'}), 404
        
        # åˆ›å»ºåˆ†æä½œä¸š
        config = {
            'enable_ai_analysis': enable_ai,
            'enable_visualization': enable_viz,
            'file_paths': [file_path]
        }
        job_id = create_analysis_job(user['id'], f"å•æ–‡ä»¶åˆ†æ-{filename}", "single_analysis", config)
        
        # å¼‚æ­¥æ‰§è¡Œåˆ†æ
        async def run_analysis():
            try:
                update_job_status(job_id, 'running', 0.0)
                
                result = await doc_manager.comprehensive_document_analysis(
                    file_paths=[file_path],
                    reference_file=None,
                    enable_ai_analysis=enable_ai,
                    enable_visualization=enable_viz,
                    job_id=job_id
                )
                
                # ä¿å­˜ç»“æœ
                result_path = os.path.join(app.config['ANALYSIS_FOLDER'], f"result_{job_id}.json")
                with open(result_path, 'w', encoding='utf-8') as f:
                    json.dump(result, f, ensure_ascii=False, indent=2, default=str)
                
                if result.get('processing_stats', {}).get('success', False):
                    update_job_status(job_id, 'completed', 100.0, result_path)
                else:
                    update_job_status(job_id, 'failed', 100.0, error_message=result.get('error', 'åˆ†æå¤±è´¥'))
                
            except Exception as e:
                logger.error(f"å•æ–‡ä»¶åˆ†æå¤±è´¥ {job_id}: {e}")
                update_job_status(job_id, 'failed', error_message=str(e))
        
        # åœ¨åå°è¿è¡Œåˆ†æ
        def run_async():
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            try:
                loop.run_until_complete(run_analysis())
            finally:
                loop.close()
        
        thread = threading.Thread(target=run_async, daemon=True)
        thread.start()
        
        return jsonify({
            'success': True,
            'message': 'å•æ–‡ä»¶åˆ†æä½œä¸šå·²åˆ›å»º',
            'job_id': job_id,
            'file_name': filename,
            'config': config
        })
        
    except Exception as e:
        logger.error(f"å•æ–‡ä»¶åˆ†æAPIé”™è¯¯: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/v2/analysis/batch', methods=['POST'])
@require_auth 
def batch_analysis():
    """æ‰¹é‡åˆ†æAPI"""
    try:
        if 'files' not in request.files:
            return jsonify({'error': 'æ²¡æœ‰ä¸Šä¼ æ–‡ä»¶'}), 400
        
        files = request.files.getlist('files')
        username = request.form.get('username')
        reference_file = request.files.get('reference_file')
        enable_ai = request.form.get('enable_ai', 'true').lower() == 'true'
        enable_viz = request.form.get('enable_visualization', 'true').lower() == 'true'
        job_name = request.form.get('job_name', f'æ‰¹é‡åˆ†æ-{datetime.now().strftime("%Y%m%d-%H%M%S")}')
        
        if not files or not username:
            return jsonify({'error': 'æ–‡ä»¶åˆ—è¡¨å’Œç”¨æˆ·åä¸èƒ½ä¸ºç©º'}), 400
        
        if len(files) > app.config['BATCH_SIZE']:
            return jsonify({'error': f'æ‰¹å¤„ç†æ•°é‡ä¸èƒ½è¶…è¿‡{app.config["BATCH_SIZE"]}ä¸ª'}), 400
        
        # è·å–ç”¨æˆ·ä¿¡æ¯
        user = get_user_by_username(username)
        if not user:
            return jsonify({'error': 'ç”¨æˆ·ä¸å­˜åœ¨'}), 404
        
        # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶
        file_paths = []
        for file in files:
            if file.filename != '':
                filename = secure_filename(file.filename)
                file_path = os.path.join(app.config['UPLOAD_FOLDER'], f"{username}_{int(time.time())}_{filename}")
                file.save(file_path)
                file_paths.append(file_path)
        
        # ä¿å­˜å‚è€ƒæ–‡ä»¶ï¼ˆå¦‚æœæœ‰ï¼‰
        reference_file_path = None
        if reference_file and reference_file.filename != '':
            ref_filename = secure_filename(reference_file.filename)
            reference_file_path = os.path.join(app.config['UPLOAD_FOLDER'], f"{username}_ref_{int(time.time())}_{ref_filename}")
            reference_file.save(reference_file_path)
        
        if not file_paths:
            return jsonify({'error': 'æ²¡æœ‰æœ‰æ•ˆæ–‡ä»¶'}), 400
        
        # åˆ›å»ºåˆ†æä½œä¸š
        config = {
            'enable_ai_analysis': enable_ai,
            'enable_visualization': enable_viz,
            'file_paths': file_paths,
            'reference_file': reference_file_path,
            'batch_size': len(file_paths)
        }
        job_id = create_analysis_job(user['id'], job_name, "batch_analysis", config)
        
        # å¼‚æ­¥æ‰§è¡Œæ‰¹é‡åˆ†æ
        async def run_batch_analysis():
            try:
                update_job_status(job_id, 'running', 0.0)
                
                result = await doc_manager.comprehensive_document_analysis(
                    file_paths=file_paths,
                    reference_file=reference_file_path,
                    enable_ai_analysis=enable_ai,
                    enable_visualization=enable_viz,
                    job_id=job_id
                )
                
                # ä¿å­˜ç»“æœ
                result_path = os.path.join(app.config['ANALYSIS_FOLDER'], f"batch_result_{job_id}.json")
                with open(result_path, 'w', encoding='utf-8') as f:
                    json.dump(result, f, ensure_ascii=False, indent=2, default=str)
                
                if result.get('processing_stats', {}).get('success', False):
                    update_job_status(job_id, 'completed', 100.0, result_path)
                else:
                    update_job_status(job_id, 'failed', 100.0, error_message=result.get('error', 'æ‰¹é‡åˆ†æå¤±è´¥'))
                
            except Exception as e:
                logger.error(f"æ‰¹é‡åˆ†æå¤±è´¥ {job_id}: {e}")
                update_job_status(job_id, 'failed', error_message=str(e))
        
        # åœ¨åå°è¿è¡Œåˆ†æ
        def run_async():
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            try:
                loop.run_until_complete(run_batch_analysis())
            finally:
                loop.close()
        
        thread = threading.Thread(target=run_async, daemon=True)
        thread.start()
        
        return jsonify({
            'success': True,
            'message': f'æ‰¹é‡åˆ†æä½œä¸šå·²åˆ›å»ºï¼Œå…±{len(file_paths)}ä¸ªæ–‡ä»¶',
            'job_id': job_id,
            'job_name': job_name,
            'total_files': len(file_paths),
            'config': {
                'ai_analysis_enabled': enable_ai,
                'visualization_enabled': enable_viz,
                'has_reference_file': bool(reference_file_path)
            }
        })
        
    except Exception as e:
        logger.error(f"æ‰¹é‡åˆ†æAPIé”™è¯¯: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/v2/jobs/<job_id>/status', methods=['GET'])
def get_job_status(job_id):
    """è·å–ä½œä¸šçŠ¶æ€"""
    try:
        conn = sqlite3.connect(app.config['DATABASE'])
        c = conn.cursor()
        c.execute("""SELECT job_id, job_name, job_type, status, progress, 
                            result_path, error_message, created_at, started_at, completed_at
                     FROM analysis_jobs WHERE job_id = ?""", (job_id,))
        job = c.fetchone()
        conn.close()
        
        if not job:
            return jsonify({'error': 'ä½œä¸šä¸å­˜åœ¨'}), 404
        
        status_info = {
            'job_id': job[0],
            'job_name': job[1],
            'job_type': job[2],
            'status': job[3],
            'progress': job[4],
            'result_path': job[5],
            'error_message': job[6],
            'created_at': job[7],
            'started_at': job[8],
            'completed_at': job[9]
        }
        
        # è®¡ç®—å¤„ç†æ—¶é—´
        if job[8] and job[9]:  # started_at and completed_at
            start_time = datetime.fromisoformat(job[8])
            end_time = datetime.fromisoformat(job[9])
            status_info['processing_time_seconds'] = (end_time - start_time).total_seconds()
        elif job[8]:  # only started_at
            start_time = datetime.fromisoformat(job[8])
            status_info['elapsed_time_seconds'] = (datetime.now() - start_time).total_seconds()
        
        return jsonify({
            'success': True,
            'job_status': status_info
        })
        
    except Exception as e:
        logger.error(f"è·å–ä½œä¸šçŠ¶æ€é”™è¯¯: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/v2/jobs/<job_id>/result', methods=['GET'])
def get_job_result(job_id):
    """è·å–ä½œä¸šç»“æœ"""
    try:
        conn = sqlite3.connect(app.config['DATABASE'])
        c = conn.cursor()
        c.execute("SELECT result_path, status FROM analysis_jobs WHERE job_id = ?", (job_id,))
        job = c.fetchone()
        conn.close()
        
        if not job:
            return jsonify({'error': 'ä½œä¸šä¸å­˜åœ¨'}), 404
        
        if job[1] != 'completed':
            return jsonify({'error': 'ä½œä¸šå°šæœªå®Œæˆ'}), 400
        
        if not job[0] or not os.path.exists(job[0]):
            return jsonify({'error': 'ç»“æœæ–‡ä»¶ä¸å­˜åœ¨'}), 404
        
        # è¯»å–ç»“æœæ–‡ä»¶
        with open(job[0], 'r', encoding='utf-8') as f:
            result = json.load(f)
        
        return jsonify({
            'success': True,
            'job_id': job_id,
            'result': result
        })
        
    except Exception as e:
        logger.error(f"è·å–ä½œä¸šç»“æœé”™è¯¯: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/v2/jobs/<job_id>/download', methods=['GET'])
def download_job_result(job_id):
    """ä¸‹è½½ä½œä¸šç»“æœæ–‡ä»¶"""
    try:
        conn = sqlite3.connect(app.config['DATABASE'])
        c = conn.cursor()
        c.execute("SELECT result_path, status FROM analysis_jobs WHERE job_id = ?", (job_id,))
        job = c.fetchone()
        conn.close()
        
        if not job:
            return jsonify({'error': 'ä½œä¸šä¸å­˜åœ¨'}), 404
        
        if job[1] != 'completed':
            return jsonify({'error': 'ä½œä¸šå°šæœªå®Œæˆ'}), 400
        
        result_path = job[0]
        if not result_path or not os.path.exists(result_path):
            return jsonify({'error': 'ç»“æœæ–‡ä»¶ä¸å­˜åœ¨'}), 404
        
        # å¦‚æœæ˜¯Excelæ–‡ä»¶ï¼Œç›´æ¥æä¾›ä¸‹è½½
        if result_path.endswith('.xlsx'):
            return send_file(result_path, as_attachment=True, 
                           download_name=f"analysis_report_{job_id}.xlsx")
        
        # å¦‚æœæ˜¯JSONç»“æœæ–‡ä»¶ï¼Œè¯»å–å¹¶æŸ¥æ‰¾ExcelæŠ¥å‘Šè·¯å¾„
        try:
            with open(result_path, 'r', encoding='utf-8') as f:
                result = json.load(f)
            
            excel_path = result.get('visualization_results', {}).get('report_path')
            if excel_path and os.path.exists(excel_path):
                return send_file(excel_path, as_attachment=True,
                               download_name=f"analysis_report_{job_id}.xlsx")
            else:
                # è¿”å›JSONæ–‡ä»¶
                return send_file(result_path, as_attachment=True,
                               download_name=f"analysis_result_{job_id}.json")
        
        except json.JSONDecodeError:
            return send_file(result_path, as_attachment=True,
                           download_name=f"analysis_result_{job_id}.txt")
        
    except Exception as e:
        logger.error(f"ä¸‹è½½ä½œä¸šç»“æœé”™è¯¯: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/v2/jobs', methods=['GET'])
def list_jobs():
    """åˆ—å‡ºä½œä¸š"""
    try:
        username = request.args.get('username')
        status = request.args.get('status')  # å¯é€‰çŠ¶æ€è¿‡æ»¤
        limit = int(request.args.get('limit', 50))
        
        conn = sqlite3.connect(app.config['DATABASE'])
        c = conn.cursor()
        
        query = """SELECT j.job_id, j.job_name, j.job_type, j.status, j.progress,
                          j.created_at, j.started_at, j.completed_at, u.username
                   FROM analysis_jobs j 
                   JOIN users u ON j.user_id = u.id"""
        params = []
        
        conditions = []
        if username:
            conditions.append("u.username = ?")
            params.append(username)
        
        if status:
            conditions.append("j.status = ?")
            params.append(status)
        
        if conditions:
            query += " WHERE " + " AND ".join(conditions)
        
        query += " ORDER BY j.created_at DESC LIMIT ?"
        params.append(limit)
        
        c.execute(query, params)
        jobs = c.fetchall()
        conn.close()
        
        job_list = []
        for job in jobs:
            job_info = {
                'job_id': job[0],
                'job_name': job[1],
                'job_type': job[2],
                'status': job[3],
                'progress': job[4],
                'created_at': job[5],
                'started_at': job[6],
                'completed_at': job[7],
                'username': job[8]
            }
            job_list.append(job_info)
        
        return jsonify({
            'success': True,
            'jobs': job_list,
            'total_count': len(job_list)
        })
        
    except Exception as e:
        logger.error(f"åˆ—å‡ºä½œä¸šé”™è¯¯: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/v2/system/stats', methods=['GET'])
def system_statistics():
    """ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯"""
    try:
        # è·å–æ•°æ®åº“ç»Ÿè®¡
        conn = sqlite3.connect(app.config['DATABASE'])
        c = conn.cursor()
        
        # ç”¨æˆ·ç»Ÿè®¡
        c.execute("SELECT COUNT(*) FROM users")
        total_users = c.fetchone()[0]
        
        # ä½œä¸šç»Ÿè®¡
        c.execute("SELECT status, COUNT(*) FROM analysis_jobs GROUP BY status")
        job_stats = dict(c.fetchall())
        
        c.execute("SELECT COUNT(*) FROM analysis_jobs WHERE created_at > datetime('now', '-24 hours')")
        jobs_last_24h = c.fetchone()[0]
        
        c.execute("SELECT COUNT(*) FROM analysis_jobs WHERE created_at > datetime('now', '-7 days')")
        jobs_last_7d = c.fetchone()[0]
        
        conn.close()
        
        # è·å–æ–‡æ¡£ç®¡ç†å™¨ç»Ÿè®¡
        manager_stats = doc_manager.processing_stats.copy()
        uptime_seconds = (datetime.now() - manager_stats['start_time']).total_seconds()
        
        return jsonify({
            'success': True,
            'system_statistics': {
                'service_info': {
                    'version': '2.0.0',
                    'uptime_hours': round(uptime_seconds / 3600, 2),
                    'start_time': manager_stats['start_time'].isoformat()
                },
                'users': {
                    'total_registered': total_users
                },
                'jobs': {
                    'total_jobs': sum(job_stats.values()),
                    'status_distribution': job_stats,
                    'jobs_last_24h': jobs_last_24h,
                    'jobs_last_7d': jobs_last_7d
                },
                'processing': {
                    'total_analyses': manager_stats['total_analyses'],
                    'successful_analyses': manager_stats['successful_analyses'],
                    'failed_analyses': manager_stats['failed_analyses'],
                    'success_rate': manager_stats['successful_analyses'] / max(1, manager_stats['total_analyses']),
                    'ai_analyses_performed': manager_stats['ai_analyses_performed'],
                    'visualizations_created': manager_stats['visualizations_created']
                },
                'components': {
                    'ai_analyzer_available': bool(doc_manager.ai_analyzer),
                    'claude_api_configured': bool(app.config.get('CLAUDE_API_KEY')),
                    'excel_visualizer_available': True,
                    'adaptive_comparator_available': True
                }
            }
        })
        
    except Exception as e:
        logger.error(f"è·å–ç³»ç»Ÿç»Ÿè®¡é”™è¯¯: {e}")
        return jsonify({'error': str(e)}), 500

# ===================== å…¼å®¹æ€§APIï¼ˆä¿æŒåŸæœ‰æ¥å£ï¼‰ =====================

@app.route('/api/save_cookies', methods=['POST'])
def save_cookies():
    """ä¿å­˜ç”¨æˆ·cookiesï¼ˆå…¼å®¹åŸAPIï¼‰"""
    try:
        data = request.json
        username = data.get('username')
        cookies = data.get('cookies')
        
        if not username or not cookies:
            return jsonify({'error': 'ç”¨æˆ·åå’Œcookiesä¸èƒ½ä¸ºç©º'}), 400
        
        # ä½¿ç”¨ç®€å•çš„åŠ å¯†
        user_key = username + app.config['SECRET_KEY']
        cookie_hash = hashlib.sha256(cookies.encode()).hexdigest()
        
        # ç®€å•çš„XORåŠ å¯†
        key_hash = hashlib.sha256(user_key.encode()).hexdigest()
        encrypted = []
        for i, char in enumerate(cookies):
            encrypted.append(chr(ord(char) ^ ord(key_hash[i % len(key_hash)])))
        encrypted_cookies = ''.join(encrypted)
        
        conn = sqlite3.connect(app.config['DATABASE'])
        c = conn.cursor()
        
        # æ£€æŸ¥ç”¨æˆ·æ˜¯å¦å­˜åœ¨
        c.execute("SELECT id FROM users WHERE username = ?", (username,))
        user = c.fetchone()
        
        if user:
            c.execute("""UPDATE users SET cookie_hash = ?, encrypted_cookies = ?, 
                        last_used = CURRENT_TIMESTAMP WHERE username = ?""",
                     (cookie_hash, encrypted_cookies, username))
        else:
            c.execute("""INSERT INTO users (username, cookie_hash, encrypted_cookies) 
                        VALUES (?, ?, ?)""",
                     (username, cookie_hash, encrypted_cookies))
        
        conn.commit()
        conn.close()
        
        return jsonify({
            'success': True,
            'message': 'Cookiesä¿å­˜æˆåŠŸ',
            'username': username
        })
        
    except Exception as e:
        return jsonify({'error': str(e)}), 500

# ===================== å¯åŠ¨æœåŠ¡ =====================

if __name__ == '__main__':
    logger.info("ğŸš€ å¯åŠ¨è…¾è®¯æ–‡æ¡£ç®¡ç†ç³»ç»Ÿ - å¢å¼ºç‰ˆAPIæœåŠ¡å™¨")
    logger.info("ğŸ“‹ é›†æˆç»„ä»¶:")
    logger.info("   âœ“ è‡ªé€‚åº”è¡¨æ ¼å¯¹æ¯”åˆ†æ")
    logger.info("   âœ“ æ™ºèƒ½åˆ—åŒ¹é…")
    if doc_manager.ai_analyzer:
        logger.info("   âœ“ AIè¯­ä¹‰åˆ†æ (Claude)")
    else:
        logger.info("   âš  AIè¯­ä¹‰åˆ†æ (æœªé…ç½®APIå¯†é’¥)")
    logger.info("   âœ“ Excel MCPå¯è§†åŒ–")
    logger.info("   âœ“ æ‰¹é‡æ–‡æ¡£å¤„ç†")
    logger.info("   âœ“ é£é™©è¯„ä¼°å’Œå»ºè®®")
    
    app.run(
        debug=False,
        host='0.0.0.0',
        port=5000,
        threaded=True
    )