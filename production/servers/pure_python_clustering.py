#!/usr/bin/env python3
"""
çº¯Pythonå®ç°çš„çƒ­åŠ›å›¾èšç±»ç®—æ³•
ä¸ä¾èµ–numpy/scipyï¼Œä½¿ç”¨æ ‡å‡†åº“å®ç°
"""

from typing import List, Tuple
import math
import random


class PurePythonClustering:
    """çº¯Pythonå®ç°çš„èšç±»ç®—æ³•"""

    def __init__(self):
        self.distance_cache = {}

    def euclidean_distance(self, vec1: List[float], vec2: List[float]) -> float:
        """è®¡ç®—æ¬§æ°è·ç¦»"""
        return math.sqrt(sum((a - b) ** 2 for a, b in zip(vec1, vec2)))

    def correlation_distance(self, vec1: List[float], vec2: List[float]) -> float:
        """è®¡ç®—ç›¸å…³æ€§è·ç¦»ï¼ˆ1 - ç›¸å…³ç³»æ•°ï¼‰"""
        n = len(vec1)
        if n == 0:
            return 1.0

        # è®¡ç®—å‡å€¼
        mean1 = sum(vec1) / n
        mean2 = sum(vec2) / n

        # è®¡ç®—ç›¸å…³ç³»æ•°
        numerator = sum((v1 - mean1) * (v2 - mean2) for v1, v2 in zip(vec1, vec2))
        denominator1 = math.sqrt(sum((v - mean1) ** 2 for v in vec1))
        denominator2 = math.sqrt(sum((v - mean2) ** 2 for v in vec2))

        if denominator1 == 0 or denominator2 == 0:
            return 1.0

        correlation = numerator / (denominator1 * denominator2)
        # è½¬æ¢ä¸ºè·ç¦»ï¼ˆ0è¡¨ç¤ºå®Œå…¨ç›¸å…³ï¼Œ2è¡¨ç¤ºå®Œå…¨è´Ÿç›¸å…³ï¼‰
        return 1 - correlation

    def find_similar_pairs(self, vectors: List[List[float]], use_correlation: bool = True) -> List[Tuple[int, int, float]]:
        """æ‰¾åˆ°æœ€ç›¸ä¼¼çš„å‘é‡å¯¹"""
        pairs = []
        n = len(vectors)

        for i in range(n):
            for j in range(i + 1, n):
                if use_correlation:
                    dist = self.correlation_distance(vectors[i], vectors[j])
                else:
                    dist = self.euclidean_distance(vectors[i], vectors[j])
                pairs.append((i, j, dist))

        # æŒ‰è·ç¦»æ’åº
        pairs.sort(key=lambda x: x[2])
        return pairs

    def hierarchical_clustering(self, vectors: List[List[float]]) -> List[int]:
        """
        ç®€åŒ–çš„å±‚æ¬¡èšç±»
        è¿”å›é‡æ’åºçš„ç´¢å¼•
        """
        n = len(vectors)
        if n <= 1:
            return list(range(n))

        # åˆå§‹åŒ–ï¼šæ¯ä¸ªå‘é‡æ˜¯ä¸€ä¸ªç°‡
        clusters = [[i] for i in range(n)]
        cluster_vectors = [vectors[i][:] for i in range(n)]

        # èšç±»è¿‡ç¨‹
        while len(clusters) > 1:
            # æ‰¾åˆ°æœ€ç›¸ä¼¼çš„ä¸¤ä¸ªç°‡
            min_dist = float('inf')
            merge_i, merge_j = 0, 1

            for i in range(len(clusters)):
                for j in range(i + 1, len(clusters)):
                    dist = self.correlation_distance(cluster_vectors[i], cluster_vectors[j])
                    if dist < min_dist:
                        min_dist = dist
                        merge_i, merge_j = i, j

            # åˆå¹¶ç°‡
            new_cluster = clusters[merge_i] + clusters[merge_j]

            # è®¡ç®—æ–°ç°‡çš„ä¸­å¿ƒï¼ˆå¹³å‡å‘é‡ï¼‰
            new_vector = []
            for k in range(len(cluster_vectors[0])):
                avg = (cluster_vectors[merge_i][k] * len(clusters[merge_i]) +
                       cluster_vectors[merge_j][k] * len(clusters[merge_j])) / len(new_cluster)
                new_vector.append(avg)

            # æ›´æ–°ç°‡åˆ—è¡¨ï¼ˆåˆ é™¤æ—§çš„ï¼Œæ·»åŠ æ–°çš„ï¼‰
            clusters = [c for idx, c in enumerate(clusters) if idx not in (merge_i, merge_j)]
            clusters.append(new_cluster)

            cluster_vectors = [v for idx, v in enumerate(cluster_vectors) if idx not in (merge_i, merge_j)]
            cluster_vectors.append(new_vector)

        # è¿”å›æœ€ç»ˆçš„é¡ºåº
        return clusters[0] if clusters else list(range(n))

    def smooth_distance(self, vec1: List[float], vec2: List[float]) -> float:
        """è®¡ç®—å¹³æ»‘è·ç¦»ï¼Œè€ƒè™‘å€¼çš„è¿ç»­æ€§"""
        # ç»„åˆç›¸å…³æ€§å’Œæ¬§æ°è·ç¦»ï¼Œæ›´é‡è§†å¹³æ»‘è¿‡æ¸¡
        corr_dist = self.correlation_distance(vec1, vec2)

        # è®¡ç®—å€¼å˜åŒ–çš„å¹³æ»‘åº¦ï¼ˆç›¸é‚»å€¼çš„å·®å¼‚ï¼‰
        smoothness = 0
        for i in range(len(vec1) - 1):
            smoothness += abs((vec1[i+1] - vec1[i]) - (vec2[i+1] - vec2[i]))
        smoothness = smoothness / (len(vec1) - 1) if len(vec1) > 1 else 0

        # ç»„åˆè·ç¦»ï¼Œæ›´é‡è§†ç›¸å…³æ€§
        return corr_dist * 0.7 + smoothness * 0.3

    def calculate_continuity_score(self, vectors: List[List[float]], order: List[int]) -> float:
        """è®¡ç®—æ’åºçš„è¿ç»­æ€§å¾—åˆ†ï¼ˆè¶Šä½è¶Šå¥½ï¼‰"""
        if len(order) <= 1:
            return 0.0

        total_distance = 0
        for i in range(len(order) - 1):
            dist = self.smooth_distance(vectors[order[i]], vectors[order[i+1]])
            total_distance += dist

        return total_distance

    def two_opt_improve(self, vectors: List[List[float]], order: List[int]) -> List[int]:
        """ä½¿ç”¨2-optç®—æ³•æ”¹å–„æ’åºï¼Œå¢å¼ºè¿ç»­æ€§"""
        improved = order[:]
        n = len(improved)
        improvement = True

        while improvement:
            improvement = False
            best_score = self.calculate_continuity_score(vectors, improved)

            for i in range(n - 1):
                for j in range(i + 2, min(i + 10, n)):  # é™åˆ¶æœç´¢èŒƒå›´æé«˜æ•ˆç‡
                    # å°è¯•åè½¬iåˆ°jä¹‹é—´çš„é¡ºåº
                    new_order = improved[:i] + improved[i:j][::-1] + improved[j:]
                    new_score = self.calculate_continuity_score(vectors, new_order)

                    if new_score < best_score:
                        improved = new_order
                        best_score = new_score
                        improvement = True
                        break
                if improvement:
                    break

        return improved

    def simulated_annealing_order(self, vectors: List[List[float]], initial_order: List[int] = None, iterations: int = 1000) -> List[int]:
        """ä½¿ç”¨æ¨¡æ‹Ÿé€€ç«ç®—æ³•ä¼˜åŒ–æ’åºï¼Œåˆ›å»ºæ›´å¹³æ»‘çš„è¿‡æ¸¡"""
        n = len(vectors)
        if n <= 1:
            return list(range(n))

        # åˆå§‹åŒ–
        if initial_order is None:
            # ä½¿ç”¨è´ªå¿ƒç®—æ³•çš„ç»“æœä½œä¸ºåˆå§‹è§£
            initial_order = self.reorder_by_similarity_greedy(vectors)

        current_order = initial_order[:]
        best_order = current_order[:]
        current_score = self.calculate_continuity_score(vectors, current_order)
        best_score = current_score

        # æ¸©åº¦å‚æ•°
        temperature = 1.0
        cooling_rate = 0.995
        min_temperature = 0.01

        for iteration in range(iterations):
            if temperature < min_temperature:
                break

            # ç”Ÿæˆé‚»åŸŸè§£ï¼ˆäº¤æ¢ä¸¤ä¸ªä½ç½®æˆ–åè½¬ä¸€å°æ®µï¼‰
            new_order = current_order[:]

            if random.random() < 0.5:
                # äº¤æ¢ä¸¤ä¸ªéšæœºä½ç½®
                i, j = random.sample(range(n), 2)
                new_order[i], new_order[j] = new_order[j], new_order[i]
            else:
                # åè½¬ä¸€å°æ®µ
                i = random.randint(0, n - 2)
                j = min(i + random.randint(2, 5), n)
                new_order[i:j] = new_order[i:j][::-1]

            # è®¡ç®—æ–°è§£çš„å¾—åˆ†
            new_score = self.calculate_continuity_score(vectors, new_order)

            # å†³å®šæ˜¯å¦æ¥å—æ–°è§£
            delta = new_score - current_score
            if delta < 0 or random.random() < math.exp(-delta / temperature):
                current_order = new_order
                current_score = new_score

                if new_score < best_score:
                    best_order = new_order[:]
                    best_score = new_score

            # é™æ¸©
            temperature *= cooling_rate

        # æœ€åç”¨2-optè¿›ä¸€æ­¥ä¼˜åŒ–
        best_order = self.two_opt_improve(vectors, best_order)

        return best_order

    def reorder_by_similarity_greedy(self, vectors: List[List[float]]) -> List[int]:
        """åŸå§‹çš„è´ªå¿ƒé‡æ’åºç®—æ³•ï¼ˆä½œä¸ºåˆå§‹è§£ï¼‰"""
        n = len(vectors)
        if n <= 1:
            return list(range(n))

        # è®¡ç®—æ¯ä¸ªå‘é‡çš„æ€»çƒ­åº¦
        heat_scores = [sum(v) for v in vectors]

        # ä»çƒ­åº¦æœ€é«˜çš„å¼€å§‹
        remaining = set(range(n))
        ordered = []

        # é€‰æ‹©èµ·å§‹ç‚¹ï¼ˆçƒ­åº¦æœ€é«˜ï¼‰
        start_idx = max(remaining, key=lambda i: heat_scores[i])
        ordered.append(start_idx)
        remaining.remove(start_idx)

        # è´ªå¿ƒæ·»åŠ æœ€ç›¸ä¼¼çš„å‘é‡
        while remaining:
            last_vec = vectors[ordered[-1]]
            best_idx = None
            best_dist = float('inf')

            for idx in remaining:
                dist = self.smooth_distance(last_vec, vectors[idx])
                if dist < best_dist:
                    best_dist = dist
                    best_idx = idx

            ordered.append(best_idx)
            remaining.remove(best_idx)

        return ordered

    def reorder_by_similarity(self, vectors: List[List[float]]) -> List[int]:
        """
        å¢å¼ºçš„é‡æ’åºç®—æ³•ï¼Œä½¿ç”¨æ¨¡æ‹Ÿé€€ç«ä¼˜åŒ–è¿ç»­æ€§
        """
        # å…ˆç”¨è´ªå¿ƒç®—æ³•è·å¾—åˆå§‹è§£
        initial_order = self.reorder_by_similarity_greedy(vectors)

        # ä½¿ç”¨æ¨¡æ‹Ÿé€€ç«ä¼˜åŒ–
        optimized_order = self.simulated_annealing_order(vectors, initial_order, iterations=500)

        return optimized_order

    def cluster_heatmap(self, matrix: List[List[float]]) -> Tuple[List[int], List[int]]:
        """
        å¯¹çƒ­åŠ›å›¾çŸ©é˜µè¿›è¡ŒåŒå‘èšç±»
        è¿”å›ï¼š(è¡Œé¡ºåº, åˆ—é¡ºåº)
        """
        if not matrix or not matrix[0]:
            return [], []

        n_rows = len(matrix)
        n_cols = len(matrix[0])

        # 1. å¯¹åˆ—è¿›è¡Œèšç±»ï¼ˆè½¬ç½®çŸ©é˜µï¼‰
        col_vectors = []
        for j in range(n_cols):
            col_vec = [matrix[i][j] for i in range(n_rows)]
            col_vectors.append(col_vec)

        col_order = self.reorder_by_similarity(col_vectors)

        # 2. æ ¹æ®åˆ—é¡ºåºé‡æ’çŸ©é˜µ
        reordered_matrix = []
        for row in matrix:
            reordered_row = [row[j] for j in col_order]
            reordered_matrix.append(reordered_row)

        # 3. å¯¹è¡Œè¿›è¡Œèšç±»
        row_order = self.reorder_by_similarity(reordered_matrix)

        return row_order, col_order

    def group_by_heat_level(self, vectors: List[List[float]]) -> List[List[int]]:
        """æŒ‰çƒ­åº¦çº§åˆ«åˆ†ç»„ - ä½¿ç”¨è‡ªé€‚åº”é˜ˆå€¼"""
        n = len(vectors)
        if n <= 1:
            return [list(range(n))]

        # è®¡ç®—æ¯ä¸ªå‘é‡çš„å¹³å‡çƒ­åº¦
        heat_levels = [(i, sum(v) / len(v)) for i, v in enumerate(vectors)]

        # æŒ‰çƒ­åº¦æ’åº
        heat_levels.sort(key=lambda x: x[1], reverse=True)

        # è®¡ç®—çƒ­åº¦å·®å¼‚ï¼Œæ‰¾åˆ°è‡ªç„¶æ–­ç‚¹
        diffs = []
        for i in range(1, len(heat_levels)):
            diff = heat_levels[i-1][1] - heat_levels[i][1]
            diffs.append((i, diff))

        # æ‰¾åˆ°æœ€å¤§çš„å‡ ä¸ªå·®å¼‚ä½œä¸ºåˆ†ç»„è¾¹ç•Œ
        diffs.sort(key=lambda x: x[1], reverse=True)

        # ä½¿ç”¨å‰2-3ä¸ªæœ€å¤§å·®å¼‚ä½œä¸ºåˆ†ç»„è¾¹ç•Œï¼ˆæœ€å¤šåˆ†4ç»„ï¼‰
        split_points = []
        avg_diff = sum(d[1] for d in diffs) / len(diffs) if diffs else 0

        for idx, diff in diffs[:3]:  # æœ€å¤šå–3ä¸ªåˆ†å‰²ç‚¹
            if diff > avg_diff * 1.5:  # å·®å¼‚è¦æ˜¾è‘—å¤§äºå¹³å‡å€¼
                split_points.append(idx)

        split_points.sort()

        # æ ¹æ®åˆ†å‰²ç‚¹åˆ›å»ºç»„
        groups = []
        start = 0

        for split in split_points:
            group = [heat_levels[i][0] for i in range(start, split)]
            if group:
                groups.append(group)
            start = split

        # æ·»åŠ æœ€åä¸€ç»„
        if start < len(heat_levels):
            group = [heat_levels[i][0] for i in range(start, len(heat_levels))]
            if group:
                groups.append(group)

        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ˜æ˜¾çš„åˆ†ç»„ï¼Œè‡³å°‘åˆ†æˆé«˜ä¸­ä½ä¸‰ç»„
        if len(groups) <= 1:
            third = len(heat_levels) // 3
            groups = []
            groups.append([heat_levels[i][0] for i in range(third)])
            groups.append([heat_levels[i][0] for i in range(third, 2*third)])
            groups.append([heat_levels[i][0] for i in range(2*third, len(heat_levels))])
            groups = [g for g in groups if g]  # ç§»é™¤ç©ºç»„

        return groups

    def hierarchical_reorder(self, vectors: List[List[float]]) -> List[int]:
        """åˆ†å±‚é‡æ’åºï¼šå…ˆåˆ†ç»„ï¼Œå†ç»„å†…ä¼˜åŒ–"""
        # 1. æŒ‰çƒ­åº¦çº§åˆ«åˆ†ç»„
        groups = self.group_by_heat_level(vectors)

        print(f"  - æ£€æµ‹åˆ° {len(groups)} ä¸ªçƒ­åº¦å±‚çº§")
        for i, group in enumerate(groups):
            avg_heat = sum(sum(vectors[idx]) / len(vectors[idx]) for idx in group) / len(group)
            print(f"    å±‚çº§{i+1}: {len(group)}ä¸ªé¡¹ç›®, å¹³å‡çƒ­åº¦{avg_heat:.2f}")

        # 2. å¯¹æ¯ä¸ªç»„å†…çš„å‘é‡è¿›è¡Œç›¸ä¼¼æ€§ä¼˜åŒ–
        ordered = []
        for group in groups:
            if len(group) <= 1:
                ordered.extend(group)
            else:
                # è·å–ç»„å†…å‘é‡
                group_vectors = [vectors[i] for i in group]

                # ç»„å†…ä¼˜åŒ–æ’åº
                group_order = self.simulated_annealing_order(group_vectors, iterations=200)

                # æ˜ å°„å›åŸå§‹ç´¢å¼•
                ordered.extend([group[i] for i in group_order])

        return ordered

    def optimize_block_diagonal(self, matrix: List[List[float]]) -> Tuple[List[int], List[int]]:
        """
        ä¼˜åŒ–å—å¯¹è§’ç»“æ„ - ä½¿ç”¨åˆ†å±‚èšç±»
        ç¡®ä¿ç›¸ä¼¼çƒ­åº¦çš„å—èšé›†åœ¨ä¸€èµ·
        """
        n_rows = len(matrix)
        n_cols = len(matrix[0]) if matrix else 0

        # 1. å¯¹åˆ—è¿›è¡Œåˆ†å±‚èšç±»
        col_vectors = []
        for j in range(n_cols):
            col_vec = [matrix[i][j] for i in range(n_rows)]
            col_vectors.append(col_vec)

        print("ğŸ“Š åˆ—èšç±»åˆ†æ:")
        col_order = self.hierarchical_reorder(col_vectors)

        # 2. æ ¹æ®åˆ—é¡ºåºé‡æ’çŸ©é˜µ
        reordered_matrix = []
        for row in matrix:
            reordered_row = [row[j] for j in col_order]
            reordered_matrix.append(reordered_row)

        # 3. å¯¹è¡Œè¿›è¡Œåˆ†å±‚èšç±»
        print("ğŸ“Š è¡Œèšç±»åˆ†æ:")
        row_order = self.hierarchical_reorder(reordered_matrix)

        # 4. æ£€æŸ¥ä¸åŒæ–¹å‘ï¼Œé€‰æ‹©æœ€ä¼˜
        configurations = [
            (row_order, col_order),
            (row_order[::-1], col_order),
            (row_order, col_order[::-1]),
            (row_order[::-1], col_order[::-1])
        ]

        best_score = -1
        best_config = configurations[0]

        for r_order, c_order in configurations:
            # è®¡ç®—çƒ­å—èšé›†åº¦
            score = 0

            # æ£€æŸ¥é«˜çƒ­åº¦åŒºåŸŸçš„èšé›†ç¨‹åº¦
            for i in range(min(n_rows // 2, len(r_order))):
                for j in range(min(n_cols // 2, len(c_order))):
                    value = matrix[r_order[i]][c_order[j]]
                    # é«˜çƒ­åº¦å€¼æƒé‡æ›´é«˜
                    if value > 0.7:
                        score += value * 3
                    elif value > 0.5:
                        score += value * 2
                    else:
                        score += value

            if score > best_score:
                best_score = score
                best_config = (r_order, c_order)

        return best_config


def apply_pure_clustering(heatmap_data: List[List[float]],
                         table_names: List[str],
                         column_names: List[str]) -> Tuple[List[List[float]], List[str], List[str], List[int], List[int]]:
    """
    åº”ç”¨çº¯Pythonèšç±»ç®—æ³•

    å‚æ•°ï¼š
        heatmap_data: çƒ­åŠ›å›¾çŸ©é˜µ
        table_names: è¡¨æ ¼åç§°åˆ—è¡¨
        column_names: åˆ—ååˆ—è¡¨

    è¿”å›ï¼š
        (èšç±»åçš„çŸ©é˜µ, é‡æ’çš„è¡¨æ ¼å, é‡æ’çš„åˆ—å, è¡Œé¡ºåº, åˆ—é¡ºåº)
    """
    clustering = PurePythonClustering()

    # æ‰§è¡Œèšç±»
    row_order, col_order = clustering.optimize_block_diagonal(heatmap_data)

    # åº”ç”¨é‡æ’åº
    clustered_matrix = []
    reordered_table_names = []

    for row_idx in row_order:
        reordered_row = [heatmap_data[row_idx][col_idx] for col_idx in col_order]
        clustered_matrix.append(reordered_row)
        reordered_table_names.append(table_names[row_idx])

    reordered_column_names = [column_names[i] for i in col_order]

    print(f"âœ… çº¯Pythonå¢å¼ºèšç±»å®Œæˆ:")
    print(f"  - ä½¿ç”¨æ¨¡æ‹Ÿé€€ç«ç®—æ³•ä¼˜åŒ–è¿ç»­æ€§")
    print(f"  - è¡Œé¡ºåº: {row_order[:5]}... -> {row_order[-5:]}")
    print(f"  - åˆ—é¡ºåº: {col_order[:5]}... -> {col_order[-5:]}")

    # è®¡ç®—èšç±»æ”¹å–„åº¦
    improvement = calculate_improvement(heatmap_data, clustered_matrix)
    continuity_improvement = calculate_continuity_improvement(heatmap_data, clustered_matrix)
    print(f"  - å—å¯¹è§’æ€§æ”¹å–„: {improvement:.1f}%")
    print(f"  - è¿ç»­æ€§æ”¹å–„: {continuity_improvement:.1f}%")

    return clustered_matrix, reordered_table_names, reordered_column_names, row_order, col_order


def calculate_improvement(original: List[List[float]], clustered: List[List[float]]) -> float:
    """è®¡ç®—èšç±»æ”¹å–„ç¨‹åº¦"""
    if not original or not original[0]:
        return 0.0

    def diagonal_score(matrix):
        score = 0
        min_dim = min(len(matrix), len(matrix[0]))
        for i in range(min_dim):
            for j in range(min_dim):
                if abs(i - j) <= 1:  # å¯¹è§’çº¿é™„è¿‘
                    score += matrix[i][j] * (2 - abs(i - j))
        return score

    original_score = diagonal_score(original)
    clustered_score = diagonal_score(clustered)

    if original_score == 0:
        return 0.0

    return (clustered_score - original_score) / original_score * 100


def calculate_continuity_improvement(original: List[List[float]], clustered: List[List[float]]) -> float:
    """è®¡ç®—è¿ç»­æ€§æ”¹å–„ç¨‹åº¦ï¼ˆç›¸é‚»è¡Œ/åˆ—çš„ç›¸ä¼¼åº¦ï¼‰"""
    if not original or not original[0] or len(original) < 2:
        return 0.0

    def continuity_score(matrix):
        """è®¡ç®—çŸ©é˜µçš„è¿ç»­æ€§å¾—åˆ†ï¼ˆè¶Šä½è¶Šå¥½ï¼‰"""
        score = 0
        n_rows = len(matrix)
        n_cols = len(matrix[0]) if matrix else 0

        # è®¡ç®—ç›¸é‚»è¡Œçš„å·®å¼‚
        for i in range(n_rows - 1):
            for j in range(n_cols):
                score += abs(matrix[i][j] - matrix[i+1][j])

        # è®¡ç®—ç›¸é‚»åˆ—çš„å·®å¼‚
        for i in range(n_rows):
            for j in range(n_cols - 1):
                score += abs(matrix[i][j] - matrix[i][j+1])

        return score

    original_score = continuity_score(original)
    clustered_score = continuity_score(clustered)

    if original_score == 0:
        return 0.0

    # è¿ç»­æ€§å¾—åˆ†è¶Šä½è¶Šå¥½ï¼Œæ‰€ä»¥æ”¹å–„æ˜¯è´Ÿå‘çš„
    return (original_score - clustered_score) / original_score * 100