#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç”Ÿäº§çº§CSVå¯¹æ¯”å’Œå®‰å…¨è¯„åˆ†ä¼˜åŒ–ç³»ç»Ÿ - Stage 3
æ•´åˆæ™ºèƒ½åˆ—åŒ¹é…ã€L2è¯­ä¹‰åˆ†æã€é£é™©è¯„åˆ†å’Œå®‰å…¨åŠ å›º
"""

import sys
import os
sys.path.append('/root/projects/tencent-doc-manager/production/core_modules')

import csv
import json
import asyncio
import logging

# å¼•å…¥Claude AIé›†æˆæ¨¡å—
try:
    from claude_wrapper_integration import ClaudeWrapperClient, ClaudeWrapperConfig
    CLAUDE_AI_AVAILABLE = True
    print("âœ… Claude AIæ¨¡å—å·²åŠ è½½")
except ImportError as e:
    CLAUDE_AI_AVAILABLE = False
    print(f"âš ï¸ Claude AIæ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
    print("  å°†ä½¿ç”¨åŸºç¡€é£é™©è¯„åˆ†ç®—æ³•")
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass
import re
import hashlib
from cookie_manager import get_cookie_manager

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class SecurityConfig:
    """å®‰å…¨é…ç½®ç±»"""
    max_file_size: int = 50 * 1024 * 1024  # 50MB
    max_rows: int = 100000
    max_columns: int = 500
    allowed_extensions: List[str] = None
    require_checksum: bool = True
    enable_audit_log: bool = True
    
    def __post_init__(self):
        if self.allowed_extensions is None:
            self.allowed_extensions = ['.csv', '.xlsx', '.xls']


@dataclass
class ComparisonResult:
    """å¯¹æ¯”ç»“æœç±»"""
    success: bool
    total_differences: int
    differences: List[Dict]
    security_score: float
    risk_level: str
    processing_time: float
    file_checksums: Dict[str, str]
    metadata: Dict[str, Any]


class ProductionCSVComparator:
    """
    ç”Ÿäº§çº§CSVå¯¹æ¯”å™¨
    æä¾›å®‰å…¨åŠ å›ºã€æ™ºèƒ½åŒ¹é…ã€é£é™©è¯„åˆ†
    """
    
    def __init__(self, security_config: SecurityConfig = None):
        """åˆå§‹åŒ–å¯¹æ¯”å™¨"""
        self.security_config = security_config or SecurityConfig()
        self.cookie_manager = get_cookie_manager()
        
        # å®¡è®¡æ—¥å¿—
        self.audit_log = []
        
        # Claude AIå®¢æˆ·ç«¯åˆå§‹åŒ–
        if CLAUDE_AI_AVAILABLE:
            self.claude_config = ClaudeWrapperConfig(
                base_url="http://localhost:8081",
                timeout=30,
                max_retries=3
            )
            self.claude_client = None  # å°†åœ¨éœ€è¦æ—¶å¼‚æ­¥åˆå§‹åŒ–
            print("ğŸ¤– Claude AIé›†æˆå·²å‡†å¤‡å°±ç»ª")
        else:
            self.claude_client = None
            print("ğŸ“ ä½¿ç”¨åŸºç¡€é£é™©è¯„åˆ†æ¨¡å¼")
        
        # é£é™©ç­‰çº§é…ç½®
        self.risk_levels = {
            'L1': {'score': 1.0, 'name': 'ç»å¯¹ä¸èƒ½ä¿®æ”¹', 'color': 'red'},
            'L2': {'score': 0.6, 'name': 'éœ€è¦è¯­ä¹‰å®¡æ ¸', 'color': 'orange'}, 
            'L3': {'score': 0.2, 'name': 'å¯è‡ªç”±ç¼–è¾‘', 'color': 'green'}
        }
        
        # æ ‡å‡†åˆ—é…ç½®
        self.standard_columns = {
            "åºå·": "L3",
            "é¡¹ç›®ç±»å‹": "L2", 
            "æ¥æº": "L1",
            "ä»»åŠ¡å‘èµ·æ—¶é—´": "L1",
            "ç›®æ ‡å¯¹é½": "L1",
            "å…³é”®KRå¯¹é½": "L1",
            "å…·ä½“è®¡åˆ’å†…å®¹": "L2",
            "é‚“æ€»æŒ‡å¯¼ç™»è®°": "L2",
            "è´Ÿè´£äºº": "L2",
            "ååŠ©äºº": "L2",
            "ç›‘ç£äºº": "L2",
            "é‡è¦ç¨‹åº¦": "L1",
            "é¢„è®¡å®Œæˆæ—¶é—´": "L1",
            "å®Œæˆè¿›åº¦": "L1",
            "å½¢æˆè®¡åˆ’æ¸…å•": "L2",
            "å¤ç›˜æ—¶é—´": "L3",
            "å¯¹ä¸Šæ±‡æŠ¥": "L3",
            "åº”ç”¨æƒ…å†µ": "L3",
            "è¿›åº¦åˆ†ææ€»ç»“": "L3"
        }
        
        logger.info("âœ… ç”Ÿäº§çº§CSVå¯¹æ¯”å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _calculate_file_checksum(self, file_path: str) -> str:
        """è®¡ç®—æ–‡ä»¶æ ¡éªŒå’Œ"""
        try:
            hasher = hashlib.sha256()
            with open(file_path, 'rb') as f:
                for chunk in iter(lambda: f.read(4096), b""):
                    hasher.update(chunk)
            return hasher.hexdigest()
        except Exception as e:
            logger.warning(f"è®¡ç®—æ ¡éªŒå’Œå¤±è´¥: {e}")
            return "unknown"
    
    def _security_validate_file(self, file_path: str) -> Tuple[bool, str]:
        """å®‰å…¨éªŒè¯æ–‡ä»¶"""
        try:
            path = Path(file_path)
            
            # æ£€æŸ¥æ–‡ä»¶å­˜åœ¨
            if not path.exists():
                return False, f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}"
            
            # æ£€æŸ¥æ–‡ä»¶æ‰©å±•å
            if path.suffix.lower() not in self.security_config.allowed_extensions:
                return False, f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {path.suffix}"
            
            # æ£€æŸ¥æ–‡ä»¶å¤§å°
            file_size = path.stat().st_size
            if file_size > self.security_config.max_file_size:
                return False, f"æ–‡ä»¶è¿‡å¤§: {file_size} bytes (æœ€å¤§: {self.security_config.max_file_size})"
            
            # æ£€æŸ¥æ–‡ä»¶åå®‰å…¨æ€§
            if re.search(r'[<>:"|?*]', path.name):
                return False, "æ–‡ä»¶ååŒ…å«ä¸å®‰å…¨å­—ç¬¦"
            
            return True, "æ–‡ä»¶éªŒè¯é€šè¿‡"
            
        except Exception as e:
            return False, f"æ–‡ä»¶éªŒè¯å¼‚å¸¸: {e}"
    
    def _load_csv_secure(self, file_path: str) -> Tuple[List[List[str]], Dict]:
        """å®‰å…¨åŠ è½½CSVæ–‡ä»¶"""
        try:
            # å®‰å…¨éªŒè¯
            valid, message = self._security_validate_file(file_path)
            if not valid:
                raise ValueError(message)
            
            # å°è¯•ä¸åŒç¼–ç 
            encodings = ['utf-8-sig', 'utf-8', 'gbk', 'gb2312', 'latin1']
            data = None
            used_encoding = None
            
            for encoding in encodings:
                try:
                    with open(file_path, 'r', encoding=encoding) as f:
                        reader = csv.reader(f)
                        data = list(reader)
                        used_encoding = encoding
                        break
                except (UnicodeDecodeError, UnicodeError):
                    continue
            
            if data is None:
                raise ValueError("æ— æ³•ä½¿ç”¨ä»»ä½•ç¼–ç è¯»å–æ–‡ä»¶")
            
            # å®‰å…¨æ£€æŸ¥
            if len(data) > self.security_config.max_rows:
                raise ValueError(f"æ–‡ä»¶è¡Œæ•°è¿‡å¤š: {len(data)} (æœ€å¤§: {self.security_config.max_rows})")
            
            if data and len(data[0]) > self.security_config.max_columns:
                raise ValueError(f"æ–‡ä»¶åˆ—æ•°è¿‡å¤š: {len(data[0])} (æœ€å¤§: {self.security_config.max_columns})")
            
            metadata = {
                'encoding': used_encoding,
                'rows': len(data),
                'columns': len(data[0]) if data else 0,
                'file_size': os.path.getsize(file_path),
                'checksum': self._calculate_file_checksum(file_path) if self.security_config.require_checksum else None
            }
            
            return data, metadata
            
        except Exception as e:
            logger.error(f"å®‰å…¨åŠ è½½CSVå¤±è´¥: {e}")
            raise
    
    def _intelligent_column_mapping(self, columns1: List[str], columns2: List[str]) -> Dict[str, Dict]:
        """æ™ºèƒ½åˆ—åæ˜ å°„"""
        try:
            mapping = {}
            confidence_scores = {}
            
            for col1 in columns1:
                if not col1.strip():
                    continue
                
                # ç²¾ç¡®åŒ¹é…
                if col1 in columns2:
                    mapping[col1] = col1
                    confidence_scores[col1] = 1.0
                    continue
                
                # æ¨¡ç³ŠåŒ¹é…
                best_match = None
                best_score = 0.0
                
                for col2 in columns2:
                    if not col2.strip():
                        continue
                    
                    # ç›¸ä¼¼åº¦è®¡ç®—ï¼ˆç®€åŒ–ç‰ˆï¼‰
                    similarity = self._calculate_column_similarity(col1, col2)
                    if similarity > best_score and similarity > 0.6:
                        best_match = col2
                        best_score = similarity
                
                if best_match:
                    mapping[col1] = best_match
                    confidence_scores[col1] = best_score
                else:
                    # æ£€æŸ¥æ ‡å‡†åˆ—å
                    standard_match = self._find_standard_column_match(col1)
                    if standard_match:
                        mapping[col1] = standard_match
                        confidence_scores[col1] = 0.8
            
            return {
                'mapping': mapping,
                'confidence_scores': confidence_scores,
                'mapping_rate': len(mapping) / max(len(columns1), 1) * 100
            }
            
        except Exception as e:
            logger.error(f"æ™ºèƒ½åˆ—æ˜ å°„å¤±è´¥: {e}")
            return {'mapping': {}, 'confidence_scores': {}, 'mapping_rate': 0.0}
    
    def _calculate_column_similarity(self, col1: str, col2: str) -> float:
        """è®¡ç®—åˆ—åç›¸ä¼¼åº¦"""
        try:
            # æ¸…ç†å’Œæ ‡å‡†åŒ–
            c1 = re.sub(r'[^\w\u4e00-\u9fff]', '', col1.lower().strip())
            c2 = re.sub(r'[^\w\u4e00-\u9fff]', '', col2.lower().strip())
            
            if c1 == c2:
                return 1.0
            
            # åŒ…å«å…³ç³»
            if c1 in c2 or c2 in c1:
                return 0.8
            
            # ç¼–è¾‘è·ç¦»ç›¸ä¼¼åº¦ï¼ˆç®€åŒ–ç‰ˆï¼‰
            max_len = max(len(c1), len(c2))
            if max_len == 0:
                return 0.0
            
            # ç®€å•çš„å­—ç¬¦é‡å ç‡
            common_chars = set(c1) & set(c2)
            similarity = len(common_chars) / max_len
            
            return min(similarity, 0.7)  # æœ€é«˜0.7ï¼Œä¸ºç²¾ç¡®åŒ¹é…ä¿ç•™ç©ºé—´
            
        except Exception as e:
            logger.error(f"ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {e}")
            return 0.0
    
    def _find_standard_column_match(self, column_name: str) -> Optional[str]:
        """æŸ¥æ‰¾æ ‡å‡†åˆ—ååŒ¹é…"""
        try:
            clean_name = re.sub(r'[^\w\u4e00-\u9fff]', '', column_name.strip())
            
            for standard_col in self.standard_columns.keys():
                clean_standard = re.sub(r'[^\w\u4e00-\u9fff]', '', standard_col)
                if clean_name in clean_standard or clean_standard in clean_name:
                    return standard_col
            
            return None
            
        except Exception as e:
            logger.error(f"æ ‡å‡†åˆ—åŒ¹é…å¤±è´¥: {e}")
            return None
    
    def _enhanced_risk_scoring(self, differences: List[Dict], column_mapping: Dict) -> Dict:
        """å¢å¼ºé£é™©è¯„åˆ†"""
        try:
            risk_scores = []
            risk_distribution = {'L1': 0, 'L2': 0, 'L3': 0}
            security_violations = []
            
            for diff in differences:
                column_name = diff.get('åˆ—å', '')
                original_value = str(diff.get('åŸå€¼', '')).strip()
                new_value = str(diff.get('æ–°å€¼', '')).strip()
                
                # è·å–é£é™©ç­‰çº§
                risk_level = self.standard_columns.get(column_name, 'L3')
                base_score = self.risk_levels[risk_level]['score']
                
                # å®‰å…¨å¢å¼ºè¯„åˆ†
                security_multiplier = 1.0
                
                # æ£€æŸ¥æ•æ„Ÿå†…å®¹å˜æ›´
                if self._contains_sensitive_info(original_value) or self._contains_sensitive_info(new_value):
                    security_multiplier *= 1.5
                    security_violations.append(f"æ•æ„Ÿä¿¡æ¯å˜æ›´: {column_name}")
                
                # æ£€æŸ¥å¤§é‡æ–‡æœ¬å˜æ›´
                text_change_ratio = self._calculate_text_change_ratio(original_value, new_value)
                if text_change_ratio > 0.5:
                    security_multiplier *= 1.2
                
                # æ£€æŸ¥å…³é”®å­—æ®µç©ºå€¼
                if not new_value and column_name in ['è´Ÿè´£äºº', 'ç›‘ç£äºº', 'é‡è¦ç¨‹åº¦']:
                    security_multiplier *= 1.8
                    security_violations.append(f"å…³é”®å­—æ®µç©ºå€¼: {column_name}")
                
                # è®¡ç®—æœ€ç»ˆè¯„åˆ†
                final_score = min(base_score * security_multiplier, 1.0)
                risk_scores.append(final_score)
                risk_distribution[risk_level] += 1
                
                # æ›´æ–°å·®å¼‚è®°å½•
                diff.update({
                    'risk_level': risk_level,
                    'risk_score': round(final_score, 3),
                    'security_multiplier': round(security_multiplier, 2),
                    'text_change_ratio': round(text_change_ratio, 2)
                })
            
            # è®¡ç®—ç»¼åˆå®‰å…¨è¯„åˆ†
            if risk_scores:
                avg_risk_score = sum(risk_scores) / len(risk_scores)
                max_risk_score = max(risk_scores)
            else:
                avg_risk_score = 0.0
                max_risk_score = 0.0
            
            # ç»¼åˆå®‰å…¨è¯„åˆ†ï¼ˆ0-100åˆ†ï¼‰
            security_score = max(0, 100 - (avg_risk_score * 60 + max_risk_score * 40))
            
            return {
                'security_score': round(security_score, 2),
                'average_risk_score': round(avg_risk_score, 3),
                'max_risk_score': round(max_risk_score, 3),
                'risk_distribution': risk_distribution,
                'security_violations': security_violations,
                'total_risks': len(risk_scores)
            }
            
        except Exception as e:
            logger.error(f"é£é™©è¯„åˆ†å¤±è´¥: {e}")
            return {
                'security_score': 0.0,
                'average_risk_score': 1.0,
                'max_risk_score': 1.0,
                'risk_distribution': {'L1': 0, 'L2': 0, 'L3': 0},
                'security_violations': [f"è¯„åˆ†è®¡ç®—å¤±è´¥: {str(e)}"],
                'total_risks': 0
            }
    
    async def _ai_enhanced_risk_scoring(self, differences: List[Dict], column_mapping: Dict) -> Dict:
        """AIå¢å¼ºé£é™©è¯„åˆ† - é›†æˆClaudeè¯­ä¹‰åˆ†æ"""
        try:
            # é¦–å…ˆæ‰§è¡ŒåŸºç¡€é£é™©è¯„åˆ†
            base_analysis = self._enhanced_risk_scoring(differences, column_mapping)
            
            # å¦‚æœClaude AIä¸å¯ç”¨ï¼Œè¿”å›åŸºç¡€åˆ†æ
            if not CLAUDE_AI_AVAILABLE or not differences:
                return base_analysis
            
            # å‡†å¤‡AIåˆ†ææ•°æ®
            ai_analysis_data = []
            for diff in differences[:10]:  # é™åˆ¶å‰10ä¸ªå·®å¼‚è¿›è¡ŒAIåˆ†æ
                ai_analysis_data.append({
                    'åˆ—å': diff.get('åˆ—å', ''),
                    'åŸå€¼': str(diff.get('åŸå€¼', ''))[:100],  # é™åˆ¶é•¿åº¦
                    'æ–°å€¼': str(diff.get('æ–°å€¼', ''))[:100],
                    'ä½ç½®': diff.get('ä½ç½®', ''),
                    'åŸºç¡€é£é™©ç­‰çº§': diff.get('risk_level', 'L3')
                })
            
            # è°ƒç”¨Claudeè¿›è¡Œè¯­ä¹‰åˆ†æ
            ai_insights = await self._get_claude_semantic_analysis(ai_analysis_data)
            
            # èåˆAIåˆ†æç»“æœ
            if ai_insights:
                # æ›´æ–°å®‰å…¨è¯„åˆ†ï¼ˆAIå»ºè®®æƒé‡ï¼š30%ï¼‰
                ai_security_score = ai_insights.get('ai_security_score', base_analysis['security_score'])
                base_analysis['security_score'] = round(
                    base_analysis['security_score'] * 0.7 + ai_security_score * 0.3, 2
                )
                
                # æ·»åŠ AIåˆ†ææ ‡è®°
                base_analysis['ai_analysis'] = {
                    'enabled': True,
                    'analyzed_count': len(ai_analysis_data),
                    'ai_insights': ai_insights.get('insights', []),
                    'recommendation': ai_insights.get('recommendation', ''),
                    'confidence': ai_insights.get('confidence', 0.0)
                }
                
                logger.info(f"ğŸ¤– AIåˆ†æå®Œæˆ: {len(ai_analysis_data)}ä¸ªå˜æ›´, ç½®ä¿¡åº¦: {ai_insights.get('confidence', 0):.2f}")
            else:
                # AIåˆ†æå¤±è´¥ï¼Œä½¿ç”¨åŸºç¡€åˆ†æ
                base_analysis['ai_analysis'] = {
                    'enabled': False,
                    'error': 'AIåˆ†æè°ƒç”¨å¤±è´¥',
                    'fallback_to_basic': True
                }
            
            return base_analysis
            
        except Exception as e:
            logger.error(f"AIå¢å¼ºé£é™©è¯„åˆ†å¤±è´¥: {e}")
            # å¤±è´¥æ—¶å›é€€åˆ°åŸºç¡€è¯„åˆ†
            basic_result = self._enhanced_risk_scoring(differences, column_mapping)
            basic_result['ai_analysis'] = {
                'enabled': False,
                'error': str(e),
                'fallback_to_basic': True
            }
            return basic_result
    
    async def _get_claude_semantic_analysis(self, differences_data: List[Dict]) -> Optional[Dict]:
        """è°ƒç”¨Claudeè¿›è¡Œè¯­ä¹‰åˆ†æ"""
        try:
            # åˆå§‹åŒ–Claudeå®¢æˆ·ç«¯
            if not self.claude_client and CLAUDE_AI_AVAILABLE:
                self.claude_client = ClaudeWrapperClient(self.claude_config)
            
            if not self.claude_client:
                return None
            
            # æ„å»ºåˆ†ææç¤º
            analysis_prompt = f"""
è¯·åˆ†æä»¥ä¸‹{len(differences_data)}ä¸ªæ•°æ®å˜æ›´ï¼Œè¯„ä¼°å…¶ä¸šåŠ¡é£é™©å’Œåˆç†æ€§ï¼š

{json.dumps(differences_data, ensure_ascii=False, indent=2)}

è¯·æä¾›ï¼š
1. æ•´ä½“å®‰å…¨è¯„åˆ†(0-100)
2. ä¸»è¦é£é™©ç‚¹å’Œå»ºè®®
3. ç½®ä¿¡åº¦è¯„ä¼°(0-1)
4. æ˜¯å¦å­˜åœ¨å¼‚å¸¸å˜æ›´

å“åº”æ ¼å¼ï¼šJSON
"""
            
            async with self.claude_client as client:
                # è°ƒç”¨Claudeåˆ†æAPI
                response = await client.analyze_risk(
                    content=analysis_prompt,
                    context={
                        "analysis_type": "csv_differences",
                        "difference_count": len(differences_data)
                    }
                )
                
                if response and not response.get('error'):
                    # å¤„ç†analyze_risk APIçš„å“åº”æ ¼å¼
                    ai_result = response.get('result', '')
                    confidence = float(response.get('confidence', 0.8))
                    risk_level = response.get('risk_level', 'L3')
                    
                    # å°è¯•è§£æç»“æ„åŒ–å“åº”
                    try:
                        if isinstance(ai_result, str) and ai_result.startswith('{'):
                            parsed_result = json.loads(ai_result)
                            security_score = float(parsed_result.get('security_score', 80))
                            insights = parsed_result.get('insights', [])
                            recommendation = parsed_result.get('recommendation', ai_result[:200])
                        else:
                            # å¤„ç†æ–‡æœ¬å“åº”
                            security_score = 85.0 if risk_level == 'L3' else 65.0 if risk_level == 'L2' else 40.0
                            insights = [ai_result[:100]] if ai_result else []
                            recommendation = ai_result if ai_result else 'æ— ç‰¹æ®Šå»ºè®®'
                        
                        return {
                            'ai_security_score': security_score,
                            'insights': insights,
                            'recommendation': recommendation,
                            'confidence': confidence,
                            'risk_level': risk_level,
                            'anomaly_detected': risk_level in ['L1', 'L2']
                        }
                        
                    except (json.JSONDecodeError, ValueError) as e:
                        logger.info(f"ä½¿ç”¨ç®€åŒ–AIå“åº”å¤„ç†: {e}")
                        # å›é€€åˆ°ç®€åŒ–å¤„ç†
                        security_score = 85.0 if risk_level == 'L3' else 65.0 if risk_level == 'L2' else 40.0
                        return {
                            'ai_security_score': security_score,
                            'insights': [str(ai_result)[:100]] if ai_result else [],
                            'recommendation': str(ai_result)[:200] if ai_result else 'æ— ç‰¹æ®Šå»ºè®®',
                            'confidence': confidence,
                            'risk_level': risk_level,
                            'anomaly_detected': risk_level in ['L1', 'L2']
                        }
                        
                else:
                    logger.warning(f"Claude APIè°ƒç”¨å¤±è´¥: {response.get('error', 'Unknown error')}")
                    return None
                    
        except Exception as e:
            logger.error(f"Claudeè¯­ä¹‰åˆ†æè°ƒç”¨å¤±è´¥: {e}")
            return None
    
    def _contains_sensitive_info(self, text: str) -> bool:
        """æ£€æµ‹æ•æ„Ÿä¿¡æ¯"""
        try:
            if not text:
                return False
            
            # æ•æ„Ÿä¿¡æ¯æ¨¡å¼
            sensitive_patterns = [
                r'\b\d{11,18}\b',  # å¯èƒ½çš„èº«ä»½è¯å·
                r'\b1[3-9]\d{9}\b',  # æ‰‹æœºå·
                r'\b\d{3,4}-?\d{8}\b',  # ç”µè¯å·ç 
                r'[\w\.-]+@[\w\.-]+\.\w+',  # é‚®ç®±
                r'\b[å¯†ç |password|pwd]\s*[:ï¼š=]\s*\S+',  # å¯†ç 
                r'\b\d{6,20}\s*[å…ƒ|å—|ä¸‡|åƒ]\b'  # é‡‘é¢
            ]
            
            for pattern in sensitive_patterns:
                if re.search(pattern, text, re.IGNORECASE):
                    return True
            
            return False
            
        except Exception as e:
            logger.error(f"æ•æ„Ÿä¿¡æ¯æ£€æµ‹å¤±è´¥: {e}")
            return False
    
    def _calculate_text_change_ratio(self, text1: str, text2: str) -> float:
        """è®¡ç®—æ–‡æœ¬å˜åŒ–æ¯”ç‡"""
        try:
            if not text1 and not text2:
                return 0.0
            if not text1 or not text2:
                return 1.0
            
            # ç®€å•çš„å­—ç¬¦å·®å¼‚æ¯”ç‡
            len1, len2 = len(text1), len(text2)
            max_len = max(len1, len2)
            min_len = min(len1, len2)
            
            if max_len == 0:
                return 0.0
            
            # é•¿åº¦å˜åŒ–æ¯”ç‡
            length_ratio = abs(len1 - len2) / max_len
            
            # å†…å®¹ç›¸ä¼¼æ€§ï¼ˆç®€åŒ–ï¼‰
            common_chars = len(set(text1.lower()) & set(text2.lower()))
            total_chars = len(set(text1.lower()) | set(text2.lower()))
            content_similarity = common_chars / max(total_chars, 1)
            
            # ç»¼åˆå˜åŒ–æ¯”ç‡
            change_ratio = (length_ratio + (1 - content_similarity)) / 2
            
            return min(change_ratio, 1.0)
            
        except Exception as e:
            logger.error(f"æ–‡æœ¬å˜åŒ–æ¯”ç‡è®¡ç®—å¤±è´¥: {e}")
            return 0.5
    
    def _audit_log_operation(self, operation: str, details: Dict):
        """è®°å½•å®¡è®¡æ—¥å¿—"""
        try:
            if self.security_config.enable_audit_log:
                audit_entry = {
                    'timestamp': datetime.now().isoformat(),
                    'operation': operation,
                    'details': details,
                    'user': 'system',  # å¯ä»¥é›†æˆç”¨æˆ·è®¤è¯
                    'session_id': f"csv_comp_{int(datetime.now().timestamp())}"
                }
                self.audit_log.append(audit_entry)
        except Exception as e:
            logger.error(f"å®¡è®¡æ—¥å¿—è®°å½•å¤±è´¥: {e}")
    
    async def compare_csv_files(self, file1_path: str, file2_path: str, 
                               output_file: str = None) -> ComparisonResult:
        """
        ç”Ÿäº§çº§CSVæ–‡ä»¶å¯¹æ¯”
        
        Args:
            file1_path: åŸºå‡†æ–‡ä»¶è·¯å¾„
            file2_path: å½“å‰æ–‡ä»¶è·¯å¾„ 
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            
        Returns:
            ComparisonResult: å¯¹æ¯”ç»“æœ
        """
        start_time = datetime.now()
        
        try:
            logger.info(f"ğŸ” å¼€å§‹ç”Ÿäº§çº§CSVå¯¹æ¯”: {Path(file1_path).name} vs {Path(file2_path).name}")
            
            # å®¡è®¡æ—¥å¿—
            self._audit_log_operation("csv_comparison_start", {
                'file1': file1_path,
                'file2': file2_path,
                'output': output_file
            })
            
            # å®‰å…¨åŠ è½½æ–‡ä»¶
            data1, meta1 = self._load_csv_secure(file1_path)
            data2, meta2 = self._load_csv_secure(file2_path)
            
            # é¢„å¤„ç†æ•°æ®ï¼ˆå¤„ç†å¤šè¡Œæ ‡é¢˜ç­‰ï¼‰
            processed_data1 = self._preprocess_csv_data(data1)
            processed_data2 = self._preprocess_csv_data(data2)
            
            if not processed_data1 or not processed_data2:
                raise ValueError("å¤„ç†åçš„æ•°æ®ä¸ºç©º")
            
            # è·å–åˆ—å
            columns1 = processed_data1[0]
            columns2 = processed_data2[0]
            
            # æ™ºèƒ½åˆ—æ˜ å°„
            column_mapping = self._intelligent_column_mapping(columns1, columns2)
            
            # æ‰§è¡Œå¯¹æ¯”
            differences = await self._perform_detailed_comparison(
                processed_data1, processed_data2, column_mapping
            )
            
            # AIå¢å¼ºé£é™©è¯„åˆ†
            risk_analysis = await self._ai_enhanced_risk_scoring(differences, column_mapping)
            
            # è®¡ç®—å¤„ç†æ—¶é—´
            processing_time = (datetime.now() - start_time).total_seconds()
            
            # ç¡®å®šæ•´ä½“é£é™©ç­‰çº§
            if risk_analysis['max_risk_score'] >= 0.8:
                risk_level = "L1 - é«˜é£é™©"
            elif risk_analysis['average_risk_score'] >= 0.4:
                risk_level = "L2 - ä¸­é£é™©"
            else:
                risk_level = "L3 - ä½é£é™©"
            
            # æ„å»ºç»“æœ
            result = ComparisonResult(
                success=True,
                total_differences=len(differences),
                differences=differences,
                security_score=risk_analysis['security_score'],
                risk_level=risk_level,
                processing_time=processing_time,
                file_checksums={
                    'file1': meta1.get('checksum', 'unknown'),
                    'file2': meta2.get('checksum', 'unknown')
                },
                metadata={
                    'file1_info': meta1,
                    'file2_info': meta2,
                    'column_mapping': column_mapping,
                    'risk_analysis': risk_analysis,
                    'audit_log_entries': len(self.audit_log)
                }
            )
            
            # ä¿å­˜ç»“æœ
            if output_file:
                await self._save_comparison_result(result, output_file)
            
            # å®¡è®¡æ—¥å¿—
            self._audit_log_operation("csv_comparison_complete", {
                'total_differences': result.total_differences,
                'security_score': result.security_score,
                'risk_level': result.risk_level,
                'processing_time': result.processing_time
            })
            
            logger.info(f"âœ… CSVå¯¹æ¯”å®Œæˆ: {result.total_differences}ä¸ªå·®å¼‚, å®‰å…¨è¯„åˆ†: {result.security_score:.1f}")
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ CSVå¯¹æ¯”å¤±è´¥: {e}")
            
            processing_time = (datetime.now() - start_time).total_seconds()
            
            # å®¡è®¡æ—¥å¿—
            self._audit_log_operation("csv_comparison_error", {
                'error': str(e),
                'processing_time': processing_time
            })
            
            return ComparisonResult(
                success=False,
                total_differences=0,
                differences=[],
                security_score=0.0,
                risk_level="é”™è¯¯",
                processing_time=processing_time,
                file_checksums={},
                metadata={'error': str(e)}
            )
    
    def _preprocess_csv_data(self, data: List[List[str]]) -> List[List[str]]:
        """é¢„å¤„ç†CSVæ•°æ®"""
        try:
            if len(data) < 2:
                return data
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å¤šè¡Œæ ‡é¢˜
            if len(data) >= 3:
                first_row = data[0]
                # å¦‚æœç¬¬ä¸€è¡Œåªæœ‰ç¬¬ä¸€åˆ—æœ‰å†…å®¹ï¼Œå¯èƒ½æ˜¯æ ‡é¢˜è¡Œ
                if first_row[0] and not any(first_row[1:10] if len(first_row) > 10 else first_row[1:]):
                    if len(data) >= 4:
                        header1 = data[1]
                        header2 = data[2]
                        
                        # åˆå¹¶åˆ—å
                        merged_header = []
                        max_cols = max(len(header1), len(header2))
                        for i in range(max_cols):
                            col1 = header1[i] if i < len(header1) else ""
                            col2 = header2[i] if i < len(header2) else ""
                            final_col = col1.strip() if col1.strip() else col2.strip()
                            merged_header.append(final_col)
                        
                        return [merged_header] + data[3:]
            
            return data
            
        except Exception as e:
            logger.error(f"æ•°æ®é¢„å¤„ç†å¤±è´¥: {e}")
            return data
    
    async def _perform_detailed_comparison(self, data1: List[List[str]], data2: List[List[str]],
                                          column_mapping: Dict) -> List[Dict]:
        """æ‰§è¡Œè¯¦ç»†å¯¹æ¯” - å¢å¼ºç‰ˆï¼šå¯¹æ¯”æ‰€æœ‰åˆ—ï¼Œä¸é™äºæ ‡å‡†åˆ—"""
        try:
            differences = []

            if not data1 or not data2:
                return differences

            headers1 = data1[0]
            headers2 = data2[0]

            # ğŸ”¥ å…³é”®æ”¹è¿›ï¼šå¯¹æ¯”æ‰€æœ‰åˆ—ï¼Œä¸åªæ˜¯æ˜ å°„çš„åˆ—
            # ä½¿ç”¨ç´¢å¼•å¯¹åº”æ–¹å¼ï¼Œç¡®ä¿æ‰€æœ‰åˆ—éƒ½è¢«å¯¹æ¯”
            max_cols = max(len(headers1), len(headers2))
            common_columns = []

            for i in range(max_cols):
                if i < len(headers1) and i < len(headers2):
                    # ä¸¤ä¸ªæ–‡ä»¶éƒ½æœ‰è¿™ä¸€åˆ—
                    common_columns.append((i, i,
                                         headers1[i] if headers1[i] else f"åˆ—{chr(65+i)}",
                                         headers2[i] if headers2[i] else f"åˆ—{chr(65+i)}"))
            
            # é€è¡Œå¯¹æ¯”
            min_rows = min(len(data1) - 1, len(data2) - 1)
            diff_count = 0
            
            for row_idx in range(min_rows):
                row1 = data1[row_idx + 1]
                row2 = data2[row_idx + 1]
                
                for col1_idx, col2_idx, col1_name, col2_name in common_columns:
                    if col1_idx < len(row1) and col2_idx < len(row2):
                        old_val = str(row1[col1_idx]).strip()
                        new_val = str(row2[col2_idx]).strip()
                        
                        if old_val != new_val:
                            diff_count += 1
                            
                            difference = {
                                "åºå·": diff_count,
                                "è¡Œå·": row_idx + 1,
                                "åˆ—å": col1_name,
                                "åˆ—ç´¢å¼•": col1_idx + 1,
                                "åŸå€¼": old_val,
                                "æ–°å€¼": new_val,
                                "ä½ç½®": f"è¡Œ{row_idx+1}åˆ—{col1_idx+1}({col1_name})",
                                "æ˜ å°„åˆ—å": col2_name if col2_name != col1_name else None,
                                "æ¯”è¾ƒæ—¶é—´": datetime.now().isoformat()
                            }
                            
                            differences.append(difference)
            
            return differences
            
        except Exception as e:
            logger.error(f"è¯¦ç»†å¯¹æ¯”å¤±è´¥: {e}")
            return []
    
    async def _save_comparison_result(self, result: ComparisonResult, output_file: str):
        """ä¿å­˜å¯¹æ¯”ç»“æœ"""
        try:
            # æ„å»ºè¾“å‡ºæ•°æ®
            output_data = {
                "comparison_summary": {
                    "success": result.success,
                    "total_differences": result.total_differences,
                    "security_score": result.security_score,
                    "risk_level": result.risk_level,
                    "processing_time": result.processing_time,
                    "comparison_time": datetime.now().isoformat()
                },
                "file_info": {
                    "file_checksums": result.file_checksums,
                    "metadata": result.metadata
                },
                "differences": result.differences,
                "audit_log": self.audit_log if self.security_config.enable_audit_log else []
            }
            
            # ä¿å­˜åˆ°æ–‡ä»¶
            os.makedirs(os.path.dirname(output_file), exist_ok=True)
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(output_data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"ğŸ“„ å¯¹æ¯”ç»“æœå·²ä¿å­˜: {output_file}")
            
        except Exception as e:
            logger.error(f"ä¿å­˜ç»“æœå¤±è´¥: {e}")
    
    def get_security_report(self) -> Dict:
        """è·å–å®‰å…¨æŠ¥å‘Š"""
        try:
            return {
                'security_config': {
                    'max_file_size': f"{self.security_config.max_file_size / 1024 / 1024:.1f}MB",
                    'max_rows': self.security_config.max_rows,
                    'max_columns': self.security_config.max_columns,
                    'allowed_extensions': self.security_config.allowed_extensions,
                    'security_features': ['checksum_validation', 'audit_logging', 'sensitive_content_detection']
                },
                'audit_log_entries': len(self.audit_log),
                'risk_levels_config': self.risk_levels,
                'standard_columns_count': len(self.standard_columns),
                'system_status': 'operational'
            }
        except Exception as e:
            logger.error(f"è·å–å®‰å…¨æŠ¥å‘Šå¤±è´¥: {e}")
            return {'error': str(e)}


# å‘½ä»¤è¡Œæ¥å£
async def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='ç”Ÿäº§çº§CSVå¯¹æ¯”å’Œå®‰å…¨è¯„åˆ†ç³»ç»Ÿ')
    parser.add_argument('file1', nargs='?', help='åŸºå‡†CSVæ–‡ä»¶')
    parser.add_argument('file2', nargs='?', help='å½“å‰CSVæ–‡ä»¶')
    parser.add_argument('-o', '--output', help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--security-report', action='store_true', help='æ˜¾ç¤ºå®‰å…¨æŠ¥å‘Š')
    
    args = parser.parse_args()
    
    # åˆ›å»ºå¯¹æ¯”å™¨
    comparator = ProductionCSVComparator()
    
    try:
        if args.security_report or (not args.file1 and not args.file2):
            print("ğŸ›¡ï¸ å®‰å…¨é…ç½®æŠ¥å‘Š:")
            report = comparator.get_security_report()
            for key, value in report.items():
                if isinstance(value, dict):
                    print(f"   {key}:")
                    for k, v in value.items():
                        print(f"     {k}: {v}")
                else:
                    print(f"   {key}: {value}")
            print()
        
        if args.file1 and args.file2:
            print(f"ğŸ” å¼€å§‹ç”Ÿäº§çº§CSVå¯¹æ¯”:")
            print(f"   åŸºå‡†æ–‡ä»¶: {Path(args.file1).name}")
            print(f"   å½“å‰æ–‡ä»¶: {Path(args.file2).name}")
            
            result = await comparator.compare_csv_files(args.file1, args.file2, args.output)
            
            if result.success:
                print(f"\nâœ… å¯¹æ¯”å®Œæˆ!")
                print(f"   æ€»å·®å¼‚æ•°: {result.total_differences}")
                print(f"   å®‰å…¨è¯„åˆ†: {result.security_score:.1f}/100")
                print(f"   é£é™©ç­‰çº§: {result.risk_level}")
                print(f"   å¤„ç†æ—¶é—´: {result.processing_time:.2f}ç§’")
                
                if result.metadata.get('risk_analysis', {}).get('security_violations'):
                    print(f"   å®‰å…¨è­¦å‘Š: {len(result.metadata['risk_analysis']['security_violations'])}é¡¹")
                    for violation in result.metadata['risk_analysis']['security_violations'][:3]:
                        print(f"     â€¢ {violation}")
                
                if args.output:
                    print(f"   ç»“æœæ–‡ä»¶: {args.output}")
            else:
                print(f"\nâŒ å¯¹æ¯”å¤±è´¥: {result.metadata.get('error')}")
        
    except Exception as e:
        print(f"âŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")


if __name__ == "__main__":
    asyncio.run(main())