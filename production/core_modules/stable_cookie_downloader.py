#!/usr/bin/env python3
"""
ç¨³å®šçš„Cookieä¸‹è½½å™¨ - ä¼˜åŒ–ç‰ˆ
é€šè¿‡ç›´æ¥URLè®¿é—®å’Œæ™ºèƒ½Cookieç®¡ç†å®ç°é«˜ç¨³å®šæ€§
"""

import os
import time
import json
import hashlib
import requests
from typing import Dict, List, Tuple, Optional
from datetime import datetime, timedelta
from urllib.parse import urlparse, parse_qs, urlencode
import logging
from pathlib import Path

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class StableCookieDownloader:
    """
    ç¨³å®šçš„Cookieä¸‹è½½å™¨
    æ ¸å¿ƒä¼˜åŒ–ï¼š
    1. ç›´æ¥ä½¿ç”¨å¯¼å‡ºURLï¼Œé¿å…é¡µé¢è‡ªåŠ¨åŒ–
    2. Cookieæ™ºèƒ½ç®¡ç†å’Œè‡ªåŠ¨è½®æ¢
    3. å¤šç§å¯¼å‡ºæ¥å£å¤‡ä»½
    4. æ™ºèƒ½é‡è¯•å’Œé™çº§æœºåˆ¶
    """
    
    def __init__(self, cookie_file: str = None):
        self.cookie_file = cookie_file or '/root/projects/tencent-doc-manager/config/cookies.json'
        self.cookies_pool = []  # Cookieæ± 
        self.current_cookie_index = 0
        self.download_stats = {
            'success': 0,
            'failed': 0,
            'total_time': 0,
            'last_success': None
        }
        
        # å¯¼å‡ºURLæ¨¡æ¿ï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰
        self.export_endpoints = [
            {
                'name': 'v1_export',
                'url': 'https://docs.qq.com/v1/export/export_office',
                'method': 'GET',
                'success_rate': 0.95
            },
            {
                'name': 'sheet_export',  
                'url': 'https://docs.qq.com/sheet/export',
                'method': 'GET',
                'success_rate': 0.90
            },
            {
                'name': 'cgi_export',
                'url': 'https://docs.qq.com/cgi-bin/excel/export',
                'method': 'POST',
                'success_rate': 0.85
            }
        ]
        
        # åŠ è½½Cookie
        self._load_cookies()
    
    def _load_cookies(self):
        """åŠ è½½Cookieé…ç½®"""
        try:
            if os.path.exists(self.cookie_file):
                with open(self.cookie_file, 'r') as f:
                    config = json.load(f)
                    
                # ä¸»Cookie
                if config.get('current_cookies'):
                    self.cookies_pool.append({
                        'cookie_string': config['current_cookies'],
                        'last_used': None,
                        'failure_count': 0,
                        'success_count': 0
                    })
                
                # å¤‡ç”¨Cookie
                if config.get('backup_cookies'):
                    for backup in config['backup_cookies']:
                        self.cookies_pool.append({
                            'cookie_string': backup,
                            'last_used': None,
                            'failure_count': 0,
                            'success_count': 0
                        })
                
                logger.info(f"âœ… åŠ è½½äº† {len(self.cookies_pool)} ä¸ªCookie")
            else:
                logger.warning(f"Cookieæ–‡ä»¶ä¸å­˜åœ¨: {self.cookie_file}")
                
        except Exception as e:
            logger.error(f"åŠ è½½Cookieå¤±è´¥: {e}")
    
    def _get_current_cookie(self) -> Dict:
        """è·å–å½“å‰ä½¿ç”¨çš„Cookie"""
        if not self.cookies_pool:
            raise Exception("æ²¡æœ‰å¯ç”¨çš„Cookie")
        
        # é€‰æ‹©å¤±è´¥æ¬¡æ•°æœ€å°‘çš„Cookie
        sorted_cookies = sorted(self.cookies_pool, key=lambda x: x['failure_count'])
        return sorted_cookies[0]
    
    def _rotate_cookie(self):
        """è½®æ¢åˆ°ä¸‹ä¸€ä¸ªCookie"""
        if len(self.cookies_pool) > 1:
            self.current_cookie_index = (self.current_cookie_index + 1) % len(self.cookies_pool)
            logger.info(f"åˆ‡æ¢åˆ°Cookie #{self.current_cookie_index + 1}")
    
    def _extract_doc_id(self, url: str) -> Tuple[str, Optional[str]]:
        """
        ä»URLæå–æ–‡æ¡£IDå’Œtab ID
        
        æ”¯æŒçš„URLæ ¼å¼ï¼š
        - https://docs.qq.com/sheet/DWEVjZndkR2xVSWJN?tab=c2p5hs
        - https://docs.qq.com/sheet/DWEVjZndkR2xVSWJN
        """
        # æå–è·¯å¾„ä¸­çš„æ–‡æ¡£ID
        parsed = urlparse(url)
        path_parts = parsed.path.strip('/').split('/')
        
        doc_id = None
        for part in path_parts:
            if part.startswith('D') and len(part) > 10:
                doc_id = part
                break
        
        # æå–æŸ¥è¯¢å‚æ•°ä¸­çš„tab
        query = parse_qs(parsed.query)
        tab_id = query.get('tab', [None])[0]
        
        if not doc_id:
            # å°è¯•ä»æŸ¥è¯¢å‚æ•°è·å–
            doc_id = query.get('id', [None])[0] or query.get('docid', [None])[0]
        
        return doc_id, tab_id
    
    def _build_export_url(self, doc_id: str, export_type: str = 'csv', 
                         endpoint: Dict = None, tab_id: str = None) -> str:
        """æ„å»ºå¯¼å‡ºURL"""
        if not endpoint:
            endpoint = self.export_endpoints[0]
        
        base_url = endpoint['url']
        
        if endpoint['name'] == 'v1_export':
            # V1å¯¼å‡ºæ¥å£
            params = {
                'docid': doc_id,
                'type': export_type,
                'download': '1',
                'normal': '1',
                'preview': '0',
                'export_source': 'client'
            }
            if tab_id:
                params['tab'] = tab_id
            return f"{base_url}?{urlencode(params)}"
            
        elif endpoint['name'] == 'sheet_export':
            # Sheetå¯¼å‡ºæ¥å£
            params = {
                'id': doc_id,
                'type': export_type,
                'download': '1'
            }
            if tab_id:
                params['sheet_id'] = tab_id
            return f"{base_url}?{urlencode(params)}"
            
        else:
            # CGIå¯¼å‡ºæ¥å£
            return f"{base_url}?id={doc_id}&type={export_type}"
    
    def download_document(self, url: str, export_type: str = 'csv', 
                         save_dir: str = None) -> Dict:
        """
        ä¸‹è½½è…¾è®¯æ–‡æ¡£
        
        Args:
            url: è…¾è®¯æ–‡æ¡£URL
            export_type: å¯¼å‡ºç±»å‹ (csv/xlsx)
            save_dir: ä¿å­˜ç›®å½•
            
        Returns:
            ä¸‹è½½ç»“æœå­—å…¸
        """
        start_time = time.time()
        
        # æå–æ–‡æ¡£ID
        doc_id, tab_id = self._extract_doc_id(url)
        if not doc_id:
            return {
                'success': False,
                'error': 'æ— æ³•ä»URLæå–æ–‡æ¡£ID',
                'url': url
            }
        
        logger.info(f"ğŸ“¥ å¼€å§‹ä¸‹è½½æ–‡æ¡£: {doc_id} (æ ¼å¼: {export_type})")
        
        # å°è¯•æ¯ä¸ªå¯¼å‡ºç«¯ç‚¹
        for endpoint_index, endpoint in enumerate(self.export_endpoints):
            try:
                # æ„å»ºå¯¼å‡ºURL
                export_url = self._build_export_url(doc_id, export_type, endpoint, tab_id)
                logger.info(f"å°è¯•ç«¯ç‚¹ #{endpoint_index + 1}: {endpoint['name']}")
                
                # è·å–Cookie
                cookie_info = self._get_current_cookie()
                headers = {
                    'Cookie': cookie_info['cookie_string'],
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                    'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
                    'Accept-Encoding': 'gzip, deflate, br',
                    'DNT': '1',
                    'Connection': 'keep-alive',
                    'Upgrade-Insecure-Requests': '1'
                }
                
                # å‘é€è¯·æ±‚
                response = requests.get(
                    export_url,
                    headers=headers,
                    allow_redirects=True,
                    stream=True,
                    timeout=30
                )
                
                # æ£€æŸ¥å“åº”
                if response.status_code == 200:
                    # æ£€æŸ¥å†…å®¹ç±»å‹
                    content_type = response.headers.get('Content-Type', '')
                    
                    if 'application/json' in content_type:
                        # å¯èƒ½æ˜¯é”™è¯¯å“åº”
                        error_data = response.json()
                        if error_data.get('ret') != 0:
                            logger.warning(f"æ¥å£è¿”å›é”™è¯¯: {error_data}")
                            cookie_info['failure_count'] += 1
                            continue
                    
                    # ä¿å­˜æ–‡ä»¶
                    if not save_dir:
                        save_dir = '/root/projects/tencent-doc-manager/downloads'
                    os.makedirs(save_dir, exist_ok=True)
                    
                    # ç”Ÿæˆæ–‡ä»¶å
                    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                    filename = f"{doc_id}_{timestamp}.{export_type}"
                    filepath = os.path.join(save_dir, filename)
                    
                    # å†™å…¥æ–‡ä»¶
                    with open(filepath, 'wb') as f:
                        for chunk in response.iter_content(chunk_size=8192):
                            if chunk:
                                f.write(chunk)
                    
                    # éªŒè¯æ–‡ä»¶
                    file_size = os.path.getsize(filepath)
                    if file_size < 100:  # æ–‡ä»¶å¤ªå°ï¼Œå¯èƒ½æ˜¯é”™è¯¯é¡µé¢
                        logger.warning(f"æ–‡ä»¶å¤§å°å¼‚å¸¸: {file_size} bytes")
                        os.remove(filepath)
                        continue
                    
                    # æ›´æ–°ç»Ÿè®¡
                    elapsed = time.time() - start_time
                    self.download_stats['success'] += 1
                    self.download_stats['total_time'] += elapsed
                    self.download_stats['last_success'] = datetime.now()
                    cookie_info['success_count'] += 1
                    cookie_info['last_used'] = datetime.now()
                    
                    logger.info(f"âœ… ä¸‹è½½æˆåŠŸ: {filepath} ({file_size/1024:.1f} KB, {elapsed:.1f}ç§’)")
                    
                    return {
                        'success': True,
                        'filepath': filepath,
                        'file_size': file_size,
                        'download_time': elapsed,
                        'endpoint_used': endpoint['name'],
                        'doc_id': doc_id
                    }
                
                elif response.status_code == 401:
                    logger.warning("Cookieå·²å¤±æ•ˆ (401)")
                    cookie_info['failure_count'] += 5  # ä¸¥é‡å¤±è´¥
                    self._rotate_cookie()
                    
                elif response.status_code == 403:
                    logger.warning("è®¿é—®è¢«æ‹’ç» (403)")
                    cookie_info['failure_count'] += 2
                    
                else:
                    logger.warning(f"HTTP {response.status_code}")
                    cookie_info['failure_count'] += 1
                    
            except requests.exceptions.Timeout:
                logger.warning(f"è¯·æ±‚è¶…æ—¶: {endpoint['name']}")
                continue
                
            except Exception as e:
                logger.error(f"ä¸‹è½½å¼‚å¸¸: {e}")
                continue
        
        # æ‰€æœ‰å°è¯•éƒ½å¤±è´¥
        self.download_stats['failed'] += 1
        elapsed = time.time() - start_time
        
        return {
            'success': False,
            'error': 'æ‰€æœ‰ä¸‹è½½å°è¯•å‡å¤±è´¥',
            'doc_id': doc_id,
            'attempts': len(self.export_endpoints),
            'elapsed': elapsed
        }
    
    def batch_download(self, urls: List[str], export_type: str = 'csv') -> List[Dict]:
        """æ‰¹é‡ä¸‹è½½æ–‡æ¡£"""
        results = []
        total = len(urls)
        
        logger.info(f"å¼€å§‹æ‰¹é‡ä¸‹è½½ {total} ä¸ªæ–‡æ¡£")
        
        for index, url in enumerate(urls, 1):
            logger.info(f"[{index}/{total}] å¤„ç†: {url}")
            
            # ä¸‹è½½
            result = self.download_document(url, export_type)
            results.append(result)
            
            # æ™ºèƒ½å»¶æ—¶
            if result['success']:
                time.sleep(2)  # æˆåŠŸåçŸ­å»¶æ—¶
            else:
                time.sleep(5)  # å¤±è´¥åé•¿å»¶æ—¶
                self._rotate_cookie()  # å¤±è´¥ååˆ‡æ¢Cookie
        
        # ç»Ÿè®¡ç»“æœ
        success_count = sum(1 for r in results if r['success'])
        logger.info(f"æ‰¹é‡ä¸‹è½½å®Œæˆ: {success_count}/{total} æˆåŠŸ")
        
        return results
    
    def get_statistics(self) -> Dict:
        """è·å–ä¸‹è½½ç»Ÿè®¡ä¿¡æ¯"""
        total = self.download_stats['success'] + self.download_stats['failed']
        success_rate = self.download_stats['success'] / total if total > 0 else 0
        avg_time = self.download_stats['total_time'] / self.download_stats['success'] \
                   if self.download_stats['success'] > 0 else 0
        
        return {
            'total_downloads': total,
            'successful': self.download_stats['success'],
            'failed': self.download_stats['failed'],
            'success_rate': f"{success_rate * 100:.1f}%",
            'average_time': f"{avg_time:.1f}ç§’",
            'last_success': self.download_stats['last_success'],
            'cookie_pool_size': len(self.cookies_pool),
            'cookies_health': [
                {
                    'index': i,
                    'success': c['success_count'],
                    'failure': c['failure_count'],
                    'health': c['success_count'] / (c['success_count'] + c['failure_count']) * 100
                              if (c['success_count'] + c['failure_count']) > 0 else 100
                }
                for i, c in enumerate(self.cookies_pool)
            ]
        }


# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    downloader = StableCookieDownloader()
    
    # æµ‹è¯•å•ä¸ªä¸‹è½½
    test_url = "https://docs.qq.com/sheet/DWEVjZndkR2xVSWJN?tab=c2p5hs"
    
    print("æµ‹è¯•CSVä¸‹è½½...")
    result = downloader.download_document(test_url, 'csv')
    print(f"ç»“æœ: {json.dumps(result, indent=2, ensure_ascii=False)}")
    
    # æµ‹è¯•æ‰¹é‡ä¸‹è½½
    urls = [
        "https://docs.qq.com/sheet/DWEVjZndkR2xVSWJN?tab=c2p5hs",
        # æ·»åŠ æ›´å¤šURL
    ]
    
    print("\næµ‹è¯•æ‰¹é‡ä¸‹è½½...")
    results = downloader.batch_download(urls)
    
    # æ˜¾ç¤ºç»Ÿè®¡
    print("\nä¸‹è½½ç»Ÿè®¡:")
    stats = downloader.get_statistics()
    print(json.dumps(stats, indent=2, ensure_ascii=False))