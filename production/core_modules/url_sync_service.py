#!/usr/bin/env python3
"""
URLåŒæ­¥æœåŠ¡ - ç¡®ä¿ç³»ç»Ÿä¸­æ‰€æœ‰URLé…ç½®ä¿æŒä¸€è‡´

æ ¸å¿ƒåŠŸèƒ½ï¼š
1. åŒæ­¥real_documents.jsonä¸download_config.json
2. æ›´æ–°ç»¼åˆæ‰“åˆ†JSONä¸­çš„table_url
3. ç»´æŠ¤upload_mappings.jsonçš„ä¸€è‡´æ€§
4. æä¾›URLæŸ¥è¯¢å’ŒéªŒè¯æœåŠ¡

åŒæ­¥ç­–ç•¥ï¼š
- real_documents.jsonä½œä¸ºä¸»æ•°æ®æº
- å…¶ä»–é…ç½®æ–‡ä»¶è‡ªåŠ¨åŒæ­¥
- å˜æ›´æ£€æµ‹å’Œé€šçŸ¥æœºåˆ¶
"""

import json
import os
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional, Any, Set
import logging
import re
from difflib import SequenceMatcher

try:
    from .config_manager import get_config_manager
    from .upload_url_manager import get_manager as get_url_manager
except ImportError:
    # ç›´æ¥è¿è¡Œæ—¶ä½¿ç”¨
    import sys
    sys.path.insert(0, '/root/projects/tencent-doc-manager')
    from production.core_modules.config_manager import get_config_manager
    from production.core_modules.upload_url_manager import get_manager as get_url_manager

logger = logging.getLogger(__name__)


class URLSyncService:
    """URLåŒæ­¥æœåŠ¡"""
    
    def __init__(self, project_root: str = None):
        """
        åˆå§‹åŒ–URLåŒæ­¥æœåŠ¡
        
        Args:
            project_root: é¡¹ç›®æ ¹ç›®å½•
        """
        if project_root is None:
            project_root = '/root/projects/tencent-doc-manager'
        
        self.project_root = Path(project_root)
        
        # è·å–ç®¡ç†å™¨å®ä¾‹
        self.config_manager = get_config_manager()
        self.url_manager = get_url_manager()
        
        # è·¯å¾„é…ç½®
        self.scoring_dir = self.project_root / 'scoring_results'
        self.comprehensive_dir = self.scoring_dir / 'comprehensive'
        
        # URLæ˜ å°„ç¼“å­˜
        self.url_cache = {}
        self._build_url_cache()
    
    def _build_url_cache(self):
        """æ„å»ºURLç¼“å­˜"""
        # ä»real_documents.jsonè·å–ä¸»æ•°æ®
        documents_config = self.config_manager.get_config('documents')
        if documents_config:
            for doc in documents_config.get('documents', []):
                self.url_cache[doc['name']] = {
                    'url': doc['url'],
                    'doc_id': doc['doc_id'],
                    'description': doc.get('description', ''),
                    'csv_pattern': doc.get('csv_pattern', '')
                }
    
    def sync_all_configs(self) -> Dict[str, Any]:
        """
        åŒæ­¥æ‰€æœ‰é…ç½®æ–‡ä»¶ä¸­çš„URL
        
        Returns:
            åŒæ­¥ç»“æœæŠ¥å‘Š
        """
        report = {
            'timestamp': datetime.now().isoformat(),
            'synced': [],
            'errors': [],
            'changes': 0
        }
        
        # 1. åŒæ­¥download_config.json
        try:
            self._sync_download_config()
            report['synced'].append('download_config')
        except Exception as e:
            report['errors'].append(f"download_config: {e}")
        
        # 2. åŒæ­¥ç»¼åˆæ‰“åˆ†JSONæ–‡ä»¶
        try:
            changes = self._sync_comprehensive_scores()
            report['changes'] += changes
            report['synced'].append(f'comprehensive_scores ({changes} changes)')
        except Exception as e:
            report['errors'].append(f"comprehensive_scores: {e}")
        
        # 3. éªŒè¯upload_mappingsä¸€è‡´æ€§
        try:
            self._validate_upload_mappings()
            report['synced'].append('upload_mappings')
        except Exception as e:
            report['errors'].append(f"upload_mappings: {e}")
        
        logger.info(f"âœ… URLåŒæ­¥å®Œæˆ: {len(report['synced'])}ä¸ªæˆåŠŸ, {len(report['errors'])}ä¸ªé”™è¯¯")
        return report
    
    def _sync_download_config(self):
        """åŒæ­¥download_config.json"""
        documents_config = self.config_manager.get_config('documents')
        download_config = self.config_manager.get_config('download')
        
        if not documents_config or not download_config:
            return
        
        # æ„å»ºæ–°çš„document_links
        new_links = []
        for doc in documents_config.get('documents', []):
            new_links.append({
                'name': doc['name'],
                'url': doc['url'],
                'doc_id': doc['doc_id'],
                'description': doc.get('description', ''),
                'enabled': True
            })
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å˜åŒ–
        old_links = download_config.get('document_links', [])
        if self._compare_links(old_links, new_links):
            # æœ‰å˜åŒ–ï¼Œæ›´æ–°é…ç½®
            self.config_manager.update_config('download', {
                'document_links': new_links
            })
            logger.info(f"âœ… åŒæ­¥download_config: {len(new_links)}ä¸ªé“¾æ¥")
    
    def _compare_links(self, old_links: List[Dict], new_links: List[Dict]) -> bool:
        """æ¯”è¾ƒé“¾æ¥åˆ—è¡¨æ˜¯å¦æœ‰å˜åŒ–"""
        if len(old_links) != len(new_links):
            return True
        
        old_urls = {link.get('url') for link in old_links}
        new_urls = {link.get('url') for link in new_links}
        
        return old_urls != new_urls
    
    def _sync_comprehensive_scores(self) -> int:
        """
        åŒæ­¥ç»¼åˆæ‰“åˆ†JSONæ–‡ä»¶ä¸­çš„table_url
        
        Returns:
            ä¿®æ”¹çš„æ–‡ä»¶æ•°é‡
        """
        if not self.comprehensive_dir.exists():
            return 0
        
        changes = 0
        
        # éå†æ‰€æœ‰ç»¼åˆæ‰“åˆ†æ–‡ä»¶
        for json_file in self.comprehensive_dir.glob('comprehensive_score_*.json'):
            try:
                with open(json_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                modified = False
                
                # æ›´æ–°table_scoresä¸­çš„table_url
                for table_score in data.get('table_scores', []):
                    table_name = table_score.get('table_name', '')
                    
                    # å°è¯•åŒ¹é…è¡¨åè·å–URL
                    matched_url = self._match_table_url(table_name)
                    if matched_url and table_score.get('table_url') != matched_url:
                        table_score['table_url'] = matched_url
                        modified = True
                
                # å¦‚æœæœ‰ä¿®æ”¹ï¼Œä¿å­˜æ–‡ä»¶
                if modified:
                    with open(json_file, 'w', encoding='utf-8') as f:
                        json.dump(data, f, ensure_ascii=False, indent=2)
                    changes += 1
                    logger.debug(f"æ›´æ–°æ–‡ä»¶: {json_file.name}")
            
            except Exception as e:
                logger.warning(f"å¤„ç†æ–‡ä»¶å¤±è´¥ {json_file.name}: {e}")
        
        if changes > 0:
            logger.info(f"âœ… æ›´æ–°äº† {changes} ä¸ªç»¼åˆæ‰“åˆ†æ–‡ä»¶çš„URL")
        
        return changes
    
    def _match_table_url(self, table_name: str) -> Optional[str]:
        """
        æ ¹æ®è¡¨ååŒ¹é…URL
        
        Args:
            table_name: è¡¨æ ¼åç§°
        
        Returns:
            åŒ¹é…çš„URLï¼Œå¦‚æœæ²¡æœ‰æ‰¾åˆ°è¿”å›None
        """
        # æ¸…ç†è¡¨å
        clean_name = self._clean_table_name(table_name)
        
        # ç²¾ç¡®åŒ¹é…
        if clean_name in self.url_cache:
            return self.url_cache[clean_name]['url']
        
        # æ¨¡ç³ŠåŒ¹é…
        for doc_name, doc_info in self.url_cache.items():
            # éƒ¨åˆ†åŒ¹é…
            if doc_name in clean_name or clean_name in doc_name:
                return doc_info['url']
            
            # ä½¿ç”¨ç›¸ä¼¼åº¦åŒ¹é…
            similarity = SequenceMatcher(None, doc_name, clean_name).ratio()
            if similarity > 0.7:  # 70%ç›¸ä¼¼åº¦
                return doc_info['url']
        
        return None
    
    def _clean_table_name(self, table_name: str) -> str:
        """æ¸…ç†è¡¨åï¼Œæå–æ ¸å¿ƒéƒ¨åˆ†"""
        # ç§»é™¤æ–‡ä»¶è·¯å¾„ä¿¡æ¯
        if '/' in table_name:
            table_name = table_name.split('/')[-1]
        
        # ç§»é™¤æ–‡ä»¶æ‰©å±•å
        table_name = re.sub(r'\.(csv|xlsx|xls)$', '', table_name)
        
        # ç§»é™¤æ—¶é—´æˆ³
        table_name = re.sub(r'_\d{8}_\d{6}', '', table_name)
        table_name = re.sub(r'_\d{14}', '', table_name)
        
        # ç§»é™¤ç‰ˆæœ¬æ ‡è®°
        table_name = re.sub(r'_(baseline|midweek|final|current|previous)', '', table_name)
        
        # ç§»é™¤å‘¨æ•°æ ‡è®°
        table_name = re.sub(r'_W\d+', '', table_name)
        
        # ç§»é™¤å¯¹æ¯”æ ‡è®°
        table_name = re.sub(r'_vs_.*', '', table_name)
        
        # ç§»é™¤simplifiedå‰ç¼€
        if table_name.startswith('simplified_'):
            table_name = table_name[11:]
        
        # ç§»é™¤tencentå‰ç¼€
        if table_name.startswith('tencent_'):
            table_name = table_name[8:]
        
        return table_name.strip()
    
    def _validate_upload_mappings(self):
        """éªŒè¯upload_mappingsçš„ä¸€è‡´æ€§"""
        # è·å–æ‰€æœ‰ä¸Šä¼ è®°å½•
        stats = self.url_manager.get_statistics()
        
        # éªŒè¯æ¯ä¸ªè®°å½•çš„URLæ˜¯å¦ä»ç„¶æœ‰æ•ˆ
        invalid_count = 0
        for record in self.url_manager.mappings:
            doc_url = record.doc_url
            
            # æ£€æŸ¥URLæ ¼å¼
            if not doc_url.startswith('https://docs.qq.com/'):
                invalid_count += 1
                logger.warning(f"æ— æ•ˆURLæ ¼å¼: {doc_url}")
        
        if invalid_count > 0:
            logger.warning(f"âš ï¸ å‘ç° {invalid_count} ä¸ªæ— æ•ˆçš„ä¸Šä¼ URL")
    
    def get_url_for_document(self, doc_name: str) -> Optional[str]:
        """
        è·å–æ–‡æ¡£çš„URL
        
        Args:
            doc_name: æ–‡æ¡£åç§°
        
        Returns:
            æ–‡æ¡£URLï¼Œå¦‚æœæ²¡æœ‰æ‰¾åˆ°è¿”å›None
        """
        # å…ˆå°è¯•ç¼“å­˜
        if doc_name in self.url_cache:
            return self.url_cache[doc_name]['url']
        
        # å°è¯•åŒ¹é…
        return self._match_table_url(doc_name)
    
    def get_all_urls(self) -> List[Dict[str, str]]:
        """
        è·å–æ‰€æœ‰æ–‡æ¡£URL
        
        Returns:
            æ–‡æ¡£URLåˆ—è¡¨
        """
        urls = []
        for doc_name, doc_info in self.url_cache.items():
            urls.append({
                'name': doc_name,
                'url': doc_info['url'],
                'doc_id': doc_info['doc_id'],
                'description': doc_info.get('description', '')
            })
        return urls
    
    def add_url_mapping(self, doc_name: str, url: str, doc_id: str = None) -> bool:
        """
        æ·»åŠ æ–°çš„URLæ˜ å°„
        
        Args:
            doc_name: æ–‡æ¡£åç§°
            url: æ–‡æ¡£URL
            doc_id: æ–‡æ¡£IDï¼ˆå¯é€‰ï¼‰
        
        Returns:
            æ˜¯å¦æˆåŠŸæ·»åŠ 
        """
        if not doc_id:
            # ä»URLæå–doc_id
            match = re.search(r'/sheet/([A-Za-z0-9]+)', url)
            if match:
                doc_id = match.group(1)
        
        # æ›´æ–°ç¼“å­˜
        self.url_cache[doc_name] = {
            'url': url,
            'doc_id': doc_id or '',
            'description': '',
            'csv_pattern': ''
        }
        
        # æ›´æ–°real_documents.json
        documents_config = self.config_manager.get_config('documents')
        if documents_config:
            documents = documents_config.get('documents', [])
            
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
            exists = False
            for doc in documents:
                if doc['name'] == doc_name:
                    doc['url'] = url
                    doc['doc_id'] = doc_id or doc.get('doc_id', '')
                    exists = True
                    break
            
            # æ·»åŠ æ–°æ–‡æ¡£
            if not exists:
                documents.append({
                    'name': doc_name,
                    'url': url,
                    'doc_id': doc_id or '',
                    'csv_pattern': '',
                    'description': ''
                })
            
            # ä¿å­˜é…ç½®
            self.config_manager.update_config('documents', {
                'documents': documents
            })
            
            # è§¦å‘åŒæ­¥
            self.sync_all_configs()
            
            logger.info(f"âœ… æ·»åŠ URLæ˜ å°„: {doc_name} -> {url}")
            return True
        
        return False
    
    def check_url_consistency(self) -> Dict[str, List[str]]:
        """
        æ£€æŸ¥ç³»ç»Ÿä¸­URLçš„ä¸€è‡´æ€§
        
        Returns:
            ä¸ä¸€è‡´çš„URLæŠ¥å‘Š
        """
        inconsistencies = {}
        
        # è·å–å„ä¸ªæ¥æºçš„URL
        real_docs_urls = self._get_real_documents_urls()
        download_config_urls = self._get_download_config_urls()
        comprehensive_urls = self._get_comprehensive_urls()
        
        # æ£€æŸ¥ä¸ä¸€è‡´
        all_doc_names = set()
        all_doc_names.update(real_docs_urls.keys())
        all_doc_names.update(download_config_urls.keys())
        all_doc_names.update(comprehensive_urls.keys())
        
        for doc_name in all_doc_names:
            urls = set()
            
            if doc_name in real_docs_urls:
                urls.add(real_docs_urls[doc_name])
            if doc_name in download_config_urls:
                urls.add(download_config_urls[doc_name])
            if doc_name in comprehensive_urls:
                urls.add(comprehensive_urls[doc_name])
            
            if len(urls) > 1:
                inconsistencies[doc_name] = list(urls)
        
        if inconsistencies:
            logger.warning(f"âš ï¸ å‘ç° {len(inconsistencies)} ä¸ªURLä¸ä¸€è‡´")
        
        return inconsistencies
    
    def _get_real_documents_urls(self) -> Dict[str, str]:
        """è·å–real_documents.jsonä¸­çš„URL"""
        urls = {}
        documents_config = self.config_manager.get_config('documents')
        if documents_config:
            for doc in documents_config.get('documents', []):
                urls[doc['name']] = doc['url']
        return urls
    
    def _get_download_config_urls(self) -> Dict[str, str]:
        """è·å–download_config.jsonä¸­çš„URL"""
        urls = {}
        download_config = self.config_manager.get_config('download')
        if download_config:
            for link in download_config.get('document_links', []):
                urls[link['name']] = link['url']
        return urls
    
    def _get_comprehensive_urls(self) -> Dict[str, str]:
        """è·å–ç»¼åˆæ‰“åˆ†æ–‡ä»¶ä¸­çš„URL"""
        urls = {}
        
        if self.comprehensive_dir.exists():
            # åªè·å–æœ€æ–°çš„æ–‡ä»¶
            files = sorted(self.comprehensive_dir.glob('comprehensive_score_*.json'))
            if files:
                latest_file = files[-1]
                try:
                    with open(latest_file, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                    
                    for table_score in data.get('table_scores', []):
                        table_name = self._clean_table_name(table_score.get('table_name', ''))
                        table_url = table_score.get('table_url', '')
                        if table_name and table_url:
                            urls[table_name] = table_url
                except:
                    pass
        
        return urls


# å•ä¾‹å®ä¾‹
_sync_service: Optional[URLSyncService] = None

def get_sync_service() -> URLSyncService:
    """è·å–URLåŒæ­¥æœåŠ¡å•ä¾‹"""
    global _sync_service
    if _sync_service is None:
        _sync_service = URLSyncService()
    return _sync_service


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    service = get_sync_service()
    
    # æ‰§è¡ŒåŒæ­¥
    print("ğŸ”„ å¼€å§‹URLåŒæ­¥...")
    report = service.sync_all_configs()
    
    print(f"\nğŸ“Š åŒæ­¥æŠ¥å‘Š:")
    print(f"  æˆåŠŸ: {report['synced']}")
    print(f"  é”™è¯¯: {report['errors']}")
    print(f"  å˜æ›´: {report['changes']}ä¸ªæ–‡ä»¶")
    
    # æ£€æŸ¥ä¸€è‡´æ€§
    inconsistencies = service.check_url_consistency()
    if inconsistencies:
        print(f"\nâš ï¸ URLä¸ä¸€è‡´:")
        for doc_name, urls in inconsistencies.items():
            print(f"  {doc_name}: {urls}")
    else:
        print("\nâœ… æ‰€æœ‰URLä¸€è‡´")
    
    # æ˜¾ç¤ºæ‰€æœ‰URL
    all_urls = service.get_all_urls()
    print(f"\nğŸ“„ ç³»ç»Ÿä¸­çš„æ–‡æ¡£URL ({len(all_urls)}ä¸ª):")
    for url_info in all_urls:
        print(f"  - {url_info['name']}: {url_info['url']}")