#!/usr/bin/env python3
"""
è‡ªåŠ¨ç”Ÿæˆç»¼åˆæ‰“åˆ†æ–‡ä»¶
åœ¨8093å·¥ä½œæµå®Œæˆåè°ƒç”¨ï¼Œä»è¯¦ç»†æ‰“åˆ†å’Œå¯¹æ¯”ç»“æœç”Ÿæˆç»¼åˆæ‰“åˆ†
"""

import json
import os
from datetime import datetime
from pathlib import Path
import logging

logger = logging.getLogger(__name__)

class AutoComprehensiveGenerator:
    """è‡ªåŠ¨ç»¼åˆæ‰“åˆ†ç”Ÿæˆå™¨"""

    def __init__(self):
        self.scoring_dir = Path('/root/projects/tencent-doc-manager/scoring_results')
        self.comparison_dir = Path('/root/projects/tencent-doc-manager/comparison_results')
        self.detailed_dir = self.scoring_dir / 'detailed'
        self.comprehensive_dir = self.scoring_dir / 'comprehensive'

        # è·å–å½“å‰å‘¨æ•°
        self.current_week = self._get_current_week()
        # åˆ›å»ºå¯¹åº”å‘¨çš„ç›®å½•
        self.week_dir = self.scoring_dir / f'2025_W{self.current_week}'
        self.week_dir.mkdir(parents=True, exist_ok=True)

        # æ ‡å‡†19åˆ—
        self.STANDARD_COLUMNS = [
            "åºå·", "é¡¹ç›®ç±»å‹", "æ¥æº", "ä»»åŠ¡å‘èµ·æ—¶é—´", "ç›®æ ‡å¯¹é½",
            "å…³é”®KRå¯¹é½", "å…·ä½“è®¡åˆ’å†…å®¹", "é‚“æ€»æŒ‡å¯¼ç™»è®°", "è´Ÿè´£äºº", "ååŠ©äºº",
            "ç›‘ç£äºº", "é‡è¦ç¨‹åº¦", "é¢„è®¡å®Œæˆæ—¶é—´", "å®Œæˆè¿›åº¦", "å®Œæˆé“¾æ¥",
            "ç»ç†åˆ†æå¤ç›˜", "æœ€æ–°å¤ç›˜æ—¶é—´", "å¯¹ä¸Šæ±‡æŠ¥", "åº”ç”¨æƒ…å†µ"
        ]

        # L1/L2/L3åˆ—å®šä¹‰
        self.L1_COLUMNS = ["æ¥æº", "ä»»åŠ¡å‘èµ·æ—¶é—´", "ç›®æ ‡å¯¹é½", "å…³é”®KRå¯¹é½", "è´Ÿè´£äºº", "é‡è¦ç¨‹åº¦", "é¢„è®¡å®Œæˆæ—¶é—´"]
        self.L2_COLUMNS = ["é¡¹ç›®ç±»å‹", "å…·ä½“è®¡åˆ’å†…å®¹", "é‚“æ€»æŒ‡å¯¼ç™»è®°", "ååŠ©äºº", "ç›‘ç£äºº", "å¯¹ä¸Šæ±‡æŠ¥"]
        self.L3_COLUMNS = ["åºå·", "å®Œæˆè¿›åº¦", "å®Œæˆé“¾æ¥", "ç»ç†åˆ†æå¤ç›˜", "æœ€æ–°å¤ç›˜æ—¶é—´", "åº”ç”¨æƒ…å†µ"]

    def _get_current_week(self) -> str:
        """è·å–å½“å‰å‘¨æ•°"""
        return str(datetime.now().isocalendar()[1]).zfill(2)

    def _get_baseline_week(self) -> str:
        """è·å–åŸºçº¿å‘¨æ•°ï¼ˆé€šå¸¸æ˜¯ä¸Šä¸€å‘¨ï¼‰"""
        now = datetime.now()
        weekday = now.weekday()  # 0=å‘¨ä¸€
        hour = now.hour
        current_week = now.isocalendar()[1]

        # æ ¹æ®æŠ€æœ¯è§„èŒƒv1.6çš„é€»è¾‘
        if weekday < 1 or (weekday == 1 and hour < 12):
            # å‘¨ä¸€å…¨å¤© OR å‘¨äºŒ12ç‚¹å‰ â†’ ä½¿ç”¨ä¸Šå‘¨åŸºçº¿
            target_week = current_week - 1
        else:
            # å‘¨äºŒ12ç‚¹å OR å‘¨ä¸‰åˆ°å‘¨æ—¥ â†’ ä½¿ç”¨æœ¬å‘¨åŸºçº¿
            target_week = current_week

        return str(target_week).zfill(2)

    def generate_from_latest_results(self, excel_url=None) -> str:
        """ä»æœ€æ–°çš„è¯¦ç»†æ‰“åˆ†ç»“æœç”Ÿæˆç»¼åˆæ‰“åˆ†

        Args:
            excel_url: ä¸Šä¼ åçš„è…¾è®¯æ–‡æ¡£URL
        """

        # 1. æŸ¥æ‰¾æœ€æ–°çš„è¯¦ç»†æ‰“åˆ†æ–‡ä»¶
        detailed_files = sorted(self.detailed_dir.glob('detailed_score_*.json'),
                              key=lambda x: x.stat().st_mtime, reverse=True)

        if not detailed_files:
            raise FileNotFoundError("æ²¡æœ‰æ‰¾åˆ°è¯¦ç»†æ‰“åˆ†æ–‡ä»¶")

        latest_detailed = detailed_files[0]
        logger.info(f"ä½¿ç”¨è¯¦ç»†æ‰“åˆ†æ–‡ä»¶: {latest_detailed.name}")

        # 2. åŠ è½½è¯¦ç»†æ‰“åˆ†æ•°æ®
        with open(latest_detailed, 'r', encoding='utf-8') as f:
            detailed_data = json.load(f)

        # 3. æå–è¡¨æ ¼ä¿¡æ¯
        table_name = self._extract_table_name(detailed_data)

        # 4. ç”Ÿæˆçƒ­åŠ›å›¾çŸ©é˜µå’Œåˆ—ä¿®æ”¹æ•°æ®
        heatmap_matrix, column_modifications = self._process_detailed_scores(detailed_data)

        # 5. ç»Ÿè®¡é£é™©åˆ†å¸ƒ
        risk_stats = self._calculate_risk_stats(detailed_data)

        # 6. ç”Ÿæˆç»¼åˆæ‰“åˆ†æ•°æ®ç»“æ„
        comprehensive_data = self._build_comprehensive_structure(
            table_name, heatmap_matrix, column_modifications,
            detailed_data, risk_stats, excel_url
        )

        # 7. ä¿å­˜æ–‡ä»¶
        output_file = self._save_comprehensive_file(comprehensive_data)

        logger.info(f"âœ… ç»¼åˆæ‰“åˆ†æ–‡ä»¶å·²ç”Ÿæˆ: {output_file}")
        return str(output_file)

    def _extract_table_name(self, detailed_data):
        """æå–è¡¨æ ¼åç§°"""
        # ä»metadataæˆ–scoresä¸­æå–
        table_name = detailed_data.get('metadata', {}).get('table_name', '')

        # å¦‚æœæ˜¯ä¸´æ—¶æ–‡ä»¶åï¼Œå°è¯•ä»å·¥ä½œæµå†å²è®°å½•ä¸­æŸ¥æ‰¾çœŸå®æ–‡æ¡£å
        if table_name.startswith('tmp'):
            # æŸ¥æ‰¾å¯¹åº”çš„å·¥ä½œæµæ–‡ä»¶æ¥è·å–çœŸå®æ–‡æ¡£å
            import glob
            import json
            from pathlib import Path

            # å…ˆå°è¯•ä»workflowå†å²ä¸­æŸ¥æ‰¾
            workflow_dir = Path('/root/projects/tencent-doc-manager/workflow_history')

            # æŸ¥æ‰¾åŒ…å«è¿™ä¸ªä¸´æ—¶IDçš„workflowæ–‡ä»¶
            for workflow_file in workflow_dir.glob('workflow_*.json'):
                try:
                    with open(workflow_file, 'r', encoding='utf-8') as f:
                        workflow_data = json.load(f)

                    # æ£€æŸ¥score_fileæ˜¯å¦åŒ…å«å½“å‰çš„ä¸´æ—¶ID
                    score_file = workflow_data.get('results', {}).get('score_file', '')
                    if table_name in score_file:
                        # ä»target_fileä¸­æå–æ–‡æ¡£å
                        target_file = workflow_data.get('results', {}).get('target_file', '')
                        if target_file:
                            import re
                            # åŒ¹é…æ ¼å¼ï¼štencent_{æ–‡æ¡£å}_{æ—¶é—´æˆ³}_{ç‰ˆæœ¬}_W{å‘¨}.csv
                            match = re.search(r'tencent_(.+?)_\d{8}_\d{4}_(baseline|midweek)_W\d+\.csv$', target_file)
                            if match:
                                extracted_name = match.group(1)
                                logger.info(f"ä»workflowå†å²æå–è¡¨å: {table_name} -> {extracted_name}")
                                return extracted_name
                except Exception as e:
                    logger.debug(f"è¯»å–workflowæ–‡ä»¶å¤±è´¥: {e}")
                    continue

            # å¦‚æœworkflowä¸­æ‰¾ä¸åˆ°ï¼Œå°è¯•ä»å¯¹åº”æ—¶é—´çš„CSVæ–‡ä»¶æŸ¥æ‰¾ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰
            # ä½†ä¸ä½¿ç”¨æœ€æ–°æ–‡ä»¶ï¼Œè€Œæ˜¯å°è¯•åŒ¹é…æ—¶é—´æˆ³
            if '_' in table_name:
                # æå–è¯¦ç»†æ–‡ä»¶çš„æ—¶é—´æˆ³ï¼ˆä¾‹å¦‚ï¼štmpi6tacfy8_20250925_183507ï¼‰
                parts = table_name.split('_')
                if len(parts) >= 3:
                    date_str = parts[-2]  # 20250925
                    time_str = parts[-1]  # 183507

                    # åŠ¨æ€è·å–å½“å‰å‘¨çš„midweekç›®å½•
                    current_year = datetime.now().year
                    current_week = datetime.now().isocalendar()[1]
                    csv_dir = Path(f'/root/projects/tencent-doc-manager/csv_versions/{current_year}_W{current_week:02d}/midweek')
                    # æŸ¥æ‰¾åŒä¸€æ—¶é—´æ®µçš„CSVæ–‡ä»¶ï¼ˆå…è®¸5åˆ†é’Ÿè¯¯å·®ï¼‰
                    for csv_file in csv_dir.glob(f'*_{date_str}_*.csv'):
                        csv_time = csv_file.stem.split('_')[-3]  # è·å–æ—¶é—´éƒ¨åˆ†
                        # ç®€å•æ—¶é—´æ¯”è¾ƒï¼ˆå…è®¸ç›¸è¿‘æ—¶é—´ï¼‰
                        if abs(int(time_str[:4]) - int(csv_time)) <= 5:  # 5åˆ†é’Ÿå†…
                            import re
                            match = re.search(r'^tencent_(.+?)_\d{8}_\d{4}_(baseline|midweek)_W\d+$', csv_file.stem)
                            if match:
                                extracted_name = match.group(1)
                                logger.info(f"ä»æ—¶é—´åŒ¹é…çš„CSVæ–‡ä»¶æå–è¡¨å: {table_name} -> {extracted_name}")
                                return extracted_name

            # å¦‚æœè¿˜æ˜¯æ‰¾ä¸åˆ°ï¼Œè¿”å›ä¸´æ—¶IDæœ¬èº«ï¼ˆä¿æŒå¯è¿½æº¯æ€§ï¼‰
            logger.warning(f"æ— æ³•ä¸ºä¸´æ—¶ID {table_name} æ‰¾åˆ°å¯¹åº”çš„æ–‡æ¡£å")
            return f"æ–‡æ¡£_{table_name}"

        return table_name

    def _process_detailed_scores(self, detailed_data):
        """å¤„ç†è¯¦ç»†æ‰“åˆ†ï¼Œç”Ÿæˆçƒ­åŠ›å›¾çŸ©é˜µå’Œåˆ—ä¿®æ”¹æ•°æ®"""

        # åˆ—åæ˜ å°„
        COLUMN_MAPPING = {
            "è®¡åˆ’è¾“å‡ºæ€è·¯\n8/28": "åºå·",
            "é¡¹ç›®ç±»å‹": "é¡¹ç›®ç±»å‹",
            "æ¥æº": "æ¥æº",
            "ä»»åŠ¡å‘èµ·æ—¶é—´": "ä»»åŠ¡å‘èµ·æ—¶é—´",
            "ç›®æ ‡å¯¹é½": "ç›®æ ‡å¯¹é½",
            "å…³é”®KRå¯¹é½": "å…³é”®KRå¯¹é½",
            "å…·ä½“è®¡åˆ’å†…å®¹": "å…·ä½“è®¡åˆ’å†…å®¹",
            "é‚“æ€»æŒ‡å¯¼ç™»è®°ï¼ˆæ—¥æ›´æ–°ï¼‰": "é‚“æ€»æŒ‡å¯¼ç™»è®°",
            "è´Ÿè´£äºº": "è´Ÿè´£äºº",
            "ååŠ©äºº": "ååŠ©äºº",
            "é‡è¦ç¨‹åº¦": "é‡è¦ç¨‹åº¦",
            "é¢„è®¡å®Œæˆæ—¶é—´": "é¢„è®¡å®Œæˆæ—¶é—´",
            "å®Œæˆé“¾æ¥": "å®Œæˆé“¾æ¥",
            "æ€»å®Œæˆè¿›åº¦": "å®Œæˆè¿›åº¦",
            "ç»ç†åˆ†æå¤ç›˜": "ç»ç†åˆ†æå¤ç›˜",
        }

        # æ”¶é›†æ¯åˆ—çš„ä¿®æ”¹å’Œé£é™©ç­‰çº§
        column_data = {}

        for score in detailed_data.get('scores', []):
            original_col = score['column_name']
            standard_col = COLUMN_MAPPING.get(original_col, original_col)

            if standard_col in self.STANDARD_COLUMNS:
                if standard_col not in column_data:
                    column_data[standard_col] = {
                        'modifications': [],
                        'risk_level': score['column_level'],
                        'scores': []
                    }

                # æå–è¡Œå·
                cell = score['cell']
                row_num = int(''.join(filter(str.isdigit, cell)))
                column_data[standard_col]['modifications'].append(row_num)
                column_data[standard_col]['scores'].append(
                    score.get('scoring_details', {}).get('final_score', 0)
                )

        # ç”Ÿæˆçƒ­åŠ›å›¾çŸ©é˜µ
        matrix_row = []
        column_modifications = {}

        for col in self.STANDARD_COLUMNS:
            if col in column_data:
                risk_level = column_data[col]['risk_level']
                has_modification = len(column_data[col]['modifications']) > 0

                # æ ¹æ®é£é™©ç­‰çº§è®¾ç½®é¢œè‰²å€¼
                if not has_modification:
                    heat_value = 0.05  # æ— ä¿®æ”¹ï¼ˆè“è‰²ï¼‰
                elif risk_level == 'L1':
                    heat_value = 0.90  # L1é«˜é£é™©ï¼ˆçº¢è‰²ï¼‰
                elif risk_level == 'L2':
                    heat_value = 0.60  # L2ä¸­é£é™©ï¼ˆæ©™è‰²ï¼‰
                else:  # L3
                    heat_value = 0.30  # L3ä½é£é™©ï¼ˆç»¿è‰²ï¼‰

                # æ„å»ºåˆ—ä¿®æ”¹æ•°æ®
                column_modifications[col] = {
                    'modified_rows': column_data[col]['modifications'],
                    'modification_count': len(column_data[col]['modifications']),
                    'risk_level': risk_level,
                    'average_score': round(sum(column_data[col]['scores']) / len(column_data[col]['scores']), 2) if column_data[col]['scores'] else 0
                }
            else:
                heat_value = 0.05  # æ— ä¿®æ”¹

            matrix_row.append(heat_value)

        return [matrix_row], column_modifications

    def _calculate_risk_stats(self, detailed_data):
        """è®¡ç®—é£é™©ç»Ÿè®¡"""
        l1_count = 0
        l2_count = 0
        l3_count = 0

        for score in detailed_data.get('scores', []):
            level = score['column_level']
            if level == 'L1':
                l1_count += 1
            elif level == 'L2':
                l2_count += 1
            elif level == 'L3':
                l3_count += 1

        return {
            'l1_count': l1_count,
            'l2_count': l2_count,
            'l3_count': l3_count,
            'total': l1_count + l2_count + l3_count
        }

    def _build_comprehensive_structure(self, table_name, heatmap_matrix,
                                      column_modifications, detailed_data, risk_stats, excel_url=None):
        """æ„å»ºç»¼åˆæ‰“åˆ†æ•°æ®ç»“æ„

        Args:
            excel_url: ä¸Šä¼ åçš„è…¾è®¯æ–‡æ¡£URL
        """

        total_mods = risk_stats['total']

        # æ„å»ºåŸºç¡€ç»“æ„
        result = {
            "metadata": {
                "version": "2.0",
                "timestamp": datetime.now().isoformat(),
                "week": f"W{self.current_week}",
                "generator": "auto_comprehensive_generator",
                "source_type": "detailed_scoring",
                "baseline_week": f"W{self._get_baseline_week()}",
                "comparison_week": f"W{self.current_week}"
            },
            "summary": {
                "total_tables": 1,
                "total_columns": 19,
                "total_modifications": total_mods,
                "l1_modifications": risk_stats['l1_count'],
                "l2_modifications": risk_stats['l2_count'],
                "l3_modifications": risk_stats['l3_count'],
                "overall_risk_score": self._calculate_overall_risk(risk_stats),
                "processing_status": "complete",
                "data_source": "auto_generated"
            },
            "table_names": [table_name],
            "column_names": self.STANDARD_COLUMNS,
            "heatmap_data": {
                "matrix": heatmap_matrix,
                "rows": len(heatmap_matrix),
                "cols": 19,
                "generation_method": "risk_based_auto",
                "color_distribution": {
                    "red_0.9": heatmap_matrix[0].count(0.90),
                    "orange_0.6": heatmap_matrix[0].count(0.60),
                    "green_0.3": heatmap_matrix[0].count(0.30),
                    "blue_0.05": heatmap_matrix[0].count(0.05)
                }
            },
            "table_details": {
                table_name: {
                    "total_rows": 270,
                    "modified_rows": total_mods,
                    "added_rows": 0,
                    "deleted_rows": 0
                }
            },
            "statistics": {
                "total_cells": 5130,
                "modified_cells": total_mods,
                "modification_rate": round(total_mods / 5130, 4) if total_mods > 0 else 0,
                "risk_distribution": detailed_data.get('summary', {}).get('risk_distribution', {})
            },
            "column_modifications_by_table": {
                table_name: {
                    "column_modifications": column_modifications,
                    "total_rows": 270
                }
            }
        }

        # æ·»åŠ excel_urlså­—æ®µï¼ˆå¦‚æœæœ‰URLï¼‰
        if excel_url:
            result["excel_urls"] = {
                table_name: excel_url
            }
            # åŒæ—¶åœ¨table_detailsä¸­æ·»åŠ 
            result["table_details"][table_name]["excel_url"] = excel_url
            logger.info(f"âœ… å·²æ·»åŠ Excel URLåˆ°ç»¼åˆæ‰“åˆ†: {excel_url}")

        return result

    def _calculate_overall_risk(self, risk_stats):
        """è®¡ç®—æ•´ä½“é£é™©åˆ†æ•°"""
        total = risk_stats['total']
        if total == 0:
            return 0

        weighted_score = (
            risk_stats['l1_count'] * 0.9 +
            risk_stats['l2_count'] * 0.6 +
            risk_stats['l3_count'] * 0.3
        ) / total

        return round(weighted_score, 2)

    def _save_comprehensive_file(self, data):
        """ä¿å­˜ç»¼åˆæ‰“åˆ†æ–‡ä»¶"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f"comprehensive_score_W{self.current_week}_AUTO_{timestamp}.json"

        # ä¿å­˜åˆ°å¯¹åº”å‘¨çš„ç›®å½•
        output_path = self.week_dir / filename

        # ç¡®ä¿ç›®å½•å­˜åœ¨
        self.week_dir.mkdir(parents=True, exist_ok=True)

        # ä¿å­˜æ–‡ä»¶
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

        # åŒæ—¶ä¿å­˜åˆ°comprehensiveç›®å½•ä½œä¸ºlatestæ–‡ä»¶ï¼ˆä¿æŒå…¼å®¹ï¼‰
        self.comprehensive_dir.mkdir(parents=True, exist_ok=True)
        latest_path = self.comprehensive_dir / f"comprehensive_score_W{self.current_week}_latest.json"
        with open(latest_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

        # åŒæ—¶ä¿å­˜åˆ°å‘¨ç›®å½•ä½œä¸ºlatest
        week_latest_path = self.week_dir / f"comprehensive_score_W{self.current_week}_latest.json"
        with open(week_latest_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

        return output_path

    def clean_old_detailed_files(self, keep_hours=2):
        """
        æ¸…ç†æ—§çš„è¯¦ç»†æ‰“åˆ†æ–‡ä»¶ï¼Œé˜²æ­¢ç´¯ç§¯
        Args:
            keep_hours: ä¿ç•™å¤šå°‘å°æ—¶å†…çš„æ–‡ä»¶ï¼ˆé»˜è®¤2å°æ—¶ï¼‰
        """
        from datetime import datetime
        import shutil

        try:
            archive_dir = self.detailed_dir / f".archive_{datetime.now().strftime('%Y%m%d')}"
            archive_dir.mkdir(exist_ok=True)

            current_time = datetime.now()
            cleaned_count = 0

            # åªæ¸…ç†tmpå¼€å¤´çš„è¯¦ç»†æ‰“åˆ†æ–‡ä»¶
            for file in self.detailed_dir.glob('detailed_score_tmp*.json'):
                file_time = datetime.fromtimestamp(file.stat().st_mtime)
                age_hours = (current_time - file_time).total_seconds() / 3600

                if age_hours > keep_hours:
                    # ç§»åŠ¨åˆ°å½’æ¡£ç›®å½•è€Œä¸æ˜¯åˆ é™¤
                    archive_path = archive_dir / file.name
                    shutil.move(str(file), str(archive_path))
                    cleaned_count += 1
                    logger.debug(f"å½’æ¡£æ—§æ–‡ä»¶: {file.name} (ç”Ÿæˆäº {age_hours:.1f} å°æ—¶å‰)")

            if cleaned_count > 0:
                logger.info(f"ğŸ§¹ å·²å½’æ¡£ {cleaned_count} ä¸ªè¶…è¿‡ {keep_hours} å°æ—¶çš„å†å²æ–‡ä»¶")

        except Exception as e:
            logger.warning(f"æ¸…ç†å†å²æ–‡ä»¶æ—¶å‡ºé”™: {e}")

    def generate_from_all_detailed_results(self, excel_urls=None, expected_count=None) -> str:
        """
        æ‰¹é‡å¤„ç†ï¼šä»æ‰€æœ‰è¯¦ç»†æ‰“åˆ†ç»“æœç”Ÿæˆç»¼åˆæ‰“åˆ†
        æ”¯æŒå¤šæ–‡æ¡£èšåˆï¼Œç”ŸæˆNÃ—19çŸ©é˜µçƒ­åŠ›å›¾

        Args:
            excel_urls: å­—å…¸ï¼Œæ ¼å¼ {è¡¨æ ¼å: URL}
            expected_count: æœŸæœ›çš„æ–‡æ¡£æ•°é‡ï¼ˆå¦‚æœæä¾›ï¼Œåªå¤„ç†æœ€æ–°çš„Nä¸ªæ–‡ä»¶ï¼‰
        """
        import re
        from datetime import datetime

        # 0. é¦–å…ˆæ¸…ç†è¶…è¿‡2å°æ—¶çš„å†å²æ–‡ä»¶
        self.clean_old_detailed_files(keep_hours=2)

        # 1. æŸ¥æ‰¾å½“å‰æ—¶é—´æ®µå†…çš„æ‰€æœ‰è¯¦ç»†æ‰“åˆ†æ–‡ä»¶
        detailed_files = sorted(self.detailed_dir.glob('detailed_score_*.json'),
                              key=lambda x: x.stat().st_mtime, reverse=True)

        if not detailed_files:
            raise FileNotFoundError("æ²¡æœ‰æ‰¾åˆ°è¯¦ç»†æ‰“åˆ†æ–‡ä»¶")

        # 2. æ™ºèƒ½é€‰æ‹©æ‰¹æ¬¡æ–‡ä»¶ - å¢å¼ºç‰ˆè¿‡æ»¤æœºåˆ¶
        current_time = datetime.now()

        # ç¬¬ä¸€æ­¥ï¼šè¿‡æ»¤æ‰è¿‡æ—§çš„æ–‡ä»¶ï¼ˆè¶…è¿‡30åˆ†é’Ÿçš„ä¸€å¾‹æ’é™¤ï¼‰
        recent_files = []
        for file in detailed_files:
            file_time = datetime.fromtimestamp(file.stat().st_mtime)
            time_diff = (current_time - file_time).total_seconds()
            if time_diff <= 1800:  # 30åˆ†é’Ÿå†…
                recent_files.append(file)
            else:
                logger.debug(f"æ’é™¤æ—§æ–‡ä»¶: {file.name} (ç”Ÿæˆäº {int(time_diff/60)} åˆ†é’Ÿå‰)")

        # ç¬¬äºŒæ­¥ï¼šæ ¹æ®expected_counté€‰æ‹©æ–‡ä»¶
        if expected_count and expected_count > 0:
            # åªå¤„ç†æœ€æ–°çš„Nä¸ªæ–‡ä»¶ï¼ŒNä¸ºé…ç½®çš„æ–‡æ¡£æ•°é‡
            # ä½†å¿…é¡»ä»recent_filesä¸­é€‰æ‹©ï¼Œé¿å…é€‰åˆ°å†å²æ–‡ä»¶
            batch_files = recent_files[:expected_count] if recent_files else []

            # éªŒè¯æ–‡ä»¶æ•°é‡æ˜¯å¦åŒ¹é…
            if len(batch_files) < expected_count:
                logger.warning(f"âš ï¸ æœŸæœ› {expected_count} ä¸ªæ–‡ä»¶ï¼Œä½†åªæ‰¾åˆ° {len(batch_files)} ä¸ª30åˆ†é’Ÿå†…çš„æ–‡ä»¶")
            else:
                logger.info(f"âœ… æ ¹æ®é…ç½®æ–‡æ¡£æ•° {expected_count}ï¼Œé€‰æ‹©æœ€æ–°çš„ {len(batch_files)} ä¸ªæ–‡ä»¶")

            # è®°å½•é€‰ä¸­çš„æ–‡ä»¶æ—¶é—´
            for f in batch_files:
                file_time = datetime.fromtimestamp(f.stat().st_mtime)
                age_minutes = int((current_time - file_time).total_seconds() / 60)
                logger.info(f"  é€‰ä¸­: {f.name} (ç”Ÿæˆäº {age_minutes} åˆ†é’Ÿå‰)")
        else:
            # å¤‡ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨5åˆ†é’Ÿæ—¶é—´çª—å£ï¼ˆæ›´ä¸¥æ ¼ï¼‰
            batch_files = []
            for file in recent_files:
                file_time = datetime.fromtimestamp(file.stat().st_mtime)
                time_diff = (current_time - file_time).total_seconds()
                if time_diff <= 300:  # 5åˆ†é’Ÿå†…çš„æ–‡ä»¶
                    batch_files.append(file)

            if not batch_files and recent_files:
                # å¦‚æœæ²¡æœ‰5åˆ†é’Ÿå†…çš„æ–‡ä»¶ï¼Œä½¿ç”¨æœ€æ–°çš„2ä¸ªæ–‡ä»¶ï¼ˆä½†å¿…é¡»åœ¨30åˆ†é’Ÿå†…ï¼‰
                batch_files = recent_files[:2]
                logger.warning(f"æœªæ‰¾åˆ°5åˆ†é’Ÿå†…çš„æ–‡ä»¶ï¼Œä½¿ç”¨30åˆ†é’Ÿå†…æœ€æ–°çš„ {len(batch_files)} ä¸ªæ–‡ä»¶")
            elif not batch_files:
                logger.error("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•30åˆ†é’Ÿå†…çš„è¯¦ç»†æ‰“åˆ†æ–‡ä»¶")

        logger.info(f"æ‰¹é‡å¤„ç† {len(batch_files)} ä¸ªè¯¦ç»†æ‰“åˆ†æ–‡ä»¶")

        # 3. å¤„ç†æ¯ä¸ªæ–‡ä»¶ï¼Œæ”¶é›†æ•°æ®
        all_table_data = []
        all_heatmap_rows = []
        total_l1 = 0
        total_l2 = 0
        total_l3 = 0
        total_mods = 0

        for detailed_file in batch_files:
            logger.info(f"å¤„ç†æ–‡ä»¶: {detailed_file.name}")

            with open(detailed_file, 'r', encoding='utf-8') as f:
                detailed_data = json.load(f)

            # æå–è¡¨æ ¼ä¿¡æ¯
            table_name = self._extract_table_name(detailed_data)

            # ç”Ÿæˆçƒ­åŠ›å›¾çŸ©é˜µå’Œåˆ—ä¿®æ”¹æ•°æ®
            heatmap_matrix, column_modifications = self._process_detailed_scores(detailed_data)

            # ç´¯åŠ çƒ­åŠ›å›¾è¡Œï¼ˆæ¯ä¸ªæ–‡æ¡£ä¸€è¡Œï¼‰
            if heatmap_matrix and len(heatmap_matrix) > 0:
                all_heatmap_rows.append(heatmap_matrix[0])  # å–ç¬¬ä¸€è¡Œï¼ˆå•æ–‡æ¡£åªæœ‰ä¸€è¡Œï¼‰

            # ç»Ÿè®¡é£é™©åˆ†å¸ƒ
            risk_stats = self._calculate_risk_stats(detailed_data)
            total_l1 += risk_stats['l1_count']
            total_l2 += risk_stats['l2_count']
            total_l3 += risk_stats['l3_count']
            total_mods += risk_stats['total']

            # æ”¶é›†è¡¨æ ¼æ•°æ®
            table_info = {
                'table_name': table_name,
                'modified_rows': risk_stats['total'],
                'column_modifications': column_modifications,
                'excel_url': excel_urls.get(table_name) if excel_urls else None
            }
            all_table_data.append(table_info)

        # 4. æ„å»ºç»¼åˆæ‰“åˆ†æ•°æ®ç»“æ„ï¼ˆå¤šæ–‡æ¡£ç‰ˆæœ¬ï¼‰
        comprehensive_data = {
            "metadata": {
                "version": "2.0",
                "timestamp": datetime.now().isoformat(),
                "week": f"W{self.current_week}",
                "generator": "auto_comprehensive_generator_batch",
                "source_type": "multi_document_scoring",
                "baseline_week": f"W{self._get_baseline_week()}",
                "comparison_week": f"W{self.current_week}"
            },
            "summary": {
                "total_tables": len(all_table_data),  # åŠ¨æ€è®¡ç®—è¡¨æ ¼æ•°é‡
                "total_columns": 19,
                "total_modifications": total_mods,
                "l1_modifications": total_l1,
                "l2_modifications": total_l2,
                "l3_modifications": total_l3,
                "overall_risk_score": self._calculate_overall_risk({
                    'l1_count': total_l1,
                    'l2_count': total_l2,
                    'l3_count': total_l3,
                    'total': total_mods
                }),
                "processing_status": "complete",
                "data_source": "batch_auto_generated"
            },
            "table_names": [item['table_name'] for item in all_table_data],
            "column_names": self.STANDARD_COLUMNS,
            "heatmap_data": {
                "matrix": all_heatmap_rows,  # NÃ—19çŸ©é˜µ
                "rows": len(all_heatmap_rows),  # åŠ¨æ€è¡Œæ•°
                "cols": 19,
                "generation_method": "batch_risk_based_auto",
                "color_distribution": self._calculate_color_distribution(all_heatmap_rows)
            },
            "table_details": {}
        }

        # 5. æ·»åŠ æ¯ä¸ªè¡¨æ ¼çš„è¯¦ç»†ä¿¡æ¯
        for table_info in all_table_data:
            table_name = table_info['table_name']
            comprehensive_data["table_details"][table_name] = {
                "total_rows": 270,  # å¯ä»¥ä»è¯¦ç»†æ–‡ä»¶ä¸­è·å–
                "modified_rows": table_info['modified_rows'],
                "added_rows": 0,
                "deleted_rows": 0
            }
            if table_info['excel_url']:
                comprehensive_data["table_details"][table_name]["excel_url"] = table_info['excel_url']

        # 6. å¦‚æœæä¾›äº†excel_urlsï¼Œæ·»åŠ åˆ°æ•°æ®ä¸­
        if excel_urls:
            comprehensive_data["excel_urls"] = excel_urls

        # 7. ä¿å­˜æ–‡ä»¶
        output_path = self._save_comprehensive_file(comprehensive_data)

        logger.info(f"âœ… æ‰¹é‡ç»¼åˆæ‰“åˆ†å·²ç”Ÿæˆ: {output_path}")
        logger.info(f"   åŒ…å« {len(all_table_data)} ä¸ªè¡¨æ ¼ï¼Œçƒ­åŠ›å›¾çŸ©é˜µ: {len(all_heatmap_rows)}Ã—19")

        return str(output_path)

    def _calculate_color_distribution(self, matrix):
        """è®¡ç®—é¢œè‰²åˆ†å¸ƒç»Ÿè®¡"""
        distribution = {
            "red_0.9": 0,
            "orange_0.6": 0,
            "green_0.3": 0,
            "blue_0.05": 0
        }

        for row in matrix:
            distribution["red_0.9"] += row.count(0.90)
            distribution["orange_0.6"] += row.count(0.60)
            distribution["green_0.3"] += row.count(0.30)
            distribution["blue_0.05"] += row.count(0.05)

        return distribution


if __name__ == "__main__":
    generator = AutoComprehensiveGenerator()
    output = generator.generate_from_latest_results()
    print(f"âœ… ç»¼åˆæ‰“åˆ†å·²ç”Ÿæˆ: {output}")