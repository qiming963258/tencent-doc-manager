#!/usr/bin/env python3
"""
å·¥ä½œæµé›†æˆæ¨¡å— - æ•´åˆä¸‹è½½ã€å¤„ç†ã€ä¸Šä¼ çš„å®Œæ•´æµç¨‹

æ ¸å¿ƒåŠŸèƒ½ï¼š
1. è‡ªåŠ¨åŒ–å®Œæ•´å·¥ä½œæµ
2. åœ¨é€‚å½“æ—¶æœºè§¦å‘ä¸Šä¼ 
3. ç®¡ç†æ–‡ä»¶ä¸URLçš„æ˜ å°„å…³ç³»
4. ç»Ÿä¸€çš„Cookieå’Œé…ç½®ç®¡ç†

å·¥ä½œæµç¨‹ï¼š
1. ä¸‹è½½CSVæ–‡ä»¶ â†’ 2. æ•°æ®å¯¹æ¯”åˆ†æ â†’ 3. ç”ŸæˆExcelæŠ¥å‘Š â†’ 4. è‡ªåŠ¨ä¸Šä¼  â†’ 5. è®°å½•URLæ˜ å°„
"""

import os
import json
import asyncio
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional, Any, Tuple
import logging

# å¯¼å…¥å„ä¸ªæ¨¡å—
from .week_time_manager import WeekTimeManager
from .tencent_doc_upload_production_v3 import sync_upload_v3
from .upload_url_manager import get_manager as get_url_manager
from .cookie_manager import CookieManager

logger = logging.getLogger(__name__)


class WorkflowIntegration:
    """å·¥ä½œæµé›†æˆç®¡ç†å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–å·¥ä½œæµç®¡ç†å™¨"""
        self.project_root = Path('/root/projects/tencent-doc-manager')
        
        # åˆå§‹åŒ–å„ä¸ªç®¡ç†å™¨
        self.week_manager = WeekTimeManager()
        self.url_manager = get_url_manager()
        self.cookie_manager = CookieManager()
        
        # é…ç½®è·¯å¾„
        self.csv_dir = self.project_root / 'csv_versions'
        self.excel_dir = self.project_root / 'excel_outputs'
        self.comparison_dir = self.project_root / 'comparison_results'
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        for dir_path in [self.csv_dir, self.excel_dir, self.comparison_dir]:
            dir_path.mkdir(parents=True, exist_ok=True)
        
        # å·¥ä½œæµé…ç½®
        self.config = self._load_config()
        
    def _load_config(self) -> Dict[str, Any]:
        """åŠ è½½å·¥ä½œæµé…ç½®"""
        config_file = self.project_root / 'workflow_config.json'
        
        default_config = {
            'auto_upload': True,  # æ˜¯å¦è‡ªåŠ¨ä¸Šä¼ 
            'upload_delay': 5,    # ä¸Šä¼ å»¶è¿Ÿï¼ˆç§’ï¼‰
            'batch_upload': False,  # æ˜¯å¦æ‰¹é‡ä¸Šä¼ 
            'max_batch_size': 5,   # æœ€å¤§æ‰¹é‡å¤§å°
            'preserve_local': True,  # æ˜¯å¦ä¿ç•™æœ¬åœ°æ–‡ä»¶
            'upload_patterns': [   # éœ€è¦ä¸Šä¼ çš„æ–‡ä»¶æ¨¡å¼
                'risk_analysis_report_*.xlsx',
                'tencent_*_final_W*.xlsx',
                'comparison_report_*.xlsx'
            ]
        }
        
        if config_file.exists():
            try:
                with open(config_file, 'r', encoding='utf-8') as f:
                    loaded_config = json.load(f)
                    default_config.update(loaded_config)
            except Exception as e:
                logger.warning(f"åŠ è½½é…ç½®å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤é…ç½®: {e}")
        
        return default_config
    
    async def process_csv_download(self, doc_url: str, doc_name: str) -> str:
        """
        å¤„ç†CSVä¸‹è½½
        
        Args:
            doc_url: è…¾è®¯æ–‡æ¡£URL
            doc_name: æ–‡æ¡£åç§°
        
        Returns:
            ä¸‹è½½çš„æ–‡ä»¶è·¯å¾„
        """
        from .tencent_export_automation import TencentExporter
        
        logger.info(f"å¼€å§‹ä¸‹è½½: {doc_name}")
        
        # è·å–Cookie
        cookie = await self.cookie_manager.get_best_cookie()
        if not cookie:
            raise ValueError("æ²¡æœ‰å¯ç”¨çš„Cookie")
        
        # åˆå§‹åŒ–å¯¼å‡ºå™¨
        exporter = TencentExporter()
        
        # ç”Ÿæˆæ–‡ä»¶å
        week_info = self.week_manager.get_current_week_info()
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        version_type = self._determine_version_type()
        
        filename = f"tencent_{doc_name}_{timestamp}_{version_type}_W{week_info['week_number']}.csv"
        file_path = self.csv_dir / f"{week_info['year']}_W{week_info['week_number']}" / version_type / filename
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        file_path.parent.mkdir(parents=True, exist_ok=True)
        
        # æ‰§è¡Œä¸‹è½½
        success = await exporter.export_to_csv(doc_url, str(file_path), cookie)
        
        if success:
            logger.info(f"âœ… ä¸‹è½½æˆåŠŸ: {file_path}")
            return str(file_path)
        else:
            raise Exception(f"ä¸‹è½½å¤±è´¥: {doc_name}")
    
    def _determine_version_type(self) -> str:
        """
        æ ¹æ®å½“å‰æ—¶é—´ç¡®å®šç‰ˆæœ¬ç±»å‹
        
        Returns:
            baseline/midweek/final
        """
        week_info = self.week_manager.get_current_week_info()
        day_of_week = datetime.now().weekday()
        
        if day_of_week == 0:  # å‘¨ä¸€
            return 'baseline'
        elif day_of_week in [2, 3]:  # å‘¨ä¸‰ã€å‘¨å››
            return 'midweek'
        elif day_of_week >= 5:  # å‘¨å…­ã€å‘¨æ—¥
            return 'final'
        else:
            return 'midweek'  # é»˜è®¤
    
    async def process_comparison(self, baseline_file: str, target_file: str) -> Dict[str, Any]:
        """
        å¤„ç†æ–‡ä»¶å¯¹æ¯”
        
        Args:
            baseline_file: åŸºå‡†æ–‡ä»¶è·¯å¾„
            target_file: ç›®æ ‡æ–‡ä»¶è·¯å¾„
        
        Returns:
            å¯¹æ¯”ç»“æœ
        """
        from .auto_comparison_task import CSVAutoComparator
        
        logger.info(f"å¼€å§‹å¯¹æ¯”: {Path(baseline_file).name} vs {Path(target_file).name}")
        
        comparator = CSVAutoComparator()
        result = await comparator.compare(baseline_file, target_file)
        
        # ä¿å­˜å¯¹æ¯”ç»“æœ
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        result_file = self.comparison_dir / f"comparison_{timestamp}.json"
        
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        
        logger.info(f"âœ… å¯¹æ¯”å®Œæˆï¼Œç»“æœä¿å­˜åˆ°: {result_file}")
        return result
    
    async def process_excel_generation(self, comparison_result: Dict[str, Any]) -> str:
        """
        ç”ŸæˆExcelæŠ¥å‘Š
        
        Args:
            comparison_result: å¯¹æ¯”ç»“æœ
        
        Returns:
            ç”Ÿæˆçš„Excelæ–‡ä»¶è·¯å¾„
        """
        logger.info("å¼€å§‹ç”ŸæˆExcelæŠ¥å‘Š")
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        excel_file = self.excel_dir / f"risk_analysis_report_{timestamp}.xlsx"
        
        # è¿™é‡Œè°ƒç”¨Excelç”Ÿæˆé€»è¾‘ï¼ˆä½¿ç”¨MCPæˆ–å…¶ä»–æ–¹æ³•ï¼‰
        # ç¤ºä¾‹ä»£ç ï¼Œå®é™…éœ€è¦æ ¹æ®å…·ä½“å®ç°è°ƒæ•´
        try:
            # ç”ŸæˆExcelæŠ¥å‘Šçš„å…·ä½“å®ç°
            # ...
            pass
        except Exception as e:
            logger.error(f"ç”ŸæˆExcelå¤±è´¥: {e}")
            raise
        
        logger.info(f"âœ… ExcelæŠ¥å‘Šç”ŸæˆæˆåŠŸ: {excel_file}")
        return str(excel_file)
    
    async def process_upload(self, file_path: str, auto_upload: bool = None) -> Optional[str]:
        """
        å¤„ç†æ–‡ä»¶ä¸Šä¼ 
        
        Args:
            file_path: è¦ä¸Šä¼ çš„æ–‡ä»¶è·¯å¾„
            auto_upload: æ˜¯å¦è‡ªåŠ¨ä¸Šä¼ ï¼ŒNoneåˆ™ä½¿ç”¨é…ç½®
        
        Returns:
            ä¸Šä¼ åçš„æ–‡æ¡£URLï¼Œå¦‚æœæœªä¸Šä¼ åˆ™è¿”å›None
        """
        if auto_upload is None:
            auto_upload = self.config.get('auto_upload', True)
        
        if not auto_upload:
            logger.info(f"è·³è¿‡ä¸Šä¼ ï¼ˆauto_upload=Falseï¼‰: {file_path}")
            return None
        
        # æ£€æŸ¥æ˜¯å¦å·²ç»ä¸Šä¼ è¿‡
        existing = self.url_manager.get_by_file(file_path)
        if existing:
            logger.info(f"æ–‡ä»¶å·²ä¸Šä¼ è¿‡: {existing.doc_url}")
            return existing.doc_url
        
        logger.info(f"å¼€å§‹ä¸Šä¼ : {Path(file_path).name}")
        
        # è·å–Cookie
        cookie = await self.cookie_manager.get_best_cookie()
        if not cookie:
            logger.error("æ²¡æœ‰å¯ç”¨çš„Cookieï¼Œè·³è¿‡ä¸Šä¼ ")
            return None
        
        # æ·»åŠ ä¸Šä¼ å»¶è¿Ÿ
        delay = self.config.get('upload_delay', 5)
        if delay > 0:
            logger.info(f"ç­‰å¾… {delay} ç§’åå¼€å§‹ä¸Šä¼ ...")
            await asyncio.sleep(delay)
        
        # æ‰§è¡Œä¸Šä¼ 
        try:
            result = sync_upload_v3(
                cookie_string=cookie,
                file_path=file_path,
                headless=True
            )
            
            if result.get('success'):
                doc_url = result.get('url')
                
                # è®°å½•æ˜ å°„å…³ç³»
                self.url_manager.add_mapping(
                    file_path=file_path,
                    doc_url=doc_url,
                    metadata={
                        'upload_time': datetime.now().isoformat(),
                        'upload_method': 'v3',
                        'doc_name': result.get('doc_name'),
                        'message': result.get('message')
                    }
                )
                
                logger.info(f"âœ… ä¸Šä¼ æˆåŠŸ: {doc_url}")
                return doc_url
            else:
                logger.error(f"ä¸Šä¼ å¤±è´¥: {result.get('message')}")
                return None
                
        except Exception as e:
            logger.error(f"ä¸Šä¼ å¼‚å¸¸: {e}")
            return None
    
    async def run_complete_workflow(self, doc_urls: List[str]) -> Dict[str, Any]:
        """
        è¿è¡Œå®Œæ•´å·¥ä½œæµ
        
        Args:
            doc_urls: è¦å¤„ç†çš„è…¾è®¯æ–‡æ¡£URLåˆ—è¡¨
        
        Returns:
            å·¥ä½œæµæ‰§è¡Œç»“æœ
        """
        logger.info("=" * 60)
        logger.info("ğŸš€ å¼€å§‹æ‰§è¡Œå®Œæ•´å·¥ä½œæµ")
        logger.info("=" * 60)
        
        results = {
            'start_time': datetime.now().isoformat(),
            'downloads': [],
            'comparisons': [],
            'excel_files': [],
            'uploads': [],
            'errors': []
        }
        
        try:
            # æ­¥éª¤1: ä¸‹è½½æ‰€æœ‰CSVæ–‡ä»¶
            downloaded_files = []
            for url in doc_urls:
                try:
                    file_path = await self.process_csv_download(url, f"doc_{len(downloaded_files)+1}")
                    downloaded_files.append(file_path)
                    results['downloads'].append({
                        'url': url,
                        'file': file_path,
                        'status': 'success'
                    })
                except Exception as e:
                    logger.error(f"ä¸‹è½½å¤±è´¥ {url}: {e}")
                    results['errors'].append(str(e))
            
            # æ­¥éª¤2: æ‰§è¡Œå¯¹æ¯”åˆ†æ
            if len(downloaded_files) >= 2:
                comparison = await self.process_comparison(
                    downloaded_files[0],  # baseline
                    downloaded_files[-1]  # target
                )
                results['comparisons'].append(comparison)
            
            # æ­¥éª¤3: ç”ŸæˆExcelæŠ¥å‘Š
            if results['comparisons']:
                excel_file = await self.process_excel_generation(results['comparisons'][0])
                results['excel_files'].append(excel_file)
                
                # æ­¥éª¤4: è‡ªåŠ¨ä¸Šä¼ Excel
                if self.config.get('auto_upload'):
                    upload_url = await self.process_upload(excel_file)
                    if upload_url:
                        results['uploads'].append({
                            'file': excel_file,
                            'url': upload_url,
                            'status': 'success'
                        })
            
            # æ­¥éª¤5: æ‰¹é‡ä¸Šä¼ å…¶ä»–æ–‡ä»¶ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if self.config.get('batch_upload'):
                await self._batch_upload_files(results)
            
        except Exception as e:
            logger.error(f"å·¥ä½œæµæ‰§è¡Œå¤±è´¥: {e}")
            results['errors'].append(str(e))
        
        results['end_time'] = datetime.now().isoformat()
        
        # ä¿å­˜å·¥ä½œæµç»“æœ
        self._save_workflow_result(results)
        
        logger.info("=" * 60)
        logger.info("âœ… å·¥ä½œæµæ‰§è¡Œå®Œæˆ")
        logger.info(f"  ä¸‹è½½: {len(results['downloads'])} ä¸ªæ–‡ä»¶")
        logger.info(f"  å¯¹æ¯”: {len(results['comparisons'])} æ¬¡")
        logger.info(f"  Excel: {len(results['excel_files'])} ä¸ª")
        logger.info(f"  ä¸Šä¼ : {len(results['uploads'])} ä¸ª")
        logger.info("=" * 60)
        
        return results
    
    async def _batch_upload_files(self, results: Dict[str, Any]):
        """æ‰¹é‡ä¸Šä¼ æ–‡ä»¶"""
        batch_size = self.config.get('max_batch_size', 5)
        patterns = self.config.get('upload_patterns', [])
        
        # æŸ¥æ‰¾ç¬¦åˆæ¨¡å¼çš„æ–‡ä»¶
        files_to_upload = []
        for pattern in patterns:
            for file_path in self.excel_dir.glob(pattern):
                # æ£€æŸ¥æ˜¯å¦å·²ä¸Šä¼ 
                if not self.url_manager.get_by_file(str(file_path)):
                    files_to_upload.append(str(file_path))
        
        # æ‰¹é‡ä¸Šä¼ 
        for i in range(0, len(files_to_upload), batch_size):
            batch = files_to_upload[i:i+batch_size]
            logger.info(f"æ‰¹é‡ä¸Šä¼  {len(batch)} ä¸ªæ–‡ä»¶")
            
            for file_path in batch:
                url = await self.process_upload(file_path)
                if url:
                    results['uploads'].append({
                        'file': file_path,
                        'url': url,
                        'status': 'batch'
                    })
    
    def _save_workflow_result(self, results: Dict[str, Any]):
        """ä¿å­˜å·¥ä½œæµç»“æœ"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        result_file = self.project_root / 'workflow_results' / f"workflow_{timestamp}.json"
        
        result_file.parent.mkdir(parents=True, exist_ok=True)
        
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        logger.info(f"å·¥ä½œæµç»“æœä¿å­˜åˆ°: {result_file}")
    
    def get_upload_url_for_file(self, file_path: str) -> Optional[str]:
        """
        è·å–æ–‡ä»¶å¯¹åº”çš„ä¸Šä¼ URL
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
        
        Returns:
            è…¾è®¯æ–‡æ¡£URLï¼Œå¦‚æœæœªæ‰¾åˆ°åˆ™è¿”å›None
        """
        record = self.url_manager.get_by_file(file_path)
        return record.doc_url if record else None
    
    def get_upload_history(self, limit: int = 10) -> List[Dict[str, Any]]:
        """
        è·å–ä¸Šä¼ å†å²
        
        Args:
            limit: è¿”å›è®°å½•æ•°é‡é™åˆ¶
        
        Returns:
            ä¸Šä¼ å†å²è®°å½•åˆ—è¡¨
        """
        stats = self.url_manager.get_statistics()
        return stats.get('recent_uploads', [])[:limit]


# å•ä¾‹å®ä¾‹
_workflow_instance: Optional[WorkflowIntegration] = None

def get_workflow_manager() -> WorkflowIntegration:
    """è·å–å·¥ä½œæµç®¡ç†å™¨å•ä¾‹"""
    global _workflow_instance
    if _workflow_instance is None:
        _workflow_instance = WorkflowIntegration()
    return _workflow_instance


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    import asyncio
    
    async def test_workflow():
        manager = get_workflow_manager()
        
        # æµ‹è¯•ä¸Šä¼ å•ä¸ªæ–‡ä»¶
        test_file = "/root/projects/tencent-doc-manager/excel_outputs/risk_analysis_report_20250819_024132.xlsx"
        if Path(test_file).exists():
            url = await manager.process_upload(test_file)
            if url:
                print(f"ä¸Šä¼ æˆåŠŸ: {url}")
            else:
                print("ä¸Šä¼ å¤±è´¥")
        
        # è·å–ä¸Šä¼ å†å²
        history = manager.get_upload_history()
        print(f"\næœ€è¿‘ä¸Šä¼ å†å²:")
        for item in history:
            print(f"  - {item['file_name']}: {item['doc_url']}")
    
    # è¿è¡Œæµ‹è¯•
    asyncio.run(test_workflow())