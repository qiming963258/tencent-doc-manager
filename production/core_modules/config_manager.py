#!/usr/bin/env python3
"""
ç»Ÿä¸€é…ç½®ç®¡ç†å™¨ - ç®¡ç†ç³»ç»Ÿæ‰€æœ‰é…ç½®æ–‡ä»¶

æ ¸å¿ƒåŠŸèƒ½ï¼š
1. è‡ªåŠ¨åˆå§‹åŒ–ç¼ºå¤±çš„é…ç½®æ–‡ä»¶
2. ç»Ÿä¸€ç®¡ç†æ‰€æœ‰é…ç½®æ–‡ä»¶
3. æä¾›é…ç½®éªŒè¯å’ŒåŒæ­¥æœºåˆ¶
4. Cookieæœ‰æ•ˆæœŸè‡ªåŠ¨æ£€æµ‹
5. é…ç½®æ–‡ä»¶å¤‡ä»½å’Œæ¢å¤

é…ç½®æ–‡ä»¶æ¸…å•ï¼š
- config.json: Cookieä¸»å­˜å‚¨
- download_config.json: æ–‡æ¡£é“¾æ¥é…ç½®
- monitor_config.json: ç›‘æ§è®¾ç½®
- schedule_tasks.json: å®šæ—¶ä»»åŠ¡
- real_documents.json: çœŸå®æ–‡æ¡£é…ç½®
"""

import json
import os
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple
import logging
import shutil
import hashlib

logger = logging.getLogger(__name__)


class ConfigManager:
    """ç»Ÿä¸€é…ç½®ç®¡ç†å™¨"""
    
    def __init__(self, project_root: str = None):
        """
        åˆå§‹åŒ–é…ç½®ç®¡ç†å™¨
        
        Args:
            project_root: é¡¹ç›®æ ¹ç›®å½•
        """
        if project_root is None:
            project_root = '/root/projects/tencent-doc-manager'
        
        self.project_root = Path(project_root)
        self.config_dir = self.project_root / 'config'
        self.production_config_dir = self.project_root / 'production' / 'config'
        
        # ç¡®ä¿é…ç½®ç›®å½•å­˜åœ¨
        self.config_dir.mkdir(parents=True, exist_ok=True)
        self.production_config_dir.mkdir(parents=True, exist_ok=True)
        
        # é…ç½®æ–‡ä»¶æ˜ å°„
        self.config_files = {
            'cookie': self.project_root / 'config.json',
            'download': self.config_dir / 'download_config.json',
            'monitor': self.config_dir / 'monitor_config.json',
            'schedule': self.config_dir / 'schedule_tasks.json',
            'documents': self.production_config_dir / 'real_documents.json'
        }
        
        # é»˜è®¤é…ç½®æ¨¡æ¿
        self.default_configs = self._get_default_configs()
        
        # åˆå§‹åŒ–æ‰€æœ‰é…ç½®æ–‡ä»¶
        self.initialize_all_configs()
        
        # åŠ è½½æ‰€æœ‰é…ç½®
        self.configs = self.load_all_configs()
    
    def _get_default_configs(self) -> Dict[str, Dict]:
        """è·å–é»˜è®¤é…ç½®æ¨¡æ¿"""
        return {
            'cookie': {
                'cookie': '',
                'last_updated': datetime.now().isoformat(),
                'cookie_id': '',
                'set_time': datetime.now().isoformat()
            },
            'download': {
                'document_links': [
                    {
                        'name': 'å‰¯æœ¬-æµ‹è¯•ç‰ˆæœ¬-å‡ºå›½é”€å”®è®¡åˆ’è¡¨',
                        'url': 'https://docs.qq.com/sheet/DWEFNU25TemFnZXJN',
                        'doc_id': 'DWEFNU25TemFnZXJN',
                        'description': 'å‡ºå›½é”€å”®æ•°æ®ç›‘æ§'
                    },
                    {
                        'name': 'å‰¯æœ¬-æµ‹è¯•ç‰ˆæœ¬-å›å›½é”€å”®è®¡åˆ’è¡¨',
                        'url': 'https://docs.qq.com/sheet/DWGZDZkxpaGVQaURr',
                        'doc_id': 'DWGZDZkxpaGVQaURr',
                        'description': 'å›å›½é”€å”®æ•°æ®ç›‘æ§'
                    },
                    {
                        'name': 'æµ‹è¯•ç‰ˆæœ¬-å°çº¢ä¹¦éƒ¨é—¨',
                        'url': 'https://docs.qq.com/sheet/DWFJzdWNwd0RGbU5R',
                        'doc_id': 'DWFJzdWNwd0RGbU5R',
                        'description': 'å°çº¢ä¹¦éƒ¨é—¨æ•°æ®ç›‘æ§'
                    }
                ],
                'download_format': 'csv',
                'schedule': {
                    'enabled': False,
                    'interval': 'weekly',
                    'last_download': None
                },
                'download_status': 'æœªé…ç½®',
                'last_updated': datetime.now().isoformat()
            },
            'monitor': {
                'alert_settings': {
                    'threshold': 'L1',
                    'levels': ['L1çº§åˆ«ä¿®æ”¹', 'é«˜é£é™©ä¿®æ”¹', 'æ‰€æœ‰ä¿®æ”¹'],
                    'current_level': 'L1çº§åˆ«ä¿®æ”¹'
                },
                'notification': {
                    'enabled': True,
                    'channels': ['console', 'log'],
                    'alert_frequency': 300
                },
                'display': {
                    'refresh_interval': 300,
                    'auto_refresh': False,
                    'heatmap_smoothing': True,
                    'show_tooltips': True,
                    'color_scheme': 'scientific'
                },
                'comparison': {
                    'auto_comparison': True,
                    'comparison_interval': 3600,
                    'baseline_retention_days': 30,
                    'generate_reports': True
                },
                'last_updated': datetime.now().isoformat()
            },
            'schedule': {
                'preset_tasks': [
                    {
                        'task_id': 'weekly_baseline_download',
                        'name': 'å‘¨äºŒåŸºå‡†ä¸‹è½½ä»»åŠ¡',
                        'description': 'æ¯å‘¨äºŒ12:00ä¸‹è½½åŸºå‡†CSVæ–‡ä»¶',
                        'schedule': {
                            'type': 'simple',
                            'expression': 'weekly:tuesday:12:00',
                            'timezone': 'Asia/Shanghai'
                        },
                        'enabled': False
                    },
                    {
                        'task_id': 'weekly_midweek_update',
                        'name': 'å‘¨å››ä¸­æœŸæ›´æ–°ä»»åŠ¡',
                        'description': 'æ¯å‘¨å››09:00ä¸‹è½½å…¨éƒ¨è¡¨æ ¼CSVå¹¶æ›´æ–°ç³»ç»Ÿ',
                        'schedule': {
                            'type': 'simple',
                            'expression': 'weekly:thursday:09:00',
                            'timezone': 'Asia/Shanghai'
                        },
                        'enabled': False
                    },
                    {
                        'task_id': 'weekly_full_update',
                        'name': 'å‘¨å…­å®Œæ•´æ›´æ–°ä»»åŠ¡',
                        'description': 'æ¯å‘¨å…­19:00å®Œæ•´æ›´æ–°ç³»ç»Ÿ',
                        'schedule': {
                            'type': 'simple',
                            'expression': 'weekly:saturday:19:00',
                            'timezone': 'Asia/Shanghai'
                        },
                        'enabled': False
                    }
                ],
                'global_settings': {
                    'max_concurrent_tasks': 2,
                    'retry_failed_tasks': True,
                    'retry_count': 3,
                    'log_level': 'INFO'
                }
            },
            'documents': {
                'documents': [
                    {
                        'name': 'å‰¯æœ¬-æµ‹è¯•ç‰ˆæœ¬-å‡ºå›½é”€å”®è®¡åˆ’è¡¨',
                        'url': 'https://docs.qq.com/sheet/DWEFNU25TemFnZXJN',
                        'doc_id': 'DWEFNU25TemFnZXJN',
                        'csv_pattern': 'test',
                        'description': 'å‡ºå›½é”€å”®æ•°æ®ç›‘æ§'
                    },
                    {
                        'name': 'å‰¯æœ¬-æµ‹è¯•ç‰ˆæœ¬-å›å›½é”€å”®è®¡åˆ’è¡¨',
                        'url': 'https://docs.qq.com/sheet/DWGZDZkxpaGVQaURr',
                        'doc_id': 'DWGZDZkxpaGVQaURr',
                        'csv_pattern': 'realtest',
                        'description': 'å›å›½é”€å”®æ•°æ®ç›‘æ§'
                    },
                    {
                        'name': 'æµ‹è¯•ç‰ˆæœ¬-å°çº¢ä¹¦éƒ¨é—¨',
                        'url': 'https://docs.qq.com/sheet/DWFJzdWNwd0RGbU5R',
                        'doc_id': 'DWFJzdWNwd0RGbU5R',
                        'csv_pattern': 'test_data',
                        'description': 'å°çº¢ä¹¦éƒ¨é—¨æ•°æ®ç›‘æ§'
                    }
                ],
                'paste_format': 'ã€è…¾è®¯æ–‡æ¡£ã€‘{name}\\n{url}',
                'max_documents': None
            }
        }
    
    def initialize_all_configs(self):
        """åˆå§‹åŒ–æ‰€æœ‰é…ç½®æ–‡ä»¶ï¼ˆå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»ºï¼‰"""
        initialized = []
        
        for config_name, config_path in self.config_files.items():
            if not config_path.exists():
                # åˆ›å»ºé»˜è®¤é…ç½®
                default_config = self.default_configs.get(config_name, {})
                
                try:
                    with open(config_path, 'w', encoding='utf-8') as f:
                        json.dump(default_config, f, ensure_ascii=False, indent=2)
                    
                    initialized.append(config_name)
                    logger.info(f"âœ… åˆå§‹åŒ–é…ç½®æ–‡ä»¶: {config_path}")
                except Exception as e:
                    logger.error(f"âŒ åˆ›å»ºé…ç½®æ–‡ä»¶å¤±è´¥ {config_path}: {e}")
        
        if initialized:
            logger.info(f"ğŸ“ åˆå§‹åŒ–äº† {len(initialized)} ä¸ªé…ç½®æ–‡ä»¶: {', '.join(initialized)}")
        else:
            logger.info("âœ… æ‰€æœ‰é…ç½®æ–‡ä»¶å·²å­˜åœ¨")
    
    def load_all_configs(self) -> Dict[str, Dict]:
        """åŠ è½½æ‰€æœ‰é…ç½®æ–‡ä»¶"""
        configs = {}
        
        for config_name, config_path in self.config_files.items():
            if config_path.exists():
                try:
                    with open(config_path, 'r', encoding='utf-8') as f:
                        configs[config_name] = json.load(f)
                except Exception as e:
                    logger.error(f"åŠ è½½é…ç½®å¤±è´¥ {config_name}: {e}")
                    configs[config_name] = self.default_configs.get(config_name, {})
            else:
                configs[config_name] = self.default_configs.get(config_name, {})
        
        return configs
    
    def save_config(self, config_name: str, data: Dict) -> bool:
        """
        ä¿å­˜å•ä¸ªé…ç½®æ–‡ä»¶
        
        Args:
            config_name: é…ç½®åç§°
            data: é…ç½®æ•°æ®
        
        Returns:
            æ˜¯å¦æˆåŠŸä¿å­˜
        """
        if config_name not in self.config_files:
            logger.error(f"æœªçŸ¥çš„é…ç½®ç±»å‹: {config_name}")
            return False
        
        config_path = self.config_files[config_name]
        
        # å¤‡ä»½åŸé…ç½®
        self.backup_config(config_name)
        
        try:
            # æ›´æ–°æœ€åä¿®æ”¹æ—¶é—´
            data['last_updated'] = datetime.now().isoformat()
            
            with open(config_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            
            # æ›´æ–°å†…å­˜ä¸­çš„é…ç½®
            self.configs[config_name] = data
            
            logger.info(f"âœ… ä¿å­˜é…ç½® {config_name}: {config_path}")
            return True
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜é…ç½®å¤±è´¥ {config_name}: {e}")
            return False
    
    def backup_config(self, config_name: str):
        """å¤‡ä»½é…ç½®æ–‡ä»¶"""
        if config_name not in self.config_files:
            return
        
        config_path = self.config_files[config_name]
        if not config_path.exists():
            return
        
        # åˆ›å»ºå¤‡ä»½ç›®å½•
        backup_dir = self.config_dir / 'backup'
        backup_dir.mkdir(exist_ok=True)
        
        # ç”Ÿæˆå¤‡ä»½æ–‡ä»¶å
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        backup_file = backup_dir / f"{config_name}_{timestamp}.json"
        
        try:
            shutil.copy2(config_path, backup_file)
            logger.debug(f"å¤‡ä»½é…ç½® {config_name} åˆ° {backup_file}")
            
            # æ¸…ç†æ—§å¤‡ä»½ï¼ˆä¿ç•™æœ€è¿‘10ä¸ªï¼‰
            self._cleanup_old_backups(backup_dir, config_name, keep=10)
        except Exception as e:
            logger.warning(f"å¤‡ä»½é…ç½®å¤±è´¥ {config_name}: {e}")
    
    def _cleanup_old_backups(self, backup_dir: Path, config_name: str, keep: int = 10):
        """æ¸…ç†æ—§å¤‡ä»½æ–‡ä»¶"""
        pattern = f"{config_name}_*.json"
        backups = sorted(backup_dir.glob(pattern))
        
        if len(backups) > keep:
            for backup_file in backups[:-keep]:
                try:
                    backup_file.unlink()
                    logger.debug(f"åˆ é™¤æ—§å¤‡ä»½: {backup_file}")
                except:
                    pass
    
    def get_config(self, config_name: str) -> Optional[Dict]:
        """è·å–é…ç½®"""
        return self.configs.get(config_name)
    
    def update_config(self, config_name: str, updates: Dict, merge: bool = True) -> bool:
        """
        æ›´æ–°é…ç½®
        
        Args:
            config_name: é…ç½®åç§°
            updates: æ›´æ–°å†…å®¹
            merge: æ˜¯å¦åˆå¹¶æ›´æ–°ï¼ˆTrueï¼‰æˆ–å®Œå…¨æ›¿æ¢ï¼ˆFalseï¼‰
        
        Returns:
            æ˜¯å¦æˆåŠŸæ›´æ–°
        """
        current_config = self.get_config(config_name)
        if current_config is None:
            logger.error(f"é…ç½®ä¸å­˜åœ¨: {config_name}")
            return False
        
        if merge:
            # æ·±åº¦åˆå¹¶é…ç½®
            new_config = self._deep_merge(current_config, updates)
        else:
            new_config = updates
        
        return self.save_config(config_name, new_config)
    
    def _deep_merge(self, base: Dict, updates: Dict) -> Dict:
        """æ·±åº¦åˆå¹¶å­—å…¸"""
        result = base.copy()
        
        for key, value in updates.items():
            if key in result and isinstance(result[key], dict) and isinstance(value, dict):
                result[key] = self._deep_merge(result[key], value)
            else:
                result[key] = value
        
        return result
    
    def validate_cookie(self) -> Tuple[bool, str]:
        """
        éªŒè¯Cookieæœ‰æ•ˆæ€§
        
        Returns:
            (æ˜¯å¦æœ‰æ•ˆ, æ¶ˆæ¯)
        """
        cookie_config = self.get_config('cookie')
        if not cookie_config or not cookie_config.get('cookie'):
            return False, "Cookieæœªé…ç½®"
        
        # æ£€æŸ¥Cookieå¹´é¾„
        set_time_str = cookie_config.get('set_time')
        if set_time_str:
            try:
                set_time = datetime.fromisoformat(set_time_str)
                age = datetime.now() - set_time
                
                if age > timedelta(hours=48):
                    return False, f"Cookieå·²è¿‡æœŸ ({age.total_seconds()/3600:.1f}å°æ—¶)"
                elif age > timedelta(hours=24):
                    return True, f"Cookieå³å°†è¿‡æœŸ ({age.total_seconds()/3600:.1f}å°æ—¶)"
                else:
                    return True, f"Cookieæœ‰æ•ˆ ({age.total_seconds()/3600:.1f}å°æ—¶)"
            except:
                pass
        
        return True, "CookieçŠ¶æ€æœªçŸ¥"
    
    def sync_url_configs(self):
        """åŒæ­¥URLé…ç½®ï¼ˆç¡®ä¿download_configå’Œreal_documentsä¸€è‡´ï¼‰"""
        documents_config = self.get_config('documents')
        download_config = self.get_config('download')
        
        if documents_config and download_config:
            # ä»real_documentsåŒæ­¥åˆ°download_config
            doc_links = []
            for doc in documents_config.get('documents', []):
                doc_links.append({
                    'name': doc.get('name'),
                    'url': doc.get('url'),
                    'doc_id': doc.get('doc_id'),
                    'description': doc.get('description', '')
                })
            
            # æ›´æ–°download_config
            self.update_config('download', {
                'document_links': doc_links
            })
            
            logger.info(f"âœ… åŒæ­¥äº† {len(doc_links)} ä¸ªæ–‡æ¡£URL")
    
    def get_all_document_urls(self) -> List[Dict]:
        """è·å–æ‰€æœ‰æ–‡æ¡£URLï¼ˆåˆå¹¶æ‰€æœ‰æ¥æºï¼‰"""
        urls = []
        
        # ä»download_configè·å–
        download_config = self.get_config('download')
        if download_config:
            urls.extend(download_config.get('document_links', []))
        
        # ä»real_documentsè·å–
        documents_config = self.get_config('documents')
        if documents_config:
            for doc in documents_config.get('documents', []):
                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                if not any(u['url'] == doc['url'] for u in urls):
                    urls.append({
                        'name': doc.get('name'),
                        'url': doc.get('url'),
                        'doc_id': doc.get('doc_id'),
                        'description': doc.get('description', '')
                    })
        
        return urls
    
    def get_schedule_status(self) -> Dict[str, bool]:
        """è·å–è°ƒåº¦ä»»åŠ¡çŠ¶æ€"""
        schedule_config = self.get_config('schedule')
        status = {
            'baseline_enabled': False,
            'midweek_enabled': False,
            'weekend_enabled': False,
            'scheduler_running': False
        }
        
        if schedule_config:
            for task in schedule_config.get('preset_tasks', []):
                task_id = task.get('task_id', '')
                enabled = task.get('enabled', False)
                
                if 'baseline' in task_id:
                    status['baseline_enabled'] = enabled
                elif 'midweek' in task_id:
                    status['midweek_enabled'] = enabled
                elif 'weekend' in task_id or 'full' in task_id:
                    status['weekend_enabled'] = enabled
            
            # æ£€æŸ¥æ˜¯å¦æœ‰ä»»ä½•ä»»åŠ¡å¯ç”¨
            status['scheduler_running'] = any([
                status['baseline_enabled'],
                status['midweek_enabled'],
                status['weekend_enabled']
            ])
        
        return status
    
    def get_monitor_settings(self) -> Dict:
        """è·å–ç›‘æ§è®¾ç½®"""
        monitor_config = self.get_config('monitor')
        if monitor_config:
            return monitor_config
        return self.default_configs['monitor']
    
    def get_statistics(self) -> Dict:
        """è·å–é…ç½®ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            'total_configs': len(self.config_files),
            'loaded_configs': len(self.configs),
            'document_count': 0,
            'schedule_enabled': False,
            'cookie_status': 'unknown',
            'last_sync': None
        }
        
        # æ–‡æ¡£æ•°é‡
        urls = self.get_all_document_urls()
        stats['document_count'] = len(urls)
        
        # è°ƒåº¦çŠ¶æ€
        schedule_status = self.get_schedule_status()
        stats['schedule_enabled'] = schedule_status['scheduler_running']
        
        # CookieçŠ¶æ€
        valid, msg = self.validate_cookie()
        stats['cookie_status'] = 'valid' if valid else 'invalid'
        stats['cookie_message'] = msg
        
        return stats


# å•ä¾‹å®ä¾‹
_config_manager: Optional[ConfigManager] = None

def get_config_manager() -> ConfigManager:
    """è·å–é…ç½®ç®¡ç†å™¨å•ä¾‹"""
    global _config_manager
    if _config_manager is None:
        _config_manager = ConfigManager()
    return _config_manager


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    manager = get_config_manager()
    
    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    stats = manager.get_statistics()
    print("ğŸ“Š é…ç½®ç»Ÿè®¡:")
    for key, value in stats.items():
        print(f"  {key}: {value}")
    
    # éªŒè¯Cookie
    valid, msg = manager.validate_cookie()
    print(f"\nğŸ” CookieçŠ¶æ€: {msg}")
    
    # è·å–æ‰€æœ‰æ–‡æ¡£URL
    urls = manager.get_all_document_urls()
    print(f"\nğŸ“„ æ–‡æ¡£URL ({len(urls)}ä¸ª):")
    for url_info in urls:
        print(f"  - {url_info['name']}: {url_info['url']}")
    
    # åŒæ­¥URLé…ç½®
    manager.sync_url_configs()
    print("\nâœ… URLé…ç½®å·²åŒæ­¥")