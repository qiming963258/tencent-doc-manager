#!/usr/bin/env python3
"""
é¡µé¢ç»“æ„åˆ†æå™¨ - ä¿å­˜HTMLå¹¶åˆ†æUIå…ƒç´ 
"""

import asyncio
from pathlib import Path
from playwright.async_api import async_playwright

async def analyze_page_structure():
    cookie_string = "RK=nI/1/PDC5y; ptcz=578cdfe2a37d597cc98734b933efee8f49efc3b1724d2a91c03f3c6f8afc9d9c; low_login_enable=1; pgv_pvid=2105700616; adtag=tdocs; polish_tooltip=true; backup_cdn_domain=docs.gtimg.com; yyb_muid=3A4651960B836983251B45FD0A5C683E; dark_mode_setting=system; optimal_cdn_domain=docs2.gtimg.com; uid=144115414584628119; utype=qq; DOC_QQ_APPID=101458937; DOC_QQ_OPENID=138EA9B807B6C2DCFE96C5B1A650E932; env_id=gray-pct25; gray_user=true; DOC_SID=89f6e6a637c54775a50b132cf3958837accebd74c8d84e798619bcfb0eee9b64; SID=89f6e6a637c54775a50b132cf3958837accebd74c8d84e798619bcfb0eee9b64; traceid=25ea7d9ac7; TOK=25ea7d9ac7e65c7d; hashkey=25ea7d9a; fingerprint=cfe08ff9e3d948feb374ee3861bc221f98; language=zh-CN; uid_key=EOP1mMQHGjBEdVk5S0xhL1lLT3ZpaEVRbVE0eVElTWt2cHFzWnRiSjZVczZSSGVyZENaT2Z3PT0igQJleUpoYkdjaU9pSkJRME5CVEVjaUxDSjBlWEFpT2lKS1YxUWlmUS5leUpVYVc1NVNVUWlPaUl4TkRReE1UVTBNVFExT0RRMk1qZ3hNVGtpTENKV1pYSWlPaUl4SWl3aVJHOXRZV2x1SWpvaWMyRmhjMTkwYjJNaUxDSlNaaUk2SWtab1VsUnhkU0lzSW1WNGNDSTZNVGMxT0RJMU1EQTRPQ3dpYVdGMElqb3hOelUxTmpVNU1EZzRMQ0pwYzNNaU9pSlVaVzVqWlc1MElFUnZZM01pZlEuSzV6Rlk3X0FJYi1fUzY3ekRpdTBsNEtXV2RkMGN0SUFVZy1DR0VLZmJ6TSjQkLPGBjABOPnHsDBCIDEzOEVBOUI4MDdCNkMyRENGRTk2QzVCMUE2NTBFOTMySMHYw8QG; loginTime=1755664608597; adtag=tdocs"
    
    # è§£æCookie
    cookies = []
    for cookie_pair in cookie_string.split('; '):
        if '=' in cookie_pair:
            name, value = cookie_pair.split('=', 1)
            cookies.append({
                'name': name.strip(),
                'value': value.strip(),
                'domain': '.docs.qq.com',
                'path': '/'
            })
    
    analysis_file = Path("/root/projects/tencent-doc-manager/enterprise_download_system/page_analysis.html")
    selector_file = Path("/root/projects/tencent-doc-manager/enterprise_download_system/ui_selectors.txt")
    
    async with async_playwright() as playwright:
        print("ğŸš€ å¯åŠ¨åˆ†ææµè§ˆå™¨...")
        browser = await playwright.chromium.launch(
            headless=True,  # æœåŠ¡å™¨ç¯å¢ƒä½¿ç”¨æ— å¤´æ¨¡å¼
            args=['--no-sandbox', '--disable-setuid-sandbox', '--disable-dev-shm-usage']
        )
        context = await browser.new_context(
            user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/139.0.0.0 Safari/537.36',
            viewport={'width': 1920, 'height': 1080},
            extra_http_headers={
                'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'
            }
        )
        
        await context.add_cookies(cookies)
        page = await context.new_page()
        
        print("ğŸ“„ è®¿é—®è…¾è®¯æ–‡æ¡£æ¡Œé¢é¡µé¢...")
        await page.goto('https://docs.qq.com/desktop', wait_until='domcontentloaded', timeout=60000)
        print("â³ ç­‰å¾…é¡µé¢å®Œå…¨åŠ è½½...")
        await page.wait_for_timeout(20000)  # ç­‰å¾…20ç§’ç¡®ä¿JSå®Œå…¨æ‰§è¡Œ
        
        # ä¿å­˜é¡µé¢HTML
        html_content = await page.content()
        analysis_file.write_text(html_content, encoding='utf-8')
        print(f"âœ… é¡µé¢HTMLå·²ä¿å­˜: {analysis_file}")
        
        # åˆ†æUIå…ƒç´ 
        analysis_results = []
        analysis_results.append("=== è…¾è®¯æ–‡æ¡£æ¡Œé¢é¡µé¢UIå…ƒç´ åˆ†æ ===\\n")
        analysis_results.append(f"é¡µé¢æ ‡é¢˜: {await page.title()}")
        analysis_results.append(f"é¡µé¢URL: {page.url}")
        analysis_results.append(f"é¡µé¢HTMLé•¿åº¦: {len(html_content):,} å­—ç¬¦\\n")
        
        # åˆ†æå¯¼èˆªæŒ‰é’®
        analysis_results.append("=== å¯¼èˆªæŒ‰é’®åˆ†æ ===")
        nav_selectors = [
            'text=Cloud Docs', 'text=æ–‡æ¡£', 'text=æˆ‘çš„æ–‡æ¡£', 'text=My Documents',
            '[class*="nav"]', '[class*="menu"]', '[class*="tab"]',
            'button', 'a[href]', '[role="button"]'
        ]
        
        for selector in nav_selectors:
            try:
                elements = await page.query_selector_all(selector)
                if elements:
                    analysis_results.append(f"âœ… {selector}: {len(elements)} ä¸ªå…ƒç´ ")
                    for i, elem in enumerate(elements[:3]):
                        try:
                            text = await elem.inner_text()
                            if text.strip():
                                analysis_results.append(f"   {i+1}. {text.strip()[:50]}")
                        except:
                            pass
                else:
                    analysis_results.append(f"âŒ {selector}: 0 ä¸ªå…ƒç´ ")
            except:
                analysis_results.append(f"âš ï¸  {selector}: æŸ¥è¯¢é”™è¯¯")
        
        analysis_results.append("\\n=== æ–‡ä»¶åˆ—è¡¨å…ƒç´ åˆ†æ ===")
        file_selectors = [
            '.desktop-file-list-item', '.file-item', '.document-item',
            '.desktop-list-view-item', '[data-testid*="file"]', 
            '[class*="file"]', '[class*="doc"]', '[class*="item"]',
            '[class*="row"]', '[class*="list"]', 'tr', 'li'
        ]
        
        for selector in file_selectors:
            try:
                elements = await page.query_selector_all(selector)
                if elements:
                    analysis_results.append(f"âœ… {selector}: {len(elements)} ä¸ªå…ƒç´ ")
                    for i, elem in enumerate(elements[:3]):
                        try:
                            text = await elem.inner_text()
                            if text.strip() and len(text.strip()) > 5:
                                analysis_results.append(f"   {i+1}. {text.strip()[:80]}")
                        except:
                            pass
                else:
                    analysis_results.append(f"âŒ {selector}: 0 ä¸ªå…ƒç´ ")
            except:
                analysis_results.append(f"âš ï¸  {selector}: æŸ¥è¯¢é”™è¯¯")
        
        analysis_results.append("\\n=== ç­›é€‰æŒ‰é’®åˆ†æ ===")
        filter_selectors = [
            'text=ç­›é€‰', 'text=Filter', '[class*="filter"]', 
            '.desktop-filter-button', 'button[class*="filter"]'
        ]
        
        for selector in filter_selectors:
            try:
                elements = await page.query_selector_all(selector)
                if elements:
                    analysis_results.append(f"âœ… {selector}: {len(elements)} ä¸ªå…ƒç´ ")
                else:
                    analysis_results.append(f"âŒ {selector}: 0 ä¸ªå…ƒç´ ")
            except:
                analysis_results.append(f"âš ï¸  {selector}: æŸ¥è¯¢é”™è¯¯")
        
        # å°è¯•è·å–é¡µé¢çš„æ‰€æœ‰classåç§°
        analysis_results.append("\\n=== é¡µé¢æ‰€æœ‰CSSç±»å ===")
        try:
            all_classes = await page.evaluate("""
                () => {
                    const classes = new Set();
                    const elements = document.querySelectorAll('*');
                    for (let elem of elements) {
                        if (elem.className && typeof elem.className === 'string') {
                            elem.className.split(' ').forEach(cls => {
                                if (cls.trim()) classes.add(cls.trim());
                            });
                        }
                    }
                    return Array.from(classes).sort();
                }
            """)
            
            # è¿‡æ»¤å‡ºå¯èƒ½ä¸æ–‡æ¡£ç›¸å…³çš„ç±»å
            doc_related_classes = []
            for cls in all_classes:
                if any(keyword in cls.lower() for keyword in ['file', 'doc', 'item', 'list', 'row', 'menu', 'nav', 'filter']):
                    doc_related_classes.append(cls)
            
            analysis_results.append(f"ç›¸å…³CSSç±»å ({len(doc_related_classes)} ä¸ª):")
            for cls in doc_related_classes[:20]:  # æ˜¾ç¤ºå‰20ä¸ª
                analysis_results.append(f"  .{cls}")
            
        except Exception as e:
            analysis_results.append(f"è·å–CSSç±»åå¤±è´¥: {e}")
        
        # ä¿å­˜åˆ†æç»“æœ
        selector_file.write_text('\\n'.join(analysis_results), encoding='utf-8')
        print(f"âœ… UIåˆ†æç»“æœå·²ä¿å­˜: {selector_file}")
        
        # å°è¯•ç‚¹å‡»å¯¼èˆªçœ‹æ˜¯å¦èƒ½æ‰¾åˆ°æ–‡æ¡£
        print("\\nğŸ” å°è¯•å¯¼èˆªåˆ°æ–‡æ¡£åˆ—è¡¨...")
        try:
            # å°è¯•ç‚¹å‡»æ‰€æœ‰å¯èƒ½çš„å¯¼èˆªå…ƒç´ 
            nav_candidates = await page.query_selector_all('text=Cloud Docs')
            if nav_candidates:
                print("âœ… æ‰¾åˆ°Cloud DocsæŒ‰é’®ï¼Œå°è¯•ç‚¹å‡»...")
                await nav_candidates[0].click()
                await page.wait_for_timeout(15000)
                
                # é‡æ–°åˆ†æé¡µé¢
                new_html = await page.content()
                analysis_results.append(f"\\n=== å¯¼èˆªåé¡µé¢åˆ†æ ===")
                analysis_results.append(f"æ–°é¡µé¢URL: {page.url}")
                analysis_results.append(f"æ–°é¡µé¢æ ‡é¢˜: {await page.title()}")
                
                # é‡æ–°æŸ¥æ‰¾æ–‡ä»¶å…ƒç´ 
                for selector in file_selectors:
                    try:
                        elements = await page.query_selector_all(selector)
                        if elements:
                            analysis_results.append(f"å¯¼èˆªå {selector}: {len(elements)} ä¸ªå…ƒç´ ")
                            break
                    except:
                        continue
                
                # ä¿å­˜å¯¼èˆªåçš„é¡µé¢
                nav_file = Path("/root/projects/tencent-doc-manager/enterprise_download_system/page_after_nav.html")
                nav_file.write_text(new_html, encoding='utf-8')
                print(f"âœ… å¯¼èˆªåé¡µé¢å·²ä¿å­˜: {nav_file}")
        except Exception as e:
            analysis_results.append(f"å¯¼èˆªæµ‹è¯•å¤±è´¥: {e}")
        
        # æœ€ç»ˆä¿å­˜å®Œæ•´åˆ†æ
        selector_file.write_text('\\n'.join(analysis_results), encoding='utf-8')
        
        await browser.close()
        print("ğŸ‰ é¡µé¢åˆ†æå®Œæˆï¼")
        
        return len(analysis_results)

if __name__ == "__main__":
    try:
        result = asyncio.run(analyze_page_structure())
        print(f"\\nâœ… åˆ†æå®Œæˆï¼Œå…±ç”Ÿæˆ {result} è¡Œåˆ†æç»“æœ")
        print("ğŸ“ æŸ¥çœ‹ä»¥ä¸‹æ–‡ä»¶è·å–è¯¦ç»†ä¿¡æ¯:")
        print("   - page_analysis.html (å®Œæ•´é¡µé¢HTML)")
        print("   - ui_selectors.txt (UIå…ƒç´ åˆ†æ)")
        print("   - page_after_nav.html (å¯¼èˆªåé¡µé¢HTML)")
    except Exception as e:
        print(f"âŒ åˆ†æå¤±è´¥: {e}")