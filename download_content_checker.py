#!/usr/bin/env python3
"""
ä¸‹è½½å†…å®¹æ£€æŸ¥å™¨
ç”¨äºéªŒè¯ä¸‹è½½çš„æ–‡æ¡£æ˜¯å¦ä¸ºçœŸå®å†…å®¹ï¼Œå¹¶æä¾›è¯¦ç»†åˆ†æ
"""

import os
import json
import csv
import pandas as pd
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Tuple
import hashlib
import re
from collections import Counter

class DownloadContentChecker:
    """ä¸‹è½½å†…å®¹æ£€æŸ¥å™¨ - éªŒè¯å’Œåˆ†æä¸‹è½½çš„æ–‡æ¡£"""
    
    # æ¼”ç¤ºæ•°æ®ç‰¹å¾ï¼ˆç”¨äºè¯†åˆ«å‡æ•°æ®ï¼‰
    DEMO_INDICATORS = [
        "å¼ ä¸‰", "æå››", "ç‹äº”", "èµµå…­",  # å¸¸è§æµ‹è¯•å§“å
        "test", "demo", "example", "sample",  # æµ‹è¯•æ ‡è¯†
        "æµ‹è¯•", "ç¤ºä¾‹", "æ¼”ç¤º",  # ä¸­æ–‡æµ‹è¯•æ ‡è¯†
        "test_cookie", "demo_purposes"  # æµ‹è¯•Cookie
    ]
    
    # çœŸå®æ–‡æ¡£ç‰¹å¾
    REAL_INDICATORS = [
        "å®é™…", "çœŸå®", "æ­£å¼", "ç”Ÿäº§",
        "è®¡åˆ’", "æŠ¥è¡¨", "ç»Ÿè®¡", "åˆ†æ"
    ]
    
    def __init__(self):
        """åˆå§‹åŒ–æ£€æŸ¥å™¨"""
        self.check_results = []
        self.stats = {}
        
    def check_file(self, file_path: str) -> Dict[str, Any]:
        """
        æ£€æŸ¥å•ä¸ªæ–‡ä»¶
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            
        Returns:
            æ£€æŸ¥ç»“æœå­—å…¸
        """
        file_path = Path(file_path)
        
        if not file_path.exists():
            return {
                'success': False,
                'error': f'æ–‡ä»¶ä¸å­˜åœ¨: {file_path}',
                'file_path': str(file_path)
            }
        
        # åŸºç¡€ä¿¡æ¯
        result = {
            'file_path': str(file_path),
            'file_name': file_path.name,
            'file_size': file_path.stat().st_size,
            'file_size_readable': self._format_size(file_path.stat().st_size),
            'modified_time': datetime.fromtimestamp(file_path.stat().st_mtime).isoformat(),
            'file_hash': self._calculate_hash(file_path),
            'file_type': file_path.suffix.lower()
        }
        
        # æ ¹æ®æ–‡ä»¶ç±»å‹è¿›è¡Œå†…å®¹æ£€æŸ¥
        if file_path.suffix.lower() == '.csv':
            result.update(self._check_csv_content(file_path))
        elif file_path.suffix.lower() in ['.xlsx', '.xls']:
            result.update(self._check_excel_content(file_path))
        elif file_path.suffix.lower() == '.json':
            result.update(self._check_json_content(file_path))
        else:
            result.update(self._check_text_content(file_path))
        
        # çœŸå®æ€§è¯„åˆ†
        result['authenticity_score'] = self._calculate_authenticity_score(result)
        result['is_demo_data'] = result['authenticity_score'] < 50
        
        # ç”Ÿæˆæ€»ç»“
        result['summary'] = self._generate_summary(result)
        
        return result
    
    def _check_csv_content(self, file_path: Path) -> Dict[str, Any]:
        """æ£€æŸ¥CSVæ–‡ä»¶å†…å®¹"""
        try:
            # è¯»å–CSV
            df = pd.read_csv(file_path, encoding='utf-8-sig', nrows=1000)
            
            content_info = {
                'content_type': 'csv',
                'row_count': len(df),
                'column_count': len(df.columns),
                'columns': list(df.columns),
                'data_preview': df.head(5).to_dict('records'),
                'empty_cells': df.isna().sum().sum(),
                'duplicate_rows': df.duplicated().sum()
            }
            
            # å†…å®¹åˆ†æ
            content_text = df.to_string()
            content_info.update(self._analyze_content(content_text))
            
            # æ•°æ®è´¨é‡æ£€æŸ¥
            content_info['data_quality'] = {
                'completeness': (1 - content_info['empty_cells'] / (len(df) * len(df.columns))) * 100,
                'uniqueness': (1 - content_info['duplicate_rows'] / len(df)) * 100,
                'has_headers': self._check_has_headers(df)
            }
            
            # åˆ—ç±»å‹åˆ†æ
            column_types = {}
            for col in df.columns:
                try:
                    if pd.api.types.is_numeric_dtype(df[col]):
                        column_types[col] = 'numeric'
                    elif pd.api.types.is_datetime64_any_dtype(df[col]):
                        column_types[col] = 'datetime'
                    else:
                        column_types[col] = 'text'
                except:
                    column_types[col] = 'mixed'
            content_info['column_types'] = column_types
            
            return content_info
            
        except Exception as e:
            return {
                'content_type': 'csv',
                'error': f'CSVè¯»å–é”™è¯¯: {str(e)}'
            }
    
    def _check_excel_content(self, file_path: Path) -> Dict[str, Any]:
        """æ£€æŸ¥Excelæ–‡ä»¶å†…å®¹"""
        try:
            # è¯»å–Excel
            xl_file = pd.ExcelFile(file_path)
            
            content_info = {
                'content_type': 'excel',
                'sheet_count': len(xl_file.sheet_names),
                'sheet_names': xl_file.sheet_names,
                'sheets_info': {}
            }
            
            # åˆ†ææ¯ä¸ªå·¥ä½œè¡¨
            total_text = ""
            for sheet_name in xl_file.sheet_names[:5]:  # æœ€å¤šæ£€æŸ¥5ä¸ªå·¥ä½œè¡¨
                df = pd.read_excel(file_path, sheet_name=sheet_name, nrows=100)
                content_info['sheets_info'][sheet_name] = {
                    'row_count': len(df),
                    'column_count': len(df.columns),
                    'columns': list(df.columns)[:20]  # æœ€å¤šæ˜¾ç¤º20åˆ—
                }
                total_text += df.to_string()
            
            # å†…å®¹åˆ†æ
            content_info.update(self._analyze_content(total_text))
            
            return content_info
            
        except Exception as e:
            return {
                'content_type': 'excel',
                'error': f'Excelè¯»å–é”™è¯¯: {str(e)}'
            }
    
    def _check_json_content(self, file_path: Path) -> Dict[str, Any]:
        """æ£€æŸ¥JSONæ–‡ä»¶å†…å®¹"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            content_info = {
                'content_type': 'json',
                'structure': self._analyze_json_structure(data),
                'key_count': len(data) if isinstance(data, dict) else None,
                'item_count': len(data) if isinstance(data, list) else None
            }
            
            # å†…å®¹åˆ†æ
            content_text = json.dumps(data, ensure_ascii=False)
            content_info.update(self._analyze_content(content_text))
            
            return content_info
            
        except Exception as e:
            return {
                'content_type': 'json',
                'error': f'JSONè¯»å–é”™è¯¯: {str(e)}'
            }
    
    def _check_text_content(self, file_path: Path) -> Dict[str, Any]:
        """æ£€æŸ¥æ–‡æœ¬æ–‡ä»¶å†…å®¹"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read(10000)  # è¯»å–å‰10000å­—ç¬¦
            
            content_info = {
                'content_type': 'text',
                'character_count': len(content),
                'line_count': content.count('\n') + 1
            }
            
            # å†…å®¹åˆ†æ
            content_info.update(self._analyze_content(content))
            
            return content_info
            
        except Exception as e:
            return {
                'content_type': 'text',
                'error': f'æ–‡æœ¬è¯»å–é”™è¯¯: {str(e)}'
            }
    
    def _analyze_content(self, content: str) -> Dict[str, Any]:
        """åˆ†ææ–‡æœ¬å†…å®¹"""
        content_lower = content.lower()
        
        # æ£€æŸ¥æ¼”ç¤ºæ•°æ®æ ‡è¯†
        demo_count = sum(1 for indicator in self.DEMO_INDICATORS 
                        if indicator.lower() in content_lower)
        
        # æ£€æŸ¥çœŸå®æ•°æ®æ ‡è¯†
        real_count = sum(1 for indicator in self.REAL_INDICATORS 
                        if indicator in content)
        
        # æå–æ•°å­—
        numbers = re.findall(r'\d+\.?\d*', content)
        
        # æå–æ—¥æœŸ
        dates = re.findall(r'\d{4}[-/]\d{1,2}[-/]\d{1,2}', content)
        
        # è¯é¢‘ç»Ÿè®¡ï¼ˆä¸­æ–‡ï¼‰
        chinese_words = re.findall(r'[\u4e00-\u9fa5]+', content)
        word_freq = Counter(chinese_words).most_common(10)
        
        return {
            'demo_indicators_found': demo_count,
            'real_indicators_found': real_count,
            'contains_numbers': len(numbers) > 0,
            'number_count': len(numbers),
            'contains_dates': len(dates) > 0,
            'date_count': len(dates),
            'top_words': word_freq,
            'has_chinese': len(chinese_words) > 0,
            'chinese_ratio': len(''.join(chinese_words)) / max(len(content), 1)
        }
    
    def _analyze_json_structure(self, data: Any, max_depth: int = 3, current_depth: int = 0) -> str:
        """åˆ†æJSONç»“æ„"""
        if current_depth >= max_depth:
            return "..."
        
        if isinstance(data, dict):
            if not data:
                return "{}"
            keys = list(data.keys())[:5]  # æœ€å¤šæ˜¾ç¤º5ä¸ªé”®
            return "{" + ", ".join(f'"{k}": {self._analyze_json_structure(data[k], max_depth, current_depth+1)}' 
                                  for k in keys) + ("..." if len(data) > 5 else "") + "}"
        elif isinstance(data, list):
            if not data:
                return "[]"
            return f"[{self._analyze_json_structure(data[0], max_depth, current_depth+1)}...] (len={len(data)})"
        elif isinstance(data, str):
            return f'"{data[:20]}..."' if len(data) > 20 else f'"{data}"'
        else:
            return str(type(data).__name__)
    
    def _check_has_headers(self, df: pd.DataFrame) -> bool:
        """æ£€æŸ¥CSVæ˜¯å¦æœ‰åˆç†çš„è¡¨å¤´"""
        # æ£€æŸ¥ç¬¬ä¸€è¡Œæ˜¯å¦éƒ½æ˜¯å­—ç¬¦ä¸²
        first_row = df.iloc[0] if len(df) > 0 else []
        
        # å¦‚æœåˆ—åéƒ½æ˜¯æ•°å­—ï¼Œå¯èƒ½æ²¡æœ‰è¡¨å¤´
        if all(isinstance(col, (int, float)) for col in df.columns):
            return False
        
        # å¦‚æœåˆ—ååŒ…å«Unnamedï¼Œå¯èƒ½æ²¡æœ‰è¡¨å¤´
        if any('Unnamed' in str(col) for col in df.columns):
            return False
        
        return True
    
    def _calculate_hash(self, file_path: Path) -> str:
        """è®¡ç®—æ–‡ä»¶å“ˆå¸Œå€¼"""
        hash_md5 = hashlib.md5()
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_md5.update(chunk)
        return hash_md5.hexdigest()
    
    def _format_size(self, size: int) -> str:
        """æ ¼å¼åŒ–æ–‡ä»¶å¤§å°"""
        for unit in ['B', 'KB', 'MB', 'GB']:
            if size < 1024.0:
                return f"{size:.2f} {unit}"
            size /= 1024.0
        return f"{size:.2f} TB"
    
    def _calculate_authenticity_score(self, result: Dict[str, Any]) -> float:
        """è®¡ç®—çœŸå®æ€§è¯„åˆ†ï¼ˆ0-100ï¼‰"""
        score = 50.0  # åŸºç¡€åˆ†
        
        # æ–‡ä»¶å¤§å°è¯„åˆ†
        if result['file_size'] > 1024:  # å¤§äº1KB
            score += 10
        if result['file_size'] > 10240:  # å¤§äº10KB
            score += 10
        
        # å†…å®¹è¯„åˆ†
        if 'demo_indicators_found' in result:
            score -= result['demo_indicators_found'] * 10
        if 'real_indicators_found' in result:
            score += result['real_indicators_found'] * 5
        
        # CSV/Excelç‰¹å®šè¯„åˆ†
        if 'row_count' in result:
            if result['row_count'] > 10:
                score += 10
            if result['row_count'] > 100:
                score += 10
        
        if 'column_count' in result:
            if result['column_count'] > 3:
                score += 5
        
        # æ•°æ®è´¨é‡è¯„åˆ†
        if 'data_quality' in result:
            score += result['data_quality']['completeness'] * 0.2
            score += result['data_quality']['uniqueness'] * 0.1
        
        # ç¡®ä¿åˆ†æ•°åœ¨0-100ä¹‹é—´
        return max(0, min(100, score))
    
    def _generate_summary(self, result: Dict[str, Any]) -> str:
        """ç”Ÿæˆæ£€æŸ¥æ€»ç»“"""
        summary_parts = []
        
        # æ–‡ä»¶åŸºæœ¬ä¿¡æ¯
        summary_parts.append(f"æ–‡ä»¶: {result['file_name']} ({result['file_size_readable']})")
        
        # çœŸå®æ€§åˆ¤æ–­
        if result['is_demo_data']:
            summary_parts.append("âš ï¸ ç–‘ä¼¼æ¼”ç¤ºæ•°æ®")
        else:
            summary_parts.append("âœ… å¯èƒ½æ˜¯çœŸå®æ•°æ®")
        
        summary_parts.append(f"çœŸå®æ€§è¯„åˆ†: {result['authenticity_score']:.1f}/100")
        
        # å†…å®¹ç‰¹å¾
        if 'row_count' in result:
            summary_parts.append(f"æ•°æ®è§„æ¨¡: {result['row_count']}è¡Œ Ã— {result['column_count']}åˆ—")
        
        if 'demo_indicators_found' in result and result['demo_indicators_found'] > 0:
            summary_parts.append(f"å‘ç°{result['demo_indicators_found']}ä¸ªæ¼”ç¤ºæ•°æ®æ ‡è¯†")
        
        if 'real_indicators_found' in result and result['real_indicators_found'] > 0:
            summary_parts.append(f"å‘ç°{result['real_indicators_found']}ä¸ªçœŸå®æ•°æ®æ ‡è¯†")
        
        return " | ".join(summary_parts)
    
    def check_download_batch(self, file_paths: List[str]) -> Dict[str, Any]:
        """
        æ‰¹é‡æ£€æŸ¥ä¸‹è½½çš„æ–‡ä»¶
        
        Args:
            file_paths: æ–‡ä»¶è·¯å¾„åˆ—è¡¨
            
        Returns:
            æ‰¹é‡æ£€æŸ¥ç»“æœ
        """
        results = []
        all_demo = True
        all_real = True
        
        for file_path in file_paths:
            result = self.check_file(file_path)
            results.append(result)
            
            if not result.get('is_demo_data', True):
                all_demo = False
            else:
                all_real = False
        
        # æ±‡æ€»ç»Ÿè®¡
        total_size = sum(r.get('file_size', 0) for r in results)
        avg_score = sum(r.get('authenticity_score', 0) for r in results) / max(len(results), 1)
        
        return {
            'file_count': len(results),
            'total_size': self._format_size(total_size),
            'average_authenticity_score': avg_score,
            'all_demo_data': all_demo,
            'all_real_data': all_real,
            'mixed_data': not all_demo and not all_real,
            'individual_results': results,
            'overall_assessment': self._generate_overall_assessment(results, avg_score)
        }
    
    def _generate_overall_assessment(self, results: List[Dict], avg_score: float) -> str:
        """ç”Ÿæˆæ•´ä½“è¯„ä¼°"""
        if avg_score >= 80:
            return "âœ… é«˜å¯ä¿¡åº¦ï¼šä¸‹è½½çš„æ–‡ä»¶å¾ˆå¯èƒ½æ˜¯çœŸå®çš„è…¾è®¯æ–‡æ¡£"
        elif avg_score >= 60:
            return "âš ï¸ ä¸­ç­‰å¯ä¿¡åº¦ï¼šæ–‡ä»¶å¯èƒ½æ˜¯çœŸå®çš„ï¼Œä½†åŒ…å«ä¸€äº›å¯ç–‘ç‰¹å¾"
        elif avg_score >= 40:
            return "âš ï¸ ä½å¯ä¿¡åº¦ï¼šæ–‡ä»¶åŒ…å«è¾ƒå¤šæ¼”ç¤ºæ•°æ®ç‰¹å¾"
        else:
            return "âŒ æä½å¯ä¿¡åº¦ï¼šæ–‡ä»¶å¾ˆå¯èƒ½æ˜¯æ¼”ç¤ºæˆ–æµ‹è¯•æ•°æ®"


# ä¾¿æ·å‡½æ•°
def quick_check(file_path: str) -> None:
    """å¿«é€Ÿæ£€æŸ¥å•ä¸ªæ–‡ä»¶å¹¶æ‰“å°ç»“æœ"""
    checker = DownloadContentChecker()
    result = checker.check_file(file_path)
    
    print("=" * 60)
    print("ğŸ“Š ä¸‹è½½å†…å®¹æ£€æŸ¥æŠ¥å‘Š")
    print("=" * 60)
    print(f"æ–‡ä»¶: {result['file_name']}")
    print(f"å¤§å°: {result['file_size_readable']}")
    print(f"ç±»å‹: {result.get('content_type', 'unknown')}")
    print("-" * 60)
    
    if result['is_demo_data']:
        print("âš ï¸ æ£€æµ‹ç»“æœ: ç–‘ä¼¼æ¼”ç¤ºæ•°æ®")
    else:
        print("âœ… æ£€æµ‹ç»“æœ: å¯èƒ½æ˜¯çœŸå®æ•°æ®")
    
    print(f"çœŸå®æ€§è¯„åˆ†: {result['authenticity_score']:.1f}/100")
    
    if 'row_count' in result:
        print(f"æ•°æ®è§„æ¨¡: {result['row_count']}è¡Œ Ã— {result['column_count']}åˆ—")
    
    if 'demo_indicators_found' in result:
        print(f"æ¼”ç¤ºæ ‡è¯†: {result['demo_indicators_found']}ä¸ª")
        
    if 'real_indicators_found' in result:
        print(f"çœŸå®æ ‡è¯†: {result['real_indicators_found']}ä¸ª")
    
    print("-" * 60)
    print("æ€»ç»“:", result['summary'])
    print("=" * 60)


if __name__ == "__main__":
    # æµ‹è¯•ç¤ºä¾‹
    import sys
    
    if len(sys.argv) > 1:
        file_path = sys.argv[1]
        quick_check(file_path)
    else:
        print("ä½¿ç”¨æ–¹æ³•: python download_content_checker.py <æ–‡ä»¶è·¯å¾„>")
        print("ç¤ºä¾‹: python download_content_checker.py /root/projects/tencent-doc-manager/downloads/document.csv")