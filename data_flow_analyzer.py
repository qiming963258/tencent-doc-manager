#!/usr/bin/env python3
"""
æ•°æ®æµç¨‹åˆ†æå™¨
è¿½è¸ªCSVå¯¹æ¯” â†’ çƒ­åŠ›å›¾çŸ©é˜µçš„å®Œæ•´è½¬æ¢è¿‡ç¨‹
"""

import json
import os
from pathlib import Path
from typing import Dict, List, Any

class DataFlowAnalyzer:
    """æ•°æ®æµç¨‹åˆ†æå™¨"""
    
    def __init__(self):
        self.base_dir = "/root/projects/tencent-doc-manager"
        
    def analyze_complete_flow(self) -> Dict[str, Any]:
        """åˆ†æå®Œæ•´çš„æ•°æ®æµç¨‹"""
        
        analysis_report = {
            'timestamp': '2025-08-20T19:45:00',
            'analysis_type': 'complete_data_flow_tracking',
            'stages': {}
        }
        
        # é˜¶æ®µ1: CSVå¯¹æ¯”ç»“æœåˆ†æ
        comparison_analysis = self._analyze_csv_comparison_results()
        analysis_report['stages']['csv_comparison'] = comparison_analysis
        
        # é˜¶æ®µ2: æ˜ å°„è½¬æ¢åˆ†æ
        mapping_analysis = self._analyze_mapping_process(comparison_analysis)
        analysis_report['stages']['mapping_transformation'] = mapping_analysis
        
        # é˜¶æ®µ3: çƒ­åŠ›å›¾çŸ©é˜µåˆ†æ
        heatmap_analysis = self._analyze_heatmap_generation()
        analysis_report['stages']['heatmap_generation'] = heatmap_analysis
        
        # é˜¶æ®µ4: æ•°æ®ä¸€è‡´æ€§éªŒè¯
        consistency_check = self._verify_data_consistency(
            comparison_analysis, heatmap_analysis
        )
        analysis_report['stages']['consistency_verification'] = consistency_check
        
        # ç”Ÿæˆé—®é¢˜è¯Šæ–­
        analysis_report['diagnosis'] = self._generate_diagnosis(analysis_report)
        
        return analysis_report
    
    def _analyze_csv_comparison_results(self) -> Dict[str, Any]:
        """åˆ†æCSVå¯¹æ¯”ç»“æœ"""
        
        results_dir = os.path.join(self.base_dir, "csv_security_results")
        comparison_files = [f for f in os.listdir(results_dir) if f.endswith('_comparison.json')]
        
        analysis = {
            'total_comparison_files': len(comparison_files),
            'files_analyzed': {},
            'data_structures': {},
            'mapping_patterns': {}
        }
        
        for file_name in comparison_files:
            file_path = os.path.join(results_dir, file_name)
            
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                # åˆ†ææ•°æ®ç»“æ„
                differences = data.get('differences', [])
                file_info = data.get('file_info', {}).get('metadata', {})
                
                analysis['files_analyzed'][file_name] = {
                    'differences_count': len(differences),
                    'source_columns': file_info.get('file1_info', {}).get('columns', 0),
                    'source_rows': file_info.get('file1_info', {}).get('rows', 0),
                    'column_mapping': data.get('file_info', {}).get('metadata', {}).get('column_mapping', {})
                }
                
                # åˆ†æä½ç½®åˆ†å¸ƒ
                if differences:
                    analysis['files_analyzed'][file_name]['position_distribution'] = {
                        'row_range': [
                            min(d.get('è¡Œå·', 0) for d in differences),
                            max(d.get('è¡Œå·', 0) for d in differences)
                        ],
                        'column_range': [
                            min(d.get('åˆ—ç´¢å¼•', 0) for d in differences), 
                            max(d.get('åˆ—ç´¢å¼•', 0) for d in differences)
                        ],
                        'risk_score_range': [
                            min(d.get('risk_score', 0) for d in differences),
                            max(d.get('risk_score', 0) for d in differences)
                        ]
                    }
                    
            except Exception as e:
                analysis['files_analyzed'][file_name] = {'error': str(e)}
        
        return analysis
    
    def _analyze_mapping_process(self, csv_analysis: Dict) -> Dict[str, Any]:
        """åˆ†ææ˜ å°„è½¬æ¢è¿‡ç¨‹"""
        
        mapping_analysis = {
            'current_mapping_logic': {},
            'problems_identified': [],
            'proposed_solutions': []
        }
        
        # åˆ†æå½“å‰æ˜ å°„é€»è¾‘ï¼ˆä»ui_connectivity_manager.pyä¸­æå–ï¼‰
        mapping_analysis['current_mapping_logic'] = {
            'row_mapping': 'row = min(diff.get(\'è¡Œå·\', 1) - 1, 29)',
            'col_mapping': 'col = min(diff.get(\'åˆ—ç´¢å¼•\', 1) - 1, 18)',
            'intensity_mapping': 'heatmap_matrix[row][col] = max(current_value, risk_score)',
            'target_matrix_size': '30x19'
        }
        
        # è¯†åˆ«é—®é¢˜
        for file_name, file_data in csv_analysis['files_analyzed'].items():
            if 'error' not in file_data:
                source_cols = file_data['source_columns']
                source_rows = file_data['source_rows']
                
                if source_cols > 19:
                    mapping_analysis['problems_identified'].append(
                        f"{file_name}: æºæ•°æ®{source_cols}åˆ— > ç›®æ ‡19åˆ—ï¼Œå­˜åœ¨æ•°æ®ä¸¢å¤±é£é™©"
                    )
                
                if source_rows > 30:
                    mapping_analysis['problems_identified'].append(
                        f"{file_name}: æºæ•°æ®{source_rows}è¡Œ > ç›®æ ‡30è¡Œï¼Œå­˜åœ¨æ•°æ®æˆªæ–­"
                    )
                
                # æ£€æŸ¥åˆ—æ˜ å°„é—®é¢˜
                column_mapping = file_data.get('column_mapping', {}).get('mapping', {})
                if len(column_mapping) != source_cols:
                    mapping_analysis['problems_identified'].append(
                        f"{file_name}: åˆ—æ˜ å°„ä¸å®Œæ•´ï¼Œ{len(column_mapping)}æ˜ å°„ vs {source_cols}å®é™…åˆ—"
                    )
        
        return mapping_analysis
    
    def _analyze_heatmap_generation(self) -> Dict[str, Any]:
        """åˆ†æçƒ­åŠ›å›¾ç”Ÿæˆè¿‡ç¨‹"""
        
        heatmap_files = [
            'real_time_heatmap.json',
            'ui_params_real_test_20250819_224427.json'
        ]
        
        analysis = {
            'heatmap_files_found': {},
            'matrix_analysis': {},
            'generation_algorithms': []
        }
        
        for file_name in heatmap_files:
            file_path = os.path.join(self.base_dir, file_name)
            if not os.path.exists(file_path):
                file_path = os.path.join(self.base_dir, "production/servers", file_name)
            
            if os.path.exists(file_path):
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                    
                    heatmap_data = data.get('heatmap_data', [])
                    if heatmap_data:
                        analysis['heatmap_files_found'][file_name] = {
                            'matrix_dimensions': [len(heatmap_data), len(heatmap_data[0]) if heatmap_data else 0],
                            'value_range': [
                                min(min(row) for row in heatmap_data if row),
                                max(max(row) for row in heatmap_data if row)
                            ],
                            'algorithm': data.get('algorithm', 'unknown'),
                            'data_source': data.get('data_source', 'unknown'),
                            'generation_time': data.get('generation_time', 'unknown'),
                            'changes_applied': data.get('changes_applied', 0)
                        }
                        
                        # åˆ†æçƒ­åŠ›å›¾æ¨¡å¼
                        hot_spots = self._find_hotspots(heatmap_data)
                        analysis['heatmap_files_found'][file_name]['hotspots'] = hot_spots
                    
                except Exception as e:
                    analysis['heatmap_files_found'][file_name] = {'error': str(e)}
        
        return analysis
    
    def _find_hotspots(self, matrix: List[List[float]]) -> Dict[str, Any]:
        """æ‰¾åˆ°çƒ­åŠ›å›¾ä¸­çš„çƒ­ç‚¹"""
        
        hotspots = []
        threshold = 0.5  # çƒ­ç‚¹é˜ˆå€¼
        
        for i, row in enumerate(matrix):
            for j, value in enumerate(row):
                if value > threshold:
                    hotspots.append({
                        'position': [i, j],
                        'intensity': value
                    })
        
        return {
            'total_hotspots': len(hotspots),
            'max_intensity': max((h['intensity'] for h in hotspots), default=0),
            'hotspot_positions': hotspots[:10]  # å‰10ä¸ªçƒ­ç‚¹
        }
    
    def _verify_data_consistency(self, csv_analysis: Dict, heatmap_analysis: Dict) -> Dict[str, Any]:
        """éªŒè¯æ•°æ®ä¸€è‡´æ€§"""
        
        consistency_report = {
            'issues_found': [],
            'data_flow_integrity': 'unknown',
            'recommendations': []
        }
        
        # æ£€æŸ¥CSV differencesæ•°é‡ vs çƒ­åŠ›å›¾å˜æ›´æ•°é‡
        for file_name, file_data in csv_analysis['files_analyzed'].items():
            if 'differences_count' in file_data:
                csv_changes = file_data['differences_count']
                
                for heatmap_file, heatmap_data in heatmap_analysis['heatmap_files_found'].items():
                    if 'changes_applied' in heatmap_data:
                        heatmap_changes = heatmap_data['changes_applied']
                        
                        if csv_changes != heatmap_changes:
                            consistency_report['issues_found'].append(
                                f"å˜æ›´æ•°é‡ä¸ä¸€è‡´: {file_name}({csv_changes}) vs {heatmap_file}({heatmap_changes})"
                            )
        
        # è¯„ä¼°æ•°æ®æµå®Œæ•´æ€§
        if not consistency_report['issues_found']:
            consistency_report['data_flow_integrity'] = 'good'
        elif len(consistency_report['issues_found']) < 3:
            consistency_report['data_flow_integrity'] = 'partial'
        else:
            consistency_report['data_flow_integrity'] = 'broken'
        
        return consistency_report
    
    def _generate_diagnosis(self, analysis_report: Dict) -> Dict[str, Any]:
        """ç”Ÿæˆé—®é¢˜è¯Šæ–­"""
        
        diagnosis = {
            'critical_issues': [],
            'data_flow_status': 'unknown',
            'root_cause_analysis': [],
            'action_items': []
        }
        
        # åˆ†ææ•°æ®æµçŠ¶æ€
        consistency = analysis_report['stages']['consistency_verification']['data_flow_integrity']
        mapping_problems = len(analysis_report['stages']['mapping_transformation']['problems_identified'])
        
        if consistency == 'broken' or mapping_problems > 5:
            diagnosis['data_flow_status'] = 'critical'
            diagnosis['critical_issues'].append('æ•°æ®æµç¨‹å­˜åœ¨ä¸¥é‡æ–­è£‚')
        elif consistency == 'partial' or mapping_problems > 2:
            diagnosis['data_flow_status'] = 'needs_attention'
            diagnosis['critical_issues'].append('æ•°æ®æµç¨‹å­˜åœ¨éƒ¨åˆ†é—®é¢˜')
        else:
            diagnosis['data_flow_status'] = 'stable'
        
        # æ ¹å› åˆ†æ
        diagnosis['root_cause_analysis'] = [
            'å®é™…CSVåˆ—æ•°ä¸æ ‡å‡†19åˆ—ä¸åŒ¹é…',
            'è¡Œæ•°åŠ¨æ€å˜åŒ–ä½†çŸ©é˜µå›ºå®šä¸º30è¡Œ',
            'æ˜ å°„ç®—æ³•è¿‡äºç®€åŒ–ï¼Œæœªè€ƒè™‘æ•°æ®è¯­ä¹‰',
            'ç¼ºå°‘ä¸­é—´è½¬æ¢å±‚å¤„ç†æ•°æ®å·®å¼‚'
        ]
        
        # è¡ŒåŠ¨é¡¹
        diagnosis['action_items'] = [
            'é‡æ–°è®¾è®¡è‡ªé€‚åº”æ˜ å°„ç®—æ³•',
            'å»ºç«‹è¯­ä¹‰åˆ—æ˜ å°„ç³»ç»Ÿ',
            'å®ç°åŠ¨æ€çŸ©é˜µç»´åº¦è°ƒæ•´',
            'æ·»åŠ æ•°æ®ä¸€è‡´æ€§éªŒè¯æœºåˆ¶'
        ]
        
        return diagnosis

def main():
    """ä¸»å‡½æ•°"""
    analyzer = DataFlowAnalyzer()
    
    print("ğŸ” å¼€å§‹åˆ†ææ•°æ®æµç¨‹...")
    analysis_report = analyzer.analyze_complete_flow()
    
    # ä¿å­˜åˆ†ææŠ¥å‘Š
    report_file = "/root/projects/tencent-doc-manager/data_flow_analysis_report.json"
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(analysis_report, f, indent=2, ensure_ascii=False)
    
    print(f"âœ… æ•°æ®æµç¨‹åˆ†æå®Œæˆï¼ŒæŠ¥å‘Šå·²ä¿å­˜è‡³: {report_file}")
    
    # æ˜¾ç¤ºå…³é”®å‘ç°
    print("\nğŸ¯ å…³é”®å‘ç°:")
    diagnosis = analysis_report['diagnosis']
    print(f"   æ•°æ®æµçŠ¶æ€: {diagnosis['data_flow_status']}")
    print(f"   å…³é”®é—®é¢˜: {len(diagnosis['critical_issues'])}ä¸ª")
    
    for issue in diagnosis['critical_issues']:
        print(f"   - {issue}")
    
    print("\nğŸ”§ å»ºè®®è¡ŒåŠ¨:")
    for action in diagnosis['action_items']:
        print(f"   - {action}")

if __name__ == "__main__":
    main()