#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è…¾è®¯æ–‡æ¡£æ™ºèƒ½å¤„ç†ç³»ç»Ÿ - å®Œæ•´é›†æˆæµ‹è¯•ï¼ˆç¬¦åˆè§„èŒƒç‰ˆï¼‰
ä¸¥æ ¼éµå¾ª /docs/specifications ä¸­çš„æ‰€æœ‰è§„èŒƒè¦æ±‚

ä¸»è¦æ”¹è¿›ï¼š
1. ä½¿ç”¨WeekTimeManagerè¿›è¡Œæœ¬åœ°æ–‡ä»¶æŸ¥æ‰¾
2. åˆ†ç¦»ä¸‹è½½å’Œå¯¹æ¯”æµç¨‹
3. ä½¿ç”¨ColumnStandardizationProcessorV3
4. é›†æˆDeepSeek AIå®¢æˆ·ç«¯
"""

import os
import sys
import json
import time
import logging
import traceback
from pathlib import Path
from datetime import datetime
from threading import Thread, Lock
from flask import Flask, request, jsonify, send_file, render_template_string
from flask_cors import CORS

# è®¾ç½®é¡¹ç›®æ ¹ç›®å½•
PROJECT_ROOT = Path(__file__).parent
sys.path.append(str(PROJECT_ROOT))

# ==================== æ—¥å¿—é…ç½® ====================
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - [%(levelname)s] - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

# ==================== ç›®å½•é…ç½® ====================
BASE_DIR = Path("/root/projects/tencent-doc-manager")
CSV_VERSIONS_DIR = BASE_DIR / "csv_versions"
DOWNLOAD_DIR = BASE_DIR / "downloads"
COMPARISON_RESULTS_DIR = BASE_DIR / "comparison_results"
SCORING_RESULTS_DIR = BASE_DIR / "scoring_results"
EXCEL_UPLOADS_DIR = BASE_DIR / "excel_uploads"

# ç¡®ä¿ç›®å½•å­˜åœ¨
for dir_path in [CSV_VERSIONS_DIR, DOWNLOAD_DIR, COMPARISON_RESULTS_DIR, 
                  SCORING_RESULTS_DIR, EXCEL_UPLOADS_DIR]:
    dir_path.mkdir(parents=True, exist_ok=True)

# ==================== å·¥ä½œæµçŠ¶æ€ç®¡ç† ====================
class WorkflowState:
    def __init__(self):
        self.reset()
        self.lock = Lock()
    
    def reset(self):
        self.execution_id = None
        self.start_time = None
        self.progress = 0
        self.current_step = ""
        self.logs = []
        self.status = "idle"  # idle, running, completed, error
        self.results = {}
        self.baseline_file = None
        self.target_file = None
        self.score_file = None
        self.marked_file = None
        self.upload_url = None
        self.advanced_settings = {}
    
    def update_progress(self, step: str, progress: int):
        with self.lock:
            self.current_step = step
            self.progress = progress
            logger.info(f"[{progress}%] {step}")
    
    def add_log(self, message: str, level: str = "INFO"):
        with self.lock:
            timestamp = datetime.now().strftime("%H:%M:%S")
            self.logs.append({
                "timestamp": timestamp,
                "level": level,
                "message": message
            })
            logger.log(getattr(logging, level), message)
    
    def get_state(self):
        with self.lock:
            return {
                "execution_id": self.execution_id,
                "progress": self.progress,
                "current_step": self.current_step,
                "logs": self.logs[-50:],  # åªè¿”å›æœ€è¿‘50æ¡æ—¥å¿—
                "status": self.status,
                "results": self.results
            }

workflow_state = WorkflowState()

# ==================== æ¨¡å—å¯¼å…¥å’ŒçŠ¶æ€è¿½è¸ª ====================
MODULES_STATUS = {}

# 1. æ—¶é—´ç®¡ç†å™¨ï¼ˆæ ¸å¿ƒæ¨¡å—ï¼Œç¬¦åˆè§„èŒƒï¼‰
try:
    from production.core_modules.week_time_manager import WeekTimeManager
    week_manager = WeekTimeManager()
    MODULES_STATUS['week_manager'] = True
    logger.info("âœ… æˆåŠŸå¯¼å…¥å‘¨æ—¶é—´ç®¡ç†å™¨")
except ImportError as e:
    MODULES_STATUS['week_manager'] = False
    logger.error(f"âŒ æ— æ³•å¯¼å…¥å‘¨æ—¶é—´ç®¡ç†å™¨: {e}")
    raise  # å¿…é¡»æœ‰æ—¶é—´ç®¡ç†å™¨

# 2. ä¸‹è½½æ¨¡å—
try:
    from production.core_modules.tencent_export_automation import TencentDocAutoExporter
    MODULES_STATUS['downloader'] = True
    logger.info("âœ… æˆåŠŸå¯¼å…¥ä¸‹è½½æ¨¡å—")
except ImportError as e:
    MODULES_STATUS['downloader'] = False
    logger.error(f"âŒ æ— æ³•å¯¼å…¥ä¸‹è½½æ¨¡å—: {e}")

# 3. ç»Ÿä¸€CSVå¯¹æ¯”å™¨ï¼ˆç¬¦åˆè§„èŒƒï¼‰
try:
    from unified_csv_comparator import UnifiedCSVComparator
    MODULES_STATUS['comparator'] = True
    logger.info("âœ… æˆåŠŸå¯¼å…¥ç»Ÿä¸€CSVå¯¹æ¯”å™¨")
except ImportError as e:
    MODULES_STATUS['comparator'] = False
    logger.error(f"âŒ æ— æ³•å¯¼å…¥å¯¹æ¯”æ¨¡å—: {e}")

# 4. åˆ—æ ‡å‡†åŒ–V3ç‰ˆæœ¬ï¼ˆç¬¦åˆè§„èŒƒï¼‰
try:
    from column_standardization_processor_v3 import ColumnStandardizationProcessorV3
    MODULES_STATUS['standardizer'] = True
    MODULES_STATUS['standardizer_v3'] = True
    logger.info("âœ… æˆåŠŸå¯¼å…¥åˆ—æ ‡å‡†åŒ–V3æ¨¡å—")
except ImportError as e:
    MODULES_STATUS['standardizer_v3'] = False
    # å¦‚æœV3ä¸å­˜åœ¨ï¼Œå°è¯•æ—§ç‰ˆæœ¬
    try:
        from production.core_modules.column_standardization_prompt import ColumnStandardizationPrompt
        MODULES_STATUS['standardizer'] = True
        logger.warning("âš ï¸ ä½¿ç”¨æ—§ç‰ˆåˆ—æ ‡å‡†åŒ–æ¨¡å—")
    except ImportError:
        MODULES_STATUS['standardizer'] = False
        logger.error(f"âŒ æ— æ³•å¯¼å…¥åˆ—æ ‡å‡†åŒ–æ¨¡å—: {e}")

# 5. DeepSeekå®¢æˆ·ç«¯ï¼ˆç¬¦åˆè§„èŒƒï¼‰
try:
    from production.core_modules.deepseek_client import DeepSeekClient
    MODULES_STATUS['deepseek'] = True
    logger.info("âœ… æˆåŠŸå¯¼å…¥DeepSeekå®¢æˆ·ç«¯")
except ImportError as e:
    MODULES_STATUS['deepseek'] = False
    logger.warning(f"âš ï¸ DeepSeekå®¢æˆ·ç«¯æœªåŠ è½½: {e}")

# 6. L2è¯­ä¹‰åˆ†æ
try:
    from production.core_modules.l2_semantic_analysis_two_layer import L2SemanticAnalyzer
    MODULES_STATUS['l2_analyzer'] = True
    logger.info("âœ… æˆåŠŸå¯¼å…¥L2è¯­ä¹‰åˆ†ææ¨¡å—")
except ImportError as e:
    MODULES_STATUS['l2_analyzer'] = False
    logger.error(f"âŒ æ— æ³•å¯¼å…¥L2è¯­ä¹‰åˆ†ææ¨¡å—: {e}")

# 7. æ™ºèƒ½æ ‡è®°æ¨¡å—
try:
    from intelligent_excel_marker import DetailedScoreGenerator
    MODULES_STATUS['marker'] = True
    logger.info("âœ… æˆåŠŸå¯¼å…¥æ™ºèƒ½æ ‡è®°æ¨¡å—")
except ImportError as e:
    MODULES_STATUS['marker'] = False
    logger.error(f"âŒ æ— æ³•å¯¼å…¥æ™ºèƒ½æ ‡è®°æ¨¡å—: {e}")

# 8. Excelä¿®å¤æ¨¡å—
try:
    from fix_tencent_excel import fix_tencent_excel
    MODULES_STATUS['excel_fixer'] = True
    logger.info("âœ… æˆåŠŸå¯¼å…¥Excelä¿®å¤æ¨¡å—")
except ImportError as e:
    MODULES_STATUS['excel_fixer'] = False
    logger.error(f"âŒ æ— æ³•å¯¼å…¥Excelä¿®å¤æ¨¡å—: {e}")

# 9. ä¸Šä¼ æ¨¡å—
try:
    from tencent_doc_uploader_ultimate import TencentDocUploaderUltimate
    MODULES_STATUS['uploader'] = True
    logger.info("âœ… æˆåŠŸå¯¼å…¥ä¸Šä¼ æ¨¡å—(ç»ˆæç‰ˆ)")
except ImportError as e:
    MODULES_STATUS['uploader'] = False
    logger.error(f"âŒ æ— æ³•å¯¼å…¥ä¸Šä¼ æ¨¡å—: {e}")

# ==================== æ ¸å¿ƒå·¥ä½œæµå‡½æ•°ï¼ˆç¬¦åˆè§„èŒƒï¼‰ ====================
def find_or_download_files(baseline_url: str, target_url: str, cookie: str):
    """
    ç¬¦åˆè§„èŒƒçš„æ–‡ä»¶è·å–é€»è¾‘
    ä¼˜å…ˆä½¿ç”¨æœ¬åœ°æ–‡ä»¶ï¼Œå¿…è¦æ—¶æ‰ä¸‹è½½
    """
    # 1. å…ˆå°è¯•æŸ¥æ‰¾æœ¬åœ°æ–‡ä»¶
    try:
        baseline_files, baseline_desc = week_manager.find_baseline_files()
        workflow_state.add_log(f"æ‰¾åˆ°æœ¬åœ°åŸºçº¿æ–‡ä»¶: {baseline_desc}")
        
        if baseline_files:
            # ä½¿ç”¨æœ€æ–°çš„åŸºçº¿æ–‡ä»¶
            workflow_state.baseline_file = baseline_files[0]
            workflow_state.add_log(f"âœ… ä½¿ç”¨æœ¬åœ°åŸºçº¿æ–‡ä»¶: {os.path.basename(baseline_files[0])}")
        else:
            # æœ¬åœ°æ²¡æœ‰åŸºçº¿æ–‡ä»¶ï¼Œéœ€è¦ä¸‹è½½
            workflow_state.add_log("æœ¬åœ°æ— åŸºçº¿æ–‡ä»¶ï¼Œå¼€å§‹ä¸‹è½½...")
            if MODULES_STATUS.get('downloader'):
                exporter = TencentDocAutoExporter()
                result = exporter.export_document(baseline_url, cookies=cookie, format='csv')
                if result and result.get('success'):
                    workflow_state.baseline_file = result.get('file_path')
                    workflow_state.add_log(f"âœ… åŸºçº¿æ–‡æ¡£ä¸‹è½½æˆåŠŸ")
                else:
                    raise Exception("åŸºçº¿æ–‡æ¡£ä¸‹è½½å¤±è´¥")
            else:
                raise Exception("ä¸‹è½½æ¨¡å—æœªåŠ è½½ï¼Œæ— æ³•ä¸‹è½½åŸºçº¿æ–‡æ¡£")
    except Exception as e:
        workflow_state.add_log(f"âŒ è·å–åŸºçº¿æ–‡ä»¶å¤±è´¥: {str(e)}", "ERROR")
        raise
    
    # 2. æŸ¥æ‰¾æˆ–ä¸‹è½½ç›®æ ‡æ–‡ä»¶
    try:
        target_files = week_manager.find_target_files()
        
        if target_files:
            # ä½¿ç”¨æœ€æ–°çš„ç›®æ ‡æ–‡ä»¶
            workflow_state.target_file = target_files[0]
            workflow_state.add_log(f"âœ… ä½¿ç”¨æœ¬åœ°ç›®æ ‡æ–‡ä»¶: {os.path.basename(target_files[0])}")
        else:
            # æœ¬åœ°æ²¡æœ‰ç›®æ ‡æ–‡ä»¶ï¼Œéœ€è¦ä¸‹è½½
            workflow_state.add_log("æœ¬åœ°æ— ç›®æ ‡æ–‡ä»¶ï¼Œå¼€å§‹ä¸‹è½½...")
            if MODULES_STATUS.get('downloader'):
                exporter = TencentDocAutoExporter()
                result = exporter.export_document(target_url, cookies=cookie, format='csv')
                if result and result.get('success'):
                    workflow_state.target_file = result.get('file_path')
                    workflow_state.add_log(f"âœ… ç›®æ ‡æ–‡æ¡£ä¸‹è½½æˆåŠŸ")
                else:
                    raise Exception("ç›®æ ‡æ–‡æ¡£ä¸‹è½½å¤±è´¥")
            else:
                raise Exception("ä¸‹è½½æ¨¡å—æœªåŠ è½½ï¼Œæ— æ³•ä¸‹è½½ç›®æ ‡æ–‡æ¡£")
    except Exception as e:
        workflow_state.add_log(f"âŒ è·å–ç›®æ ‡æ–‡ä»¶å¤±è´¥: {str(e)}", "ERROR")
        raise

def run_compliant_workflow(baseline_url: str, target_url: str, cookie: str, advanced_settings: dict = None):
    """
    æ‰§è¡Œç¬¦åˆè§„èŒƒçš„å®Œæ•´å·¥ä½œæµç¨‹
    """
    try:
        workflow_state.reset()
        workflow_state.status = "running"
        workflow_state.start_time = datetime.now()
        workflow_state.execution_id = datetime.now().strftime("%Y%m%d_%H%M%S")
        workflow_state.advanced_settings = advanced_settings or {}
        
        # ========== æ­¥éª¤1: æ–‡ä»¶è·å–ï¼ˆç¬¦åˆè§„èŒƒï¼‰ ==========
        workflow_state.update_progress("è·å–æ–‡ä»¶", 10)
        workflow_state.add_log("å¼€å§‹è·å–åŸºçº¿å’Œç›®æ ‡æ–‡ä»¶...")
        
        find_or_download_files(baseline_url, target_url, cookie)
        
        # ========== æ­¥éª¤2: CSVå¯¹æ¯”åˆ†æï¼ˆä½¿ç”¨UnifiedCSVComparatorï¼‰ ==========
        workflow_state.update_progress("æ‰§è¡ŒCSVå¯¹æ¯”åˆ†æ", 30)
        workflow_state.add_log("å¼€å§‹å¯¹æ¯”åˆ†æ...")
        
        comparison_result = None
        if MODULES_STATUS.get('comparator'):
            unified_comparator = UnifiedCSVComparator()
            comparison_result = unified_comparator.compare(
                workflow_state.baseline_file,
                workflow_state.target_file
            )
            
            # ä¿å­˜å¯¹æ¯”ç»“æœ
            comparison_file = COMPARISON_RESULTS_DIR / f"comparison_{workflow_state.execution_id}.json"
            with open(comparison_file, 'w', encoding='utf-8') as f:
                json.dump(comparison_result, f, ensure_ascii=False, indent=2)
            
            num_changes = comparison_result.get('statistics', {}).get('total_modifications', 0)
            workflow_state.add_log(f"âœ… å¯¹æ¯”åˆ†æå®Œæˆï¼Œå‘ç° {num_changes} å¤„å˜æ›´")
        else:
            workflow_state.add_log("âš ï¸ æ¯”è¾ƒæ¨¡å—æœªåŠ è½½ï¼Œè·³è¿‡", "WARNING")
        
        # ========== æ­¥éª¤3: åˆ—æ ‡å‡†åŒ–ï¼ˆä½¿ç”¨V3ç‰ˆæœ¬ï¼‰ ==========
        workflow_state.update_progress("åˆ—æ ‡å‡†åŒ–å¤„ç†", 40)
        workflow_state.add_log("å¼€å§‹åˆ—æ ‡å‡†åŒ–...")
        
        standardized_result = None
        if MODULES_STATUS.get('standardizer') and comparison_result:
            try:
                # ä¼˜å…ˆä½¿ç”¨V3ç‰ˆæœ¬
                if MODULES_STATUS.get('standardizer_v3'):
                    # è·å–DeepSeek APIå¯†é’¥
                    api_key = os.getenv('DEEPSEEK_API_KEY')
                    if not api_key:
                        workflow_state.add_log("âš ï¸ DeepSeek APIå¯†é’¥æœªé…ç½®ï¼Œä½¿ç”¨ç®€åŒ–æ ‡å‡†åŒ–", "WARNING")
                        standardized_result = comparison_result  # ç›´æ¥ä½¿ç”¨åŸå§‹ç»“æœ
                    else:
                        processor = ColumnStandardizationProcessorV3(api_key)
                        # æå–ä¿®æ”¹åˆ—å¹¶è¿›è¡Œæ ‡å‡†åŒ–
                        if 'modified_columns' in comparison_result:
                            import asyncio
                            column_mapping = comparison_result.get('modified_columns', {})
                            # å¼‚æ­¥è°ƒç”¨æ ‡å‡†åŒ–
                            loop = asyncio.new_event_loop()
                            asyncio.set_event_loop(loop)
                            standardized_mapping = loop.run_until_complete(
                                processor.standardize_column_names(column_mapping)
                            )
                            loop.close()
                            
                            # åº”ç”¨æ ‡å‡†åŒ–ç»“æœ
                            standardized_result = comparison_result.copy()
                            standardized_result['standardized_columns'] = standardized_mapping
                            workflow_state.add_log(f"âœ… åˆ—æ ‡å‡†åŒ–V3å®Œæˆï¼Œæ ‡å‡†åŒ–äº† {len(standardized_mapping)} ä¸ªåˆ—")
                        else:
                            standardized_result = comparison_result
                            workflow_state.add_log("âš ï¸ æ— ä¿®æ”¹åˆ—éœ€è¦æ ‡å‡†åŒ–", "WARNING")
                else:
                    # ä½¿ç”¨æ—§ç‰ˆæœ¬
                    standardizer = ColumnStandardizationPrompt()
                    # ç®€å•çš„è§„åˆ™æ˜ å°„ï¼ˆå…¼å®¹æ—§ç‰ˆï¼‰
                    standardized_result = comparison_result  # ä¿æŒåŸå§‹ç»“æœ
                    workflow_state.add_log(f"âš ï¸ ä½¿ç”¨æ—§ç‰ˆåˆ—æ ‡å‡†åŒ–", "WARNING")
            except Exception as e:
                workflow_state.add_log(f"âš ï¸ åˆ—æ ‡å‡†åŒ–å‡ºé”™: {str(e)}", "WARNING")
        
        # ========== æ­¥éª¤4: L2è¯­ä¹‰åˆ†æ ==========
        workflow_state.update_progress("è¯­ä¹‰åˆ†æå’Œæ‰“åˆ†", 50)
        workflow_state.add_log("å¼€å§‹L2è¯­ä¹‰åˆ†æ...")
        
        if MODULES_STATUS.get('l2_analyzer') and comparison_result:
            try:
                analyzer = L2SemanticAnalyzer()
                modifications = []
                
                # å¤„ç†UnifiedCSVComparatorçš„è¾“å‡ºæ ¼å¼
                if comparison_result and 'modifications' in comparison_result:
                    for change in comparison_result['modifications']:
                        cell = change.get('cell', 'A1')
                        row_num = int(''.join(filter(str.isdigit, cell))) if any(c.isdigit() for c in cell) else 0
                        
                        modifications.append({
                            'column_name': cell[0] if cell else '',
                            'old_value': change.get('old', ''),
                            'new_value': change.get('new', ''),
                            'row': row_num,
                            'cell': cell
                        })
                
                semantic_scores = analyzer.analyze_modifications(modifications)
                workflow_state.add_log(f"âœ… è¯­ä¹‰åˆ†æå®Œæˆï¼Œåˆ†æäº† {len(modifications)} å¤„å˜æ›´")
            except Exception as e:
                workflow_state.add_log(f"âŒ è¯­ä¹‰åˆ†æå¤±è´¥: {str(e)}", "ERROR")
                raise
        
        # ========== æ­¥éª¤5: ç”Ÿæˆè¯¦ç»†æ‰“åˆ† ==========
        workflow_state.update_progress("ç”Ÿæˆè¯¦ç»†æ‰“åˆ†", 60)
        workflow_state.add_log("ç”Ÿæˆè¯¦ç»†æ‰“åˆ†JSON...")
        
        if MODULES_STATUS.get('marker') and workflow_state.baseline_file and workflow_state.target_file:
            try:
                generator = DetailedScoreGenerator()
                score_file_path = generator.generate_score_json(
                    baseline_file=workflow_state.baseline_file,
                    target_file=workflow_state.target_file,
                    output_dir=str(SCORING_RESULTS_DIR)
                )
                workflow_state.score_file = score_file_path
                workflow_state.add_log(f"âœ… è¯¦ç»†æ‰“åˆ†ç”Ÿæˆå®Œæˆ: {os.path.basename(score_file_path)}")
            except Exception as e:
                workflow_state.add_log(f"âŒ æ‰“åˆ†ç”Ÿæˆå¤±è´¥: {str(e)}", "ERROR")
                raise
        
        # ========== æ­¥éª¤6: Excelå¤„ç†å’Œä¸Šä¼ ï¼ˆå¦‚æœéœ€è¦ï¼‰ ==========
        if advanced_settings and advanced_settings.get('enable_excel_marking'):
            workflow_state.update_progress("Excelå¤„ç†", 70)
            workflow_state.add_log("å¼€å§‹Excelå¤„ç†...")
            
            # ä¸‹è½½Excelæ ¼å¼
            if MODULES_STATUS.get('downloader'):
                exporter_excel = TencentDocAutoExporter()
                excel_result = exporter_excel.export_document(target_url, cookies=cookie, format='xlsx')
                if excel_result and excel_result.get('success'):
                    excel_file = excel_result.get('file_path')
                    workflow_state.add_log(f"âœ… Excelæ–‡æ¡£ä¸‹è½½æˆåŠŸ")
                    
                    # ä¿®å¤Excelæ ¼å¼
                    if MODULES_STATUS.get('excel_fixer'):
                        fixed_file = fix_tencent_excel(excel_file)
                        workflow_state.add_log(f"âœ… Excelæ ¼å¼ä¿®å¤å®Œæˆ")
                        
                        # åº”ç”¨æ¶‚è‰²æ ‡è®°
                        if MODULES_STATUS.get('marker') and workflow_state.score_file:
                            from intelligent_excel_marker import apply_striped_pattern_to_excel
                            marked_file = apply_striped_pattern_to_excel(fixed_file, workflow_state.score_file)
                            workflow_state.marked_file = marked_file
                            workflow_state.add_log(f"âœ… æ¶‚è‰²æ ‡è®°å®Œæˆ")
                            
                            # ä¸Šä¼ åˆ°è…¾è®¯æ–‡æ¡£
                            if advanced_settings.get('enable_upload') and MODULES_STATUS.get('uploader'):
                                workflow_state.update_progress("ä¸Šä¼ æ–‡æ¡£", 90)
                                uploader = TencentDocUploaderUltimate()
                                upload_result = uploader.upload_document(
                                    file_path=marked_file,
                                    cookie=cookie,
                                    target_url=advanced_settings.get('upload_target_url')
                                )
                                if upload_result.get('success'):
                                    workflow_state.upload_url = upload_result.get('url')
                                    workflow_state.add_log(f"âœ… æ–‡æ¡£ä¸Šä¼ æˆåŠŸ: {workflow_state.upload_url}")
        
        # ========== å®Œæˆ ==========
        workflow_state.update_progress("å®Œæˆ", 100)
        workflow_state.status = "completed"
        workflow_state.add_log("ğŸ‰ æ‰€æœ‰æ­¥éª¤æ‰§è¡Œå®Œæˆ!")
        
        # ä¿å­˜ç»“æœ
        workflow_state.results = {
            "baseline_file": workflow_state.baseline_file,
            "target_file": workflow_state.target_file,
            "score_file": workflow_state.score_file,
            "marked_file": workflow_state.marked_file,
            "upload_url": workflow_state.upload_url,
            "comparison_result": comparison_result,
            "execution_time": (datetime.now() - workflow_state.start_time).total_seconds()
        }
        
        return workflow_state.results
        
    except Exception as e:
        workflow_state.status = "error"
        workflow_state.add_log(f"âŒ å·¥ä½œæµæ‰§è¡Œå¤±è´¥: {str(e)}", "ERROR")
        logger.error(f"å·¥ä½œæµæ‰§è¡Œå¤±è´¥: {str(e)}\n{traceback.format_exc()}")
        raise

# ==================== Flaskåº”ç”¨ ====================
app = Flask(__name__)
CORS(app)

@app.route('/')
def index():
    """ä¸»é¡µ"""
    return render_template_string('''
<!DOCTYPE html>
<html>
<head>
    <title>è…¾è®¯æ–‡æ¡£æ™ºèƒ½å¤„ç†ç³»ç»Ÿï¼ˆç¬¦åˆè§„èŒƒç‰ˆï¼‰</title>
    <meta charset="utf-8">
    <style>
        body { font-family: Arial, sans-serif; margin: 20px; background: #f5f5f5; }
        .container { max-width: 1200px; margin: 0 auto; background: white; padding: 30px; border-radius: 10px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); }
        h1 { color: #2c3e50; border-bottom: 3px solid #3498db; padding-bottom: 10px; }
        .status-badge { 
            display: inline-block; 
            padding: 5px 10px; 
            border-radius: 5px; 
            font-size: 12px; 
            font-weight: bold;
            background: #27ae60;
            color: white;
        }
        .feature-list { 
            display: grid; 
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr)); 
            gap: 15px; 
            margin: 20px 0; 
        }
        .feature-item { 
            padding: 15px; 
            background: #ecf0f1; 
            border-radius: 5px; 
            border-left: 4px solid #3498db; 
        }
        .api-endpoint { 
            background: #2c3e50; 
            color: white; 
            padding: 10px; 
            border-radius: 5px; 
            margin: 10px 0; 
            font-family: monospace; 
        }
        .compliance-notice {
            background: #e8f5e9;
            border-left: 4px solid #4caf50;
            padding: 15px;
            margin: 20px 0;
            border-radius: 5px;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>ğŸš€ è…¾è®¯æ–‡æ¡£æ™ºèƒ½å¤„ç†ç³»ç»Ÿ <span class="status-badge">ç¬¦åˆè§„èŒƒç‰ˆ v2.0</span></h1>
        
        <div class="compliance-notice">
            <strong>âœ… è§„èŒƒåˆè§„æ€§</strong><br>
            æœ¬ç³»ç»Ÿä¸¥æ ¼éµå¾ª /docs/specifications ä¸­çš„æ‰€æœ‰è§„èŒƒè¦æ±‚ï¼š
            <ul>
                <li>ä½¿ç”¨WeekTimeManagerè¿›è¡Œæœ¬åœ°æ–‡ä»¶æŸ¥æ‰¾</li>
                <li>åˆ†ç¦»ä¸‹è½½å’Œå¯¹æ¯”æµç¨‹</li>
                <li>ä½¿ç”¨UnifiedCSVComparatorè¿›è¡Œå¯¹æ¯”</li>
                <li>é›†æˆColumnStandardizationProcessorV3</li>
                <li>æ”¯æŒDeepSeek AIå¢å¼º</li>
            </ul>
        </div>
        
        <h2>æ ¸å¿ƒåŠŸèƒ½</h2>
        <div class="feature-list">
            <div class="feature-item">ğŸ“Š æ™ºèƒ½CSVå¯¹æ¯”åˆ†æ</div>
            <div class="feature-item">ğŸ” æœ¬åœ°æ–‡ä»¶ä¼˜å…ˆæŸ¥æ‰¾</div>
            <div class="feature-item">ğŸ“… æ—¶é—´å‘¨æœŸç®¡ç†</div>
            <div class="feature-item">ğŸ¤– AIè¯­ä¹‰åˆ†æ</div>
            <div class="feature-item">ğŸ¨ Excelæ™ºèƒ½æ¶‚è‰²</div>
            <div class="feature-item">â˜ï¸ è‡ªåŠ¨ä¸Šä¼ è…¾è®¯æ–‡æ¡£</div>
        </div>
        
        <h2>APIç«¯ç‚¹</h2>
        <div class="api-endpoint">POST /api/workflow - æ‰§è¡Œå®Œæ•´å·¥ä½œæµ</div>
        <div class="api-endpoint">GET /api/status - è·å–æ‰§è¡ŒçŠ¶æ€</div>
        <div class="api-endpoint">GET /api/modules - æŸ¥çœ‹æ¨¡å—çŠ¶æ€</div>
        
        <p style="text-align: center; color: #7f8c8d; margin-top: 30px;">
            ç³»ç»Ÿè¿è¡Œåœ¨ http://localhost:8093
        </p>
    </div>
</body>
</html>
    ''')

@app.route('/api/workflow', methods=['POST'])
def execute_workflow():
    """æ‰§è¡Œå·¥ä½œæµAPI"""
    try:
        data = request.json
        baseline_url = data.get('baseline_url')
        target_url = data.get('target_url')
        cookie = data.get('cookie')
        advanced_settings = data.get('advanced_settings', {})
        
        if not all([baseline_url, target_url, cookie]):
            return jsonify({"error": "ç¼ºå°‘å¿…è¦å‚æ•°"}), 400
        
        # åœ¨åå°çº¿ç¨‹æ‰§è¡Œå·¥ä½œæµ
        thread = Thread(
            target=run_compliant_workflow,
            args=(baseline_url, target_url, cookie, advanced_settings)
        )
        thread.start()
        
        return jsonify({
            "success": True,
            "execution_id": workflow_state.execution_id,
            "message": "å·¥ä½œæµå·²å¯åŠ¨"
        })
        
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route('/api/status', methods=['GET'])
def get_status():
    """è·å–å·¥ä½œæµçŠ¶æ€"""
    return jsonify(workflow_state.get_state())

@app.route('/api/modules', methods=['GET'])
def get_modules():
    """è·å–æ¨¡å—çŠ¶æ€"""
    return jsonify(MODULES_STATUS)

# ==================== ä¸»ç¨‹åºå…¥å£ ====================
if __name__ == "__main__":
    logger.info("="*60)
    logger.info("è…¾è®¯æ–‡æ¡£æ™ºèƒ½å¤„ç†ç³»ç»Ÿ - ç¬¦åˆè§„èŒƒç‰ˆ")
    logger.info("è®¿é—®: http://localhost:8093")
    logger.info("="*60)
    
    app.run(host='0.0.0.0', port=8093, debug=False)