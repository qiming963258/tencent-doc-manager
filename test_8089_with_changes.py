#!/usr/bin/env python3
"""
8089å…¨é“¾è·¯æµ‹è¯• - ç¡®ä¿æœ‰çœŸå®å˜æ›´å’Œæ¶‚è‰²
"""

import asyncio
import json
import csv
import logging
from pathlib import Path
from datetime import datetime
import time

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')
logger = logging.getLogger(__name__)

class Real8089TestWithChanges:
    """ç¡®ä¿æœ‰å˜æ›´çš„8089æµ‹è¯•"""

    def __init__(self):
        self.base_dir = Path('/root/projects/tencent-doc-manager')

    async def ensure_changes(self):
        """ç¡®ä¿æœ‰çœŸå®çš„å˜æ›´å¯ä»¥æµ‹è¯•"""
        logger.info("=" * 70)
        logger.info("ğŸ”§ å‡†å¤‡æœ‰å˜æ›´çš„æµ‹è¯•ç¯å¢ƒ")

        # 1. ä½¿ç”¨å·²æœ‰çš„æœ€è€çš„æ–‡ä»¶ä½œä¸ºåŸºçº¿
        from production.core_modules.week_time_manager import WeekTimeManager
        week_mgr = WeekTimeManager()
        week_info = week_mgr.get_current_week_info()

        # æŸ¥æ‰¾æ‰€æœ‰å¯ç”¨çš„CSVæ–‡ä»¶
        all_csv_files = list(self.base_dir.rglob("*å‡ºå›½é”€å”®*.csv"))
        if len(all_csv_files) < 2:
            logger.error("âŒ éœ€è¦è‡³å°‘2ä¸ªCSVæ–‡ä»¶è¿›è¡Œå¯¹æ¯”")
            return False

        # æŒ‰æ—¶é—´æ’åºï¼Œå–æœ€è€çš„ä½œä¸ºåŸºçº¿
        all_csv_files.sort(key=lambda x: x.stat().st_mtime)
        oldest_file = all_csv_files[0]
        newest_file = all_csv_files[-1]

        logger.info(f"ğŸ“‹ æœ€è€æ–‡ä»¶: {oldest_file.name}")
        logger.info(f"ğŸ“‹ æœ€æ–°æ–‡ä»¶: {newest_file.name}")

        # å¤åˆ¶æœ€è€çš„æ–‡ä»¶ä½œä¸ºåŸºçº¿
        baseline_dir = self.base_dir / f"csv_versions/2025_W{week_info['week_number']}/baseline"
        baseline_dir.mkdir(parents=True, exist_ok=True)

        baseline_file = baseline_dir / f"test_baseline_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"

        # è¯»å–å¹¶ä¿®æ”¹æ–‡ä»¶ä»¥åˆ›å»ºå˜æ›´
        with open(oldest_file, 'r', encoding='utf-8') as f:
            rows = list(csv.reader(f))

        # ä¿å­˜ä¸ºåŸºçº¿ï¼ˆæœªä¿®æ”¹ç‰ˆæœ¬ï¼‰
        with open(baseline_file, 'w', encoding='utf-8', newline='') as f:
            writer = csv.writer(f)
            writer.writerows(rows)
        logger.info(f"âœ… åˆ›å»ºåŸºçº¿: {baseline_file.name}")

        # åˆ›å»ºæœ‰å˜æ›´çš„å½“å‰ç‰ˆæœ¬
        if len(rows) > 10 and len(rows[0]) > 10:
            # ä¿®æ”¹å‡ ä¸ªå•å…ƒæ ¼
            changes_made = []

            # ä¿®æ”¹ç¬¬3è¡Œç¬¬2åˆ—ï¼ˆé¡¹ç›®ç±»å‹ï¼‰
            if len(rows[2]) > 1:
                old_val = rows[2][1]
                rows[2][1] = f"å˜æ›´æµ‹è¯•_{datetime.now().strftime('%H%M')}"
                changes_made.append(f"è¡Œ3åˆ—2: {old_val} â†’ {rows[2][1]}")

            # ä¿®æ”¹ç¬¬5è¡Œç¬¬7åˆ—ï¼ˆå…·ä½“è®¡åˆ’å†…å®¹ï¼‰
            if len(rows[4]) > 6:
                old_val = rows[4][6]
                rows[4][6] = "é‡è¦è®¡åˆ’å˜æ›´"
                changes_made.append(f"è¡Œ5åˆ—7: {old_val} â†’ {rows[4][6]}")

            # ä¿®æ”¹ç¬¬8è¡Œç¬¬11åˆ—ï¼ˆç›‘ç£äººï¼‰
            if len(rows) > 7 and len(rows[7]) > 10:
                old_val = rows[7][10]
                rows[7][10] = "æ–°ç›‘ç£äºº"
                changes_made.append(f"è¡Œ8åˆ—11: {old_val} â†’ {rows[7][10]}")

            logger.info(f"ğŸ“ åˆ›å»ºäº† {len(changes_made)} å¤„å˜æ›´:")
            for change in changes_made:
                logger.info(f"   - {change}")

        # ä¿å­˜ä¿®æ”¹åçš„æ–‡ä»¶ä½œä¸ºå½“å‰ç‰ˆæœ¬
        current_dir = self.base_dir / f"csv_versions/2025_W{week_info['week_number']}/midweek"
        current_dir.mkdir(parents=True, exist_ok=True)
        current_file = current_dir / f"test_current_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"

        with open(current_file, 'w', encoding='utf-8', newline='') as f:
            writer = csv.writer(f)
            writer.writerows(rows)
        logger.info(f"âœ… åˆ›å»ºå½“å‰ç‰ˆæœ¬: {current_file.name}")

        return {
            'baseline': str(baseline_file),
            'current': str(current_file),
            'changes': len(changes_made)
        }

    async def run_complete_workflow(self, files):
        """è¿è¡Œå®Œæ•´å·¥ä½œæµ"""
        logger.info("=" * 70)
        logger.info("ğŸš€ è¿è¡Œå®Œæ•´å·¥ä½œæµ")

        # å¯¼å…¥å¿…è¦çš„æ¨¡å—
        sys_path = str(self.base_dir)
        import sys
        if sys_path not in sys.path:
            sys.path.append(sys_path)

        # 1. å¯¹æ¯”æ–‡ä»¶
        logger.info("ğŸ“Š Step 1: å¯¹æ¯”æ–‡ä»¶")
        changes = []
        with open(files['baseline'], 'r', encoding='utf-8') as f1, \
             open(files['current'], 'r', encoding='utf-8') as f2:
            baseline_reader = list(csv.reader(f1))
            current_reader = list(csv.reader(f2))

            for row_idx, (row_baseline, row_current) in enumerate(zip(baseline_reader, current_reader)):
                for col_idx, (val_baseline, val_current) in enumerate(zip(row_baseline, row_current)):
                    if val_baseline.strip() != val_current.strip():
                        changes.append({
                            "row": row_idx,
                            "col": col_idx,
                            "old_value": val_baseline,
                            "new_value": val_current
                        })

        logger.info(f"âœ… å‘ç° {len(changes)} å¤„å˜æ›´")

        # 2. é£é™©è¯„åˆ†
        logger.info("ğŸ“Š Step 2: é£é™©è¯„åˆ†")
        from production.config import L1_COLUMNS, L2_COLUMNS, L3_COLUMNS, STANDARD_COLUMNS

        for change in changes:
            col_idx = change['col']
            if col_idx < len(STANDARD_COLUMNS):
                col_name = STANDARD_COLUMNS[col_idx]
                if col_name in L1_COLUMNS:
                    change['risk_level'] = "HIGH"
                    change['score'] = 85
                elif col_name in L2_COLUMNS:
                    change['risk_level'] = "MEDIUM"
                    change['score'] = 50
                else:
                    change['risk_level'] = "LOW"
                    change['score'] = 20
            else:
                change['risk_level'] = "LOW"
                change['score'] = 20

        high_risk = len([c for c in changes if c['risk_level'] == 'HIGH'])
        medium_risk = len([c for c in changes if c['risk_level'] == 'MEDIUM'])
        low_risk = len([c for c in changes if c['risk_level'] == 'LOW'])

        logger.info(f"   é«˜é£é™©: {high_risk}, ä¸­é£é™©: {medium_risk}, ä½é£é™©: {low_risk}")

        # 3. ç”ŸæˆExcel
        logger.info("ğŸ“Š Step 3: ç”Ÿæˆæ¶‚è‰²Excel")
        from openpyxl import Workbook
        from openpyxl.styles import PatternFill, Font
        from openpyxl.comments import Comment

        wb = Workbook()
        ws = wb.active
        ws.title = "æµ‹è¯•å¯¹æ¯”ç»“æœ"

        # è¯»å–å½“å‰æ–‡ä»¶æ•°æ®
        with open(files['current'], 'r', encoding='utf-8') as f:
            reader = csv.reader(f)
            for row_idx, row in enumerate(reader, 1):
                for col_idx, value in enumerate(row, 1):
                    ws.cell(row=row_idx, column=col_idx, value=value)

        # åº”ç”¨æ¶‚è‰²
        color_map = {
            "HIGH": "FFCCCC",
            "MEDIUM": "FFFFCC",
            "LOW": "CCFFCC"
        }

        for change in changes:
            row = change['row'] + 1
            col = change['col'] + 1
            risk = change['risk_level']
            color = color_map[risk]

            cell = ws.cell(row=row, column=col)
            cell.fill = PatternFill(
                start_color=color,
                end_color=color,
                fill_type="solid"  # å…³é”®ï¼šå¿…é¡»ä½¿ç”¨solid
            )

            # æ·»åŠ æ‰¹æ³¨
            cell.comment = Comment(
                f"åŸå€¼: {change['old_value'][:50]}\næ–°å€¼: {change['new_value'][:50]}\né£é™©: {risk}",
                "8089æµ‹è¯•ç³»ç»Ÿ"
            )

        # ä¿å­˜Excel
        excel_dir = self.base_dir / "excel_outputs/test_8089_complete"
        excel_dir.mkdir(parents=True, exist_ok=True)
        excel_file = excel_dir / f"colored_test_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx"
        wb.save(excel_file)

        logger.info(f"âœ… ç”Ÿæˆæ¶‚è‰²Excel: {excel_file.name}")
        logger.info(f"   æ¶‚è‰²å•å…ƒæ ¼: {len(changes)}")

        # 4. ç”Ÿæˆç»¼åˆæ‰“åˆ†
        logger.info("ğŸ“Š Step 4: ç”Ÿæˆç»¼åˆæ‰“åˆ†")
        from production.core_modules.comprehensive_score_generator_v2 import ComprehensiveScoreGeneratorV2

        # å‡†å¤‡æ•°æ®
        table_data = {
            'table_name': 'æµ‹è¯•æ–‡æ¡£-å‡ºå›½é”€å”®è®¡åˆ’è¡¨',
            'table_index': 0,
            'total_rows': len(current_reader),
            'total_modifications': len(changes),
            'column_details': []
        }

        # æŒ‰åˆ—ç»Ÿè®¡å˜æ›´
        for col_idx in range(len(STANDARD_COLUMNS)):
            col_changes = [c for c in changes if c['col'] == col_idx]
            table_data['column_details'].append({
                'column_name': STANDARD_COLUMNS[col_idx],
                'column_index': col_idx,
                'modification_count': len(col_changes),
                'modified_rows': [c['row'] for c in col_changes],
                'score': 0.8 if len(col_changes) > 0 else 0.1
            })

        # ç”Ÿæˆç»¼åˆæ‰“åˆ†æ–‡ä»¶
        generator = ComprehensiveScoreGeneratorV2()
        scoring_file = generator.generate(
            week_number=str(week_info['week_number']),
            table_data_list=[table_data],
            excel_urls={'æµ‹è¯•æ–‡æ¡£-å‡ºå›½é”€å”®è®¡åˆ’è¡¨': 'pending_upload'}
        )

        logger.info(f"âœ… ç”Ÿæˆç»¼åˆæ‰“åˆ†: {Path(scoring_file).name}")

        # 5. ä¸Šä¼ Excel
        logger.info("ğŸ“Š Step 5: ä¸Šä¼ åˆ°è…¾è®¯æ–‡æ¡£")

        # è¯»å–Cookie
        cookie_file = self.base_dir / 'config/cookies.json'
        with open(cookie_file) as f:
            cookie_data = json.load(f)
        cookie_string = cookie_data.get('current_cookies', '')

        from production.core_modules.tencent_doc_upload_production_v3 import quick_upload_v3

        try:
            result = await quick_upload_v3(
                cookie_string=cookie_string,
                file_path=str(excel_file),
                headless=True
            )

            if result and result.get('success'):
                upload_url = result.get('url')
                logger.info(f"âœ… ä¸Šä¼ æˆåŠŸ: {upload_url}")

                # æ›´æ–°URLé…ç½®
                url_file = self.base_dir / 'scoring_results/comprehensive/table_urls.json'
                url_data = {
                    "table_urls": {
                        "æµ‹è¯•æ–‡æ¡£-å‡ºå›½é”€å”®è®¡åˆ’è¡¨": upload_url
                    },
                    "last_update": datetime.now().isoformat()
                }
                with open(url_file, 'w') as f:
                    json.dump(url_data, f, indent=2)

                return upload_url
            else:
                logger.error(f"âŒ ä¸Šä¼ å¤±è´¥: {result}")
                return None
        except Exception as e:
            logger.error(f"âŒ ä¸Šä¼ å¼‚å¸¸: {e}")
            return None

    async def run_test(self):
        """è¿è¡Œæµ‹è¯•"""
        logger.info("=" * 70)
        logger.info("ğŸš€ å¼€å§‹8089å…¨é“¾è·¯æµ‹è¯•ï¼ˆç¡®ä¿æœ‰å˜æ›´ï¼‰")
        logger.info("=" * 70)

        # å‡†å¤‡æœ‰å˜æ›´çš„æµ‹è¯•æ–‡ä»¶
        files = await self.ensure_changes()
        if not files:
            logger.error("âŒ æ— æ³•å‡†å¤‡æµ‹è¯•æ–‡ä»¶")
            return None

        # è¿è¡Œå®Œæ•´å·¥ä½œæµ
        final_url = await self.run_complete_workflow(files)

        logger.info("=" * 70)
        if final_url:
            logger.info("âœ… æµ‹è¯•æˆåŠŸå®Œæˆ!")
            logger.info(f"ğŸ”— æœ€ç»ˆURL: {final_url}")
            logger.info("è¿™æ˜¯ä¸€ä¸ªçœŸå®çš„æ¶‚è‰²æ–‡æ¡£ï¼ŒåŒ…å«:")
            logger.info(f"  - {files['changes']} å¤„çœŸå®å˜æ›´")
            logger.info(f"  - solidå¡«å……æ¶‚è‰²")
            logger.info(f"  - æ‰¹æ³¨è¯´æ˜")
        else:
            logger.info("âŒ æµ‹è¯•å¤±è´¥")
        logger.info("=" * 70)

        return final_url

async def main():
    tester = Real8089TestWithChanges()
    final_url = await tester.run_test()

    if final_url:
        print(f"\n{'=' * 70}")
        print(f"ğŸ¯ æœ€ç»ˆè¿”å›çš„URLï¼ˆæœ‰çœŸå®æ¶‚è‰²ï¼‰: {final_url}")
        print(f"{'=' * 70}")
        print(f"è¯·æ£€æŸ¥:")
        print(f"1. è®¿é—®URLéªŒè¯æ¶‚è‰²: {final_url}")
        print(f"2. æ£€æŸ¥æ˜¯å¦æœ‰3å¤„å˜æ›´çš„æ¶‚è‰²")
        print(f"3. é¼ æ ‡æ‚¬åœæŸ¥çœ‹æ‰¹æ³¨")
    else:
        print("\nâŒ æœªèƒ½ç”Ÿæˆæœ€ç»ˆURL")

    return final_url

if __name__ == "__main__":
    result = asyncio.run(main())