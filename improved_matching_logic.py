#!/usr/bin/env python3
"""
æ”¹è¿›çš„æ–‡ä»¶åŒ¹é…é€»è¾‘å®ç°
ä½¿ç”¨ç²¾ç¡®åŒ¹é…æ›¿ä»£åŒ…å«åŒ¹é…ï¼Œå¹¶åˆ›å»ºURLæ˜ å°„æ–‡ä»¶
"""
import os
import re
import json
from datetime import datetime
from pathlib import Path

def extract_doc_name_from_filename(filename):
    """ä»æ–‡ä»¶åä¸­ç²¾ç¡®æå–æ–‡æ¡£åç§°

    Args:
        filename: æ–‡ä»¶åï¼Œå¦‚ tencent_å‡ºå›½é”€å”®è®¡åˆ’è¡¨_20250915_0145_baseline_W39.csv

    Returns:
        æ–‡æ¡£åç§°ï¼Œå¦‚ å‡ºå›½é”€å”®è®¡åˆ’è¡¨
    """
    basename = os.path.basename(filename)

    # åŒ¹é…æ ¼å¼ï¼štencent_{æ–‡æ¡£å}_{æ—¶é—´æˆ³}_{ç‰ˆæœ¬}_W{å‘¨}.{æ‰©å±•å}
    match = re.search(r'^tencent_(.+?)_\d{8}_\d{4}_(baseline|midweek)_W\d+\.\w+$', basename)
    if match:
        return match.group(1)

    # å¤‡ç”¨åŒ¹é…ï¼ˆå¦‚æœæ ¼å¼ä¸å®Œå…¨æ ‡å‡†ï¼‰
    match = re.search(r'^tencent_(.+?)_\d{8}_\d{4}', basename)
    if match:
        return match.group(1)

    return None

def find_exact_matching_baseline(doc_name, baseline_files):
    """ç²¾ç¡®åŒ¹é…åŸºçº¿æ–‡ä»¶

    Args:
        doc_name: è¦åŒ¹é…çš„æ–‡æ¡£åç§°
        baseline_files: åŸºçº¿æ–‡ä»¶åˆ—è¡¨

    Returns:
        åŒ¹é…çš„åŸºçº¿æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœæ²¡æœ‰åŒ¹é…åˆ™è¿”å›None
    """
    for baseline in baseline_files:
        baseline_doc_name = extract_doc_name_from_filename(baseline)

        # ç²¾ç¡®åŒ¹é…
        if baseline_doc_name and baseline_doc_name == doc_name:
            return baseline

    return None

class BaselineURLMapper:
    """åŸºçº¿æ–‡ä»¶URLæ˜ å°„ç®¡ç†å™¨"""

    def __init__(self):
        self.mapping_file = Path('/root/projects/tencent-doc-manager/baseline_url_mapping.json')
        self.mapping = self.load_mapping()

    def load_mapping(self):
        """åŠ è½½æ˜ å°„æ–‡ä»¶"""
        if self.mapping_file.exists():
            with open(self.mapping_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        return {}

    def save_mapping(self):
        """ä¿å­˜æ˜ å°„æ–‡ä»¶"""
        self.mapping_file.parent.mkdir(parents=True, exist_ok=True)
        with open(self.mapping_file, 'w', encoding='utf-8') as f:
            json.dump(self.mapping, f, ensure_ascii=False, indent=2)

    def add_baseline_mapping(self, filename, url, doc_name=None):
        """æ·»åŠ åŸºçº¿æ–‡ä»¶åˆ°URLçš„æ˜ å°„

        Args:
            filename: åŸºçº¿æ–‡ä»¶å
            url: è…¾è®¯æ–‡æ¡£URL
            doc_name: æ–‡æ¡£åç§°ï¼ˆå¯é€‰ï¼‰
        """
        basename = os.path.basename(filename)

        if doc_name is None:
            doc_name = extract_doc_name_from_filename(filename)

        self.mapping[basename] = {
            "url": url,
            "doc_name": doc_name,
            "doc_id": self._extract_doc_id(url),
            "added_time": datetime.now().isoformat(),
            "full_path": str(filename)
        }

        self.save_mapping()

    def get_url_for_baseline(self, filename):
        """è·å–åŸºçº¿æ–‡ä»¶å¯¹åº”çš„URL"""
        basename = os.path.basename(filename)
        if basename in self.mapping:
            return self.mapping[basename].get('url')
        return None

    def get_baseline_for_url(self, url):
        """æ ¹æ®URLæŸ¥æ‰¾å¯¹åº”çš„åŸºçº¿æ–‡ä»¶"""
        for filename, info in self.mapping.items():
            if info.get('url') == url:
                return filename
        return None

    def _extract_doc_id(self, url):
        """ä»URLæå–æ–‡æ¡£ID"""
        if 'docs.qq.com/sheet/' in url:
            return url.split('/')[-1]
        return None

    def cleanup_deleted_files(self):
        """æ¸…ç†å·²åˆ é™¤æ–‡ä»¶çš„æ˜ å°„"""
        to_remove = []
        for filename, info in self.mapping.items():
            full_path = info.get('full_path')
            if full_path and not os.path.exists(full_path):
                to_remove.append(filename)

        for filename in to_remove:
            del self.mapping[filename]
            print(f"æ¸…ç†å·²åˆ é™¤æ–‡ä»¶çš„æ˜ å°„: {filename}")

        if to_remove:
            self.save_mapping()

def apply_improved_matching_to_8093():
    """åº”ç”¨æ”¹è¿›çš„åŒ¹é…é€»è¾‘åˆ°8093å·¥ä½œæµ"""

    # è¯»å–8093æ–‡ä»¶
    file_path = '/root/projects/tencent-doc-manager/production_integrated_test_system_8093.py'

    if not os.path.exists(file_path):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        return

    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read()

    # æŸ¥æ‰¾éœ€è¦æ›¿æ¢çš„ä»£ç æ®µ
    old_matching = """if doc_name in basename:
                                    matched_baseline = baseline"""

    new_matching = """# ä½¿ç”¨ç²¾ç¡®åŒ¹é…
                                baseline_doc_name = extract_doc_name_from_filename(baseline)
                                if baseline_doc_name and baseline_doc_name == doc_name:
                                    matched_baseline = baseline"""

    if old_matching in content:
        # æ·»åŠ å¿…è¦çš„å¯¼å…¥
        if 'def extract_doc_name_from_filename' not in content:
            # åœ¨æ–‡ä»¶å¼€å¤´æ·»åŠ å‡½æ•°å®šä¹‰
            import_section = """
# ç²¾ç¡®åŒ¹é…å‡½æ•°
def extract_doc_name_from_filename(filename):
    \"\"\"ä»æ–‡ä»¶åä¸­ç²¾ç¡®æå–æ–‡æ¡£åç§°\"\"\"
    import re
    basename = os.path.basename(filename)
    match = re.search(r'^tencent_(.+?)_\\d{8}_\\d{4}_(baseline|midweek)_W\\d+\\.\\w+$', basename)
    if match:
        return match.group(1)
    match = re.search(r'^tencent_(.+?)_\\d{8}_\\d{4}', basename)
    if match:
        return match.group(1)
    return None

"""
            # åœ¨ç¬¬ä¸€ä¸ªå‡½æ•°å®šä¹‰å‰æ’å…¥
            content = content.replace('def main():', import_section + 'def main():', 1)

        # æ›¿æ¢åŒ¹é…é€»è¾‘
        content = content.replace(old_matching, new_matching)

        # ä¿å­˜ä¿®æ”¹
        backup_path = file_path + '.backup_before_matching_fix'
        with open(backup_path, 'w', encoding='utf-8') as f:
            f.write(content)

        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(content)

        print(f"âœ… å·²åº”ç”¨ç²¾ç¡®åŒ¹é…é€»è¾‘åˆ° {file_path}")
        print(f"âœ… å¤‡ä»½ä¿å­˜åœ¨: {backup_path}")
    else:
        print("âš ï¸ æœªæ‰¾åˆ°éœ€è¦æ›¿æ¢çš„åŒ¹é…é€»è¾‘ï¼Œå¯èƒ½å·²ç»ä¿®å¤")

def test_improved_matching():
    """æµ‹è¯•æ”¹è¿›çš„åŒ¹é…é€»è¾‘"""
    print("\nğŸ§ª æµ‹è¯•æ”¹è¿›çš„åŒ¹é…é€»è¾‘")
    print("=" * 60)

    # æµ‹è¯•æ–‡ä»¶å
    test_files = [
        "tencent_å‡ºå›½é”€å”®è®¡åˆ’è¡¨_20250915_0145_baseline_W39.csv",
        "tencent_å›å›½é”€å”®è®¡åˆ’è¡¨_20250914_2309_baseline_W39.csv",
        "tencent_å°çº¢ä¹¦éƒ¨é—¨_20250915_0146_baseline_W39.csv"
    ]

    # æµ‹è¯•æå–æ–‡æ¡£å
    print("\n1. æµ‹è¯•æ–‡æ¡£åæå–:")
    for file in test_files:
        doc_name = extract_doc_name_from_filename(file)
        print(f"  {file}")
        print(f"  â†’ æ–‡æ¡£å: {doc_name}")

    # æµ‹è¯•ç²¾ç¡®åŒ¹é…
    print("\n2. æµ‹è¯•ç²¾ç¡®åŒ¹é…:")
    baseline_files = test_files

    test_cases = [
        ("å‡ºå›½é”€å”®è®¡åˆ’è¡¨", "åº”è¯¥åŒ¹é…ç¬¬1ä¸ªæ–‡ä»¶"),
        ("å‡ºå›½", "ä¸åº”è¯¥åŒ¹é…ï¼ˆéå®Œå…¨åŒ¹é…ï¼‰"),
        ("å°çº¢ä¹¦éƒ¨é—¨", "åº”è¯¥åŒ¹é…ç¬¬3ä¸ªæ–‡ä»¶"),
        ("å°çº¢ä¹¦", "ä¸åº”è¯¥åŒ¹é…ï¼ˆéå®Œå…¨åŒ¹é…ï¼‰")
    ]

    for doc_name, expected in test_cases:
        matched = find_exact_matching_baseline(doc_name, baseline_files)
        if matched:
            print(f"  '{doc_name}' â†’ åŒ¹é…: {os.path.basename(matched)}")
        else:
            print(f"  '{doc_name}' â†’ æ— åŒ¹é…")
        print(f"    é¢„æœŸ: {expected}")

    # æµ‹è¯•URLæ˜ å°„
    print("\n3. æµ‹è¯•URLæ˜ å°„:")
    mapper = BaselineURLMapper()

    # æ·»åŠ æ˜ å°„
    mapper.add_baseline_mapping(
        test_files[0],
        "https://docs.qq.com/sheet/DWEFNU25TemFnZXJN"
    )

    # è·å–URL
    url = mapper.get_url_for_baseline(test_files[0])
    print(f"  è·å–URL: {url}")

    # åå‘æŸ¥æ‰¾
    baseline = mapper.get_baseline_for_url(url)
    print(f"  åå‘æŸ¥æ‰¾: {baseline}")

    print("\nâœ… æµ‹è¯•å®Œæˆ")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”§ æ”¹è¿›çš„æ–‡ä»¶åŒ¹é…é€»è¾‘å®ç°")
    print("=" * 80)

    # 1. æµ‹è¯•æ–°é€»è¾‘
    test_improved_matching()

    # 2. åº”ç”¨åˆ°8093å·¥ä½œæµ
    print("\nğŸ“ åº”ç”¨åˆ°8093å·¥ä½œæµ:")
    apply_improved_matching_to_8093()

    # 3. æ¸…ç†æ— æ•ˆæ˜ å°„
    print("\nğŸ§¹ æ¸…ç†æ— æ•ˆæ˜ å°„:")
    mapper = BaselineURLMapper()
    mapper.cleanup_deleted_files()

    print("\nâœ… æ‰€æœ‰ä»»åŠ¡å®Œæˆ")

if __name__ == "__main__":
    main()