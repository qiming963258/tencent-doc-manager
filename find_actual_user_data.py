#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¯»æ‰¾çœŸå®çš„ç”¨æˆ·ä¸šåŠ¡æ•°æ®
æˆ‘ä»¬ä¹‹å‰è§£æçš„å¯èƒ½åªæ˜¯å…ƒæ•°æ®ï¼ŒçœŸå®çš„è¡¨æ ¼æ•°æ®å¯èƒ½åœ¨å…¶ä»–åœ°æ–¹
"""

import json
import urllib.parse
import base64
import zlib
from pathlib import Path

def analyze_full_ejs_structure():
    """å®Œæ•´åˆ†æEJSæ–‡ä»¶ç»“æ„ï¼Œå¯»æ‰¾ç”¨æˆ·æ•°æ®"""
    print("="*60)
    print("å®Œæ•´åˆ†æEJSæ–‡ä»¶ï¼Œå¯»æ‰¾ç”¨æˆ·ä¸šåŠ¡æ•°æ®")
    print("="*60)
    
    ejs_file = "/root/projects/tencent-doc-manager/test_DWEVjZndkR2xVSWJN_CSVæ ¼å¼_20250827_231718.csv"
    
    with open(ejs_file, 'r', encoding='utf-8') as f:
        lines = f.readlines()
    
    print(f"EJSæ–‡ä»¶æ€»è¡Œæ•°: {len(lines)}")
    
    # è¯¦ç»†åˆ†ææ¯ä¸€è¡Œ
    for i, line in enumerate(lines):
        line = line.strip()
        if not line:
            continue
            
        print(f"\nç¬¬{i+1}è¡Œåˆ†æ:")
        print(f"é•¿åº¦: {len(line)} å­—ç¬¦")
        
        if line == 'head':
            print("ç±»å‹: EJSå¤´éƒ¨")
        elif line == 'json':
            print("ç±»å‹: JSONæ®µå¼€å§‹")
        elif line == 'text':
            print("ç±»å‹: æ–‡æœ¬æ®µ")
        elif line.isdigit():
            print(f"ç±»å‹: é•¿åº¦æ ‡è¯† ({line} å­—ç¬¦)")
        elif line.startswith('{') and line.endswith('}'):
            print("ç±»å‹: JSONæ•°æ®")
            try:
                data = json.loads(line)
                print(f"JSON Keys: {list(data.keys())}")
                
                # æ£€æŸ¥bodyDataä¸­çš„å®é™…å†…å®¹
                if 'bodyData' in data:
                    body_data = data['bodyData']
                    print(f"bodyData Keys: {list(body_data.keys())}")
                    
                    # æŸ¥æ‰¾å¯èƒ½åŒ…å«ç”¨æˆ·æ•°æ®çš„å­—æ®µ
                    if 'cont' in body_data:
                        cont = body_data['cont']
                        print(f"å‘ç°contå­—æ®µï¼Œé•¿åº¦: {len(cont)}")
                        
                        # å°è¯•è§£æcontå­—æ®µï¼ˆå¯èƒ½åŒ…å«å®é™…æ•°æ®ï¼‰
                        if isinstance(cont, str) and len(cont) > 100:
                            print("åˆ†æcontå†…å®¹...")
                            analyze_cont_field(cont)
                    
                    # æ£€æŸ¥å…¶ä»–å¯èƒ½çš„æ•°æ®å­—æ®µ
                    for key, value in body_data.items():
                        if isinstance(value, str) and len(value) > 1000:
                            print(f"å‘ç°å¤§æ•°æ®å­—æ®µ {key}: {len(value)} å­—ç¬¦")
            except Exception as e:
                print(f"JSONè§£æå¤±è´¥: {e}")
                
        elif line.startswith('%7B'):
            print("ç±»å‹: URLç¼–ç æ•°æ®")
            try:
                decoded = urllib.parse.unquote(line)
                data = json.loads(decoded)
                
                # è¿™æ˜¯æˆ‘ä»¬ä¹‹å‰åˆ†æçš„workbookæ•°æ®
                print("åŒ…å«workbookç­‰å­—æ®µï¼ˆå·²åˆ†æè¿‡ï¼‰")
                
                # ä½†æ£€æŸ¥æ˜¯å¦è¿˜æœ‰å…¶ä»–æ•°æ®
                for key, value in data.items():
                    if key not in ['workbook', 'related_sheet', 'max_row', 'max_col']:
                        print(f"å‘ç°å…¶ä»–å­—æ®µ {key}: {type(value)}")
                        if isinstance(value, str) and len(value) > 100:
                            print(f"  å¯èƒ½åŒ…å«ç”¨æˆ·æ•°æ®ï¼Œé•¿åº¦: {len(value)}")
                            
            except Exception as e:
                print(f"URLè§£ç å¤±è´¥: {e}")
        else:
            print(f"ç±»å‹: å…¶ä»–")
            if len(line) > 50:
                print(f"é¢„è§ˆ: {line[:100]}...")

def analyze_cont_field(cont_data):
    """åˆ†æbodyData.contå­—æ®µï¼Œå¯èƒ½åŒ…å«å®é™…ç”¨æˆ·æ•°æ®"""
    print("\n  åˆ†æcontå­—æ®µå†…å®¹:")
    
    # contå­—æ®µå¯èƒ½æ˜¯base64ç¼–ç æˆ–å…¶ä»–æ ¼å¼
    try:
        # å°è¯•base64è§£ç 
        decoded = base64.b64decode(cont_data)
        print(f"  Base64è§£ç æˆåŠŸ: {len(decoded)} bytes")
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯å‹ç¼©æ•°æ®
        if decoded[:2] in [b'\x78\x01', b'\x78\x9c', b'\x78\xda']:
            print("  æ£€æµ‹åˆ°zlibå‹ç¼©")
            try:
                decompressed = zlib.decompress(decoded)
                print(f"  è§£å‹æˆåŠŸ: {len(decompressed)} bytes")
                
                # ä¿å­˜è§£å‹åçš„æ•°æ®
                output_file = "cont_field_decompressed.bin"
                with open(output_file, 'wb') as f:
                    f.write(decompressed)
                print(f"  å·²ä¿å­˜: {output_file}")
                
                # å°è¯•è§£æå†…å®¹
                try:
                    text = decompressed.decode('utf-8')
                    print(f"  UTF-8è§£ç æˆåŠŸï¼ŒæŸ¥æ‰¾ç”¨æˆ·æ•°æ®...")
                    
                    # æŸ¥æ‰¾å¯èƒ½çš„è¡¨æ ¼æ•°æ®æ ‡è¯†
                    keywords = ['å°çº¢ä¹¦', 'éƒ¨é—¨', 'æµ‹è¯•', 'cell', 'row', 'column', 'data']
                    found_keywords = [kw for kw in keywords if kw in text]
                    
                    if found_keywords:
                        print(f"  âœ… æ‰¾åˆ°ç”¨æˆ·æ•°æ®å…³é”®è¯: {found_keywords}")
                        return analyze_user_data_content(text)
                    else:
                        print("  æœªæ‰¾åˆ°æ˜æ˜¾çš„ç”¨æˆ·æ•°æ®å…³é”®è¯")
                        
                except Exception as e:
                    print(f"  UTF-8è§£ç å¤±è´¥: {e}")
                    
            except Exception as e:
                print(f"  zlibè§£å‹å¤±è´¥: {e}")
        else:
            print("  ä¸æ˜¯zlibå‹ç¼©æ ¼å¼")
            
    except Exception as e:
        print(f"  Base64è§£ç å¤±è´¥: {e}")
        
    # å°è¯•ç›´æ¥æ–‡æœ¬æœç´¢
    if 'å°çº¢ä¹¦' in cont_data or 'éƒ¨é—¨' in cont_data:
        print("  âœ… åœ¨åŸå§‹contæ•°æ®ä¸­æ‰¾åˆ°ç”¨æˆ·å…³é”®è¯!")
        return True
    
    return False

def analyze_user_data_content(text_content):
    """åˆ†æå¯èƒ½çš„ç”¨æˆ·æ•°æ®å†…å®¹"""
    print("\n    è¯¦ç»†åˆ†æç”¨æˆ·æ•°æ®:")
    
    # æŸ¥æ‰¾è¡¨æ ¼ç›¸å…³å†…å®¹
    import re
    
    # æŸ¥æ‰¾ä¸­æ–‡å†…å®¹
    chinese_pattern = r'[\u4e00-\u9fff]+'
    chinese_matches = re.findall(chinese_pattern, text_content)
    
    if chinese_matches:
        print(f"    æ‰¾åˆ°ä¸­æ–‡å†…å®¹: {len(chinese_matches)}ä¸ª")
        for i, match in enumerate(chinese_matches[:10]):
            print(f"      {i+1}. {match}")
    
    # æŸ¥æ‰¾æ•°å­—
    number_pattern = r'\d+'
    numbers = re.findall(number_pattern, text_content)
    if len(numbers) > 10:
        print(f"    æ‰¾åˆ°å¤§é‡æ•°å­—: {len(numbers)}ä¸ª")
    
    # æŸ¥æ‰¾å¯èƒ½çš„å•å…ƒæ ¼å¼•ç”¨
    cell_pattern = r'[A-Z]+\d+'
    cells = re.findall(cell_pattern, text_content)
    if cells:
        print(f"    æ‰¾åˆ°å•å…ƒæ ¼å¼•ç”¨: {cells[:10]}")
    
    # ä¿å­˜æ–‡æœ¬ä»¥ä¾¿è¿›ä¸€æ­¥åˆ†æ
    with open('user_data_content.txt', 'w', encoding='utf-8') as f:
        f.write(text_content)
    
    print(f"    ç”¨æˆ·æ•°æ®å·²ä¿å­˜: user_data_content.txt")
    return True

def search_in_related_sheet():
    """æ£€æŸ¥related_sheetå­—æ®µï¼Œå¯èƒ½åŒ…å«å®é™…æ•°æ®"""
    print("\n" + "="*60)
    print("åˆ†ærelated_sheetå­—æ®µ")
    print("="*60)
    
    # ä»ä¹‹å‰çš„åˆ†æä¸­ï¼Œæˆ‘ä»¬çŸ¥é“æœ‰related_sheetå­—æ®µ
    ejs_file = "/root/projects/tencent-doc-manager/test_DWEVjZndkR2xVSWJN_CSVæ ¼å¼_20250827_231718.csv"
    
    with open(ejs_file, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # æŸ¥æ‰¾URLç¼–ç çš„æ•°æ®è¡Œ
    lines = content.split('\n')
    for line in lines:
        if line.startswith('%7B'):
            try:
                decoded = urllib.parse.unquote(line)
                data = json.loads(decoded)
                
                if 'related_sheet' in data:
                    related_sheet = data['related_sheet']
                    print(f"related_sheeté•¿åº¦: {len(related_sheet)}")
                    
                    # å°è¯•base64è§£ç 
                    try:
                        decoded_sheet = base64.b64decode(related_sheet)
                        print(f"Base64è§£ç : {len(decoded_sheet)} bytes")
                        
                        # æ£€æŸ¥å‹ç¼©
                        if decoded_sheet[:2] in [b'\x78\x01', b'\x78\x9c', b'\x78\xda']:
                            decompressed = zlib.decompress(decoded_sheet)
                            print(f"zlibè§£å‹: {len(decompressed)} bytes")
                            
                            # ä¿å­˜related_sheetæ•°æ®
                            with open('related_sheet_data.bin', 'wb') as f:
                                f.write(decompressed)
                            
                            # å°è¯•åˆ†æå†…å®¹
                            try:
                                text = decompressed.decode('utf-8', errors='ignore')
                                
                                # æŸ¥æ‰¾ç”¨æˆ·æ•°æ®
                                if 'å°çº¢ä¹¦' in text or 'éƒ¨é—¨' in text or 'æµ‹è¯•' in text:
                                    print("âœ… åœ¨related_sheetä¸­æ‰¾åˆ°ç”¨æˆ·æ•°æ®!")
                                    
                                    with open('related_sheet_text.txt', 'w', encoding='utf-8') as f:
                                        f.write(text)
                                    
                                    return analyze_actual_table_data(text)
                                else:
                                    print("related_sheetä¸­æœªæ‰¾åˆ°æ˜æ˜¾çš„ç”¨æˆ·æ•°æ®")
                                    
                            except Exception as e:
                                print(f"related_sheetæ–‡æœ¬è§£ç å¤±è´¥: {e}")
                                
                    except Exception as e:
                        print(f"related_sheetè§£ç å¤±è´¥: {e}")
                        
            except Exception as e:
                continue
    
    return False

def analyze_actual_table_data(content):
    """åˆ†æå®é™…çš„è¡¨æ ¼æ•°æ®"""
    print("\n    åˆ†æå®é™…è¡¨æ ¼æ•°æ®:")
    
    # è¿™é‡Œåº”è¯¥åŒ…å«çœŸå®çš„è¡¨æ ¼æ•°æ®è§£æé€»è¾‘
    # æ ¹æ®è…¾è®¯æ–‡æ¡£çš„æ ¼å¼ç‰¹ç‚¹è¿›è¡Œè§£æ
    
    lines = content.split('\n')
    print(f"    æ•°æ®æ€»è¡Œæ•°: {len(lines)}")
    
    # æŸ¥æ‰¾è¡¨æ ¼æ•°æ®æ¨¡å¼
    table_data = []
    for line in lines[:20]:  # æ£€æŸ¥å‰20è¡Œ
        if line.strip() and len(line) < 200:  # åˆç†çš„è¡Œé•¿åº¦
            print(f"    æ•°æ®è¡Œ: {line[:100]}...")
            table_data.append(line.strip())
    
    if table_data:
        print(f"    âœ… æ‰¾åˆ° {len(table_data)} è¡Œè¡¨æ ¼æ•°æ®")
        return True
    
    return False

def main():
    """ä¸»å‡½æ•° - å®é™…æµ‹è¯•ç”¨æˆ·æ•°æ®æå–"""
    print("ğŸ§ª å®é™…æµ‹è¯•ï¼šæå–çœŸå®ç”¨æˆ·ä¸šåŠ¡æ•°æ®")
    print("="*60)
    
    success_count = 0
    
    # 1. å®Œæ•´åˆ†æEJSç»“æ„
    print("æ­¥éª¤1: å®Œæ•´åˆ†æEJSæ–‡ä»¶ç»“æ„")
    analyze_full_ejs_structure()
    
    # 2. æ£€æŸ¥related_sheetå­—æ®µ
    print("\næ­¥éª¤2: åˆ†ærelated_sheetå­—æ®µ")
    if search_in_related_sheet():
        success_count += 1
    
    # 3. æ£€æŸ¥å…¶ä»–å¯èƒ½çš„æ•°æ®æº
    print("\næ­¥éª¤3: æ£€æŸ¥ç”Ÿæˆçš„æ–‡ä»¶")
    
    files_to_check = [
        'cont_field_decompressed.bin',
        'related_sheet_data.bin', 
        'user_data_content.txt',
        'related_sheet_text.txt'
    ]
    
    found_files = []
    for file in files_to_check:
        if Path(file).exists():
            found_files.append(file)
            print(f"âœ… ç”Ÿæˆäº†: {file}")
    
    # 4. æœ€ç»ˆè¯„ä¼°
    print("\n" + "="*60)
    print("å®é™…æµ‹è¯•ç»“æœ")
    print("="*60)
    
    if success_count > 0:
        print(f"âœ… æˆåŠŸæ‰¾åˆ°ç”¨æˆ·æ•°æ®æº: {success_count}ä¸ª")
        print("ğŸ‰ å®é™…æµ‹è¯•æˆåŠŸï¼æˆ‘ä»¬èƒ½å¤Ÿæå–çœŸå®çš„ç”¨æˆ·ä¸šåŠ¡æ•°æ®")
    else:
        print("âŒ æœªèƒ½æ‰¾åˆ°æ˜ç¡®çš„ç”¨æˆ·ä¸šåŠ¡æ•°æ®")
        print("ğŸ’¡ å¯èƒ½éœ€è¦:")
        print("   1. è¿›ä¸€æ­¥åˆ†æprotobufç»“æ„")
        print("   2. æµ‹è¯•åŒ…å«æ›´å¤šä¸šåŠ¡æ•°æ®çš„æ–‡æ¡£")
        print("   3. å®Œå–„æµè§ˆå™¨è‡ªåŠ¨åŒ–æ–¹æ¡ˆ")
    
    if found_files:
        print(f"\nğŸ“ ç”Ÿæˆçš„åˆ†ææ–‡ä»¶: {len(found_files)}ä¸ª")
        for file in found_files:
            size = Path(file).stat().st_size
            print(f"   {file}: {size:,} bytes")

if __name__ == "__main__":
    main()