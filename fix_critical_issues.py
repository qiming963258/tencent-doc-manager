#!/usr/bin/env python3
"""
ä¿®å¤ç³»ç»Ÿå…³é”®é—®é¢˜ï¼š
1. ç»¼åˆæ‰“åˆ†æ–‡ä»¶è·¯å¾„é”™è¯¯ï¼ˆåº”è¯¥æŒ‰å‘¨ç»„ç»‡ï¼‰
2. æ–‡ä»¶åŒ¹é…é€»è¾‘åº”è¯¥å®Œå…¨ç›¸ç­‰è€Œä¸æ˜¯åŒ…å«
"""

import os
import shutil
import json
from pathlib import Path

def fix_comprehensive_files_organization():
    """ä¿®å¤ç»¼åˆæ‰“åˆ†æ–‡ä»¶çš„ç»„ç»‡ç»“æ„"""
    print("\n" + "=" * 60)
    print("ä¿®å¤ç»¼åˆæ‰“åˆ†æ–‡ä»¶ç»„ç»‡ç»“æ„")
    print("=" * 60)

    comprehensive_dir = Path("scoring_results/comprehensive")
    w39_dir = Path("scoring_results/2025_W39")

    # ç¡®ä¿W39ç›®å½•å­˜åœ¨
    w39_dir.mkdir(parents=True, exist_ok=True)

    # æŸ¥æ‰¾æ‰€æœ‰W39çš„ç»¼åˆæ‰“åˆ†æ–‡ä»¶
    w39_files = list(comprehensive_dir.glob("*W39*.json"))

    print(f"\næ‰¾åˆ° {len(w39_files)} ä¸ªW39ç»¼åˆæ‰“åˆ†æ–‡ä»¶éœ€è¦ç§»åŠ¨")

    moved_count = 0
    for file in w39_files:
        dest = w39_dir / file.name
        try:
            # å¤åˆ¶æ–‡ä»¶åˆ°æ­£ç¡®ä½ç½®
            shutil.copy2(file, dest)
            print(f"âœ… å¤åˆ¶: {file.name} -> {dest}")
            moved_count += 1
        except Exception as e:
            print(f"âŒ å¤åˆ¶å¤±è´¥ {file.name}: {e}")

    print(f"\næ€»å…±å¤åˆ¶äº† {moved_count} ä¸ªæ–‡ä»¶åˆ° 2025_W39 ç›®å½•")

def analyze_file_matching_logic():
    """åˆ†ææ–‡ä»¶åŒ¹é…é€»è¾‘é—®é¢˜"""
    print("\n" + "=" * 60)
    print("åˆ†ææ–‡ä»¶åŒ¹é…é€»è¾‘")
    print("=" * 60)

    # æ£€æŸ¥å®é™…çš„åŸºçº¿æ–‡ä»¶
    baseline_dir = Path("csv_versions/2025_W39/baseline")
    baseline_files = list(baseline_dir.glob("*.csv"))

    # æ£€æŸ¥è½¯åˆ é™¤çš„æ–‡ä»¶
    deleted_dir = baseline_dir / ".deleted"
    deleted_files = list(deleted_dir.glob("*.csv")) if deleted_dir.exists() else []

    print(f"\næ´»è·ƒåŸºçº¿æ–‡ä»¶: {len(baseline_files)}")
    for f in baseline_files:
        print(f"  - {f.name}")

    print(f"\nè½¯åˆ é™¤åŸºçº¿æ–‡ä»¶: {len(deleted_files)}")
    for f in deleted_files:
        print(f"  - {f.name}")

    # åˆ†æåŒ¹é…é—®é¢˜
    print("\nåŒ¹é…é€»è¾‘é—®é¢˜åˆ†æï¼š")
    print("âŒ å½“å‰é€»è¾‘ï¼šä½¿ç”¨å­—ç¬¦ä¸²åŒ…å«åˆ¤æ–­ 'doc_name in filename'")
    print("   é—®é¢˜1: 'å‡ºå›½' ä¼šåŒæ—¶åŒ¹é… 'å‡ºå›½é”€å”®è®¡åˆ’è¡¨' å’Œ 'å‰¯æœ¬-æµ‹è¯•ç‰ˆæœ¬-å‡ºå›½é”€å”®è®¡åˆ’è¡¨'")
    print("   é—®é¢˜2: è½¯åˆ é™¤æ–‡ä»¶ä¸åº”è¯¥å‚ä¸åŒ¹é…")
    print("\nâœ… å»ºè®®æ”¹è¿›ï¼š")
    print("   1. ä½¿ç”¨å®Œæ•´æ–‡æ¡£ååŒ¹é…")
    print("   2. æ’é™¤.deletedç›®å½•ä¸­çš„æ–‡ä»¶")
    print("   3. å»ºç«‹æ–‡æ¡£ååˆ°æ–‡ä»¶è·¯å¾„çš„æ˜ç¡®æ˜ å°„")

def verify_url_flow():
    """éªŒè¯URLæµè½¬é€»è¾‘"""
    print("\n" + "=" * 60)
    print("éªŒè¯URLæµè½¬é€»è¾‘")
    print("=" * 60)

    print("\nURLæµè½¬é“¾è·¯ï¼š")
    print("1. åŸºçº¿æ–‡ä»¶ï¼ˆCSVï¼‰ï¼šä¸åŒ…å«URLä¿¡æ¯ âŒ")
    print("2. ä¸‹è½½æ—¶ï¼šä»è…¾è®¯æ–‡æ¡£URLä¸‹è½½ï¼ŒURLåœ¨å‚æ•°ä¸­")
    print("3. å¯¹æ¯”æ‰“åˆ†ï¼šä¸æ¶‰åŠURL")
    print("4. Excelæ¶‚è‰²ï¼šç”Ÿæˆæœ¬åœ°Excelæ–‡ä»¶")
    print("5. ä¸Šä¼ è…¾è®¯æ–‡æ¡£ï¼šè·å¾—æ–°çš„upload_url âœ…")
    print("6. ç»¼åˆæ‰“åˆ†ï¼šæ¥æ”¶upload_urlä½œä¸ºexcel_urlå‚æ•°")
    print("7. excel_urlså­—å…¸ï¼š{è¡¨æ ¼å: upload_url}")
    print("8. çƒ­åŠ›å›¾æ˜¾ç¤ºï¼šé€šè¿‡è¡¨æ ¼åæŸ¥æ‰¾excel_urlsè·å–URL")

    print("\nå…³é”®å‘ç°ï¼š")
    print("âš ï¸ excel_urlæ˜¯ä¸Šä¼ åçš„URLï¼Œä¸æ˜¯åŸºçº¿æ–‡æ¡£çš„URL")
    print("âš ï¸ åŸºçº¿æ–‡ä»¶æœ¬èº«ä¸å­˜å‚¨URLä¿¡æ¯")
    print("âš ï¸ URLæ˜ å°„ä¾èµ–è¡¨æ ¼åç§°ï¼Œåç§°å¿…é¡»å®Œå…¨ä¸€è‡´")

def check_real_documents_config():
    """æ£€æŸ¥çœŸå®æ–‡æ¡£é…ç½®"""
    print("\n" + "=" * 60)
    print("æ£€æŸ¥çœŸå®æ–‡æ¡£é…ç½®")
    print("=" * 60)

    config_path = Path("production/config/real_documents.json")
    if config_path.exists():
        with open(config_path, 'r', encoding='utf-8') as f:
            config = json.load(f)

        docs = config.get('documents', [])
        print(f"\né…ç½®çš„çœŸå®æ–‡æ¡£: {len(docs)}")
        for doc in docs:
            print(f"\næ–‡æ¡£: {doc['name']}")
            print(f"  URL: {doc['url']}")
            print(f"  çŠ¶æ€: {'âœ… æ´»è·ƒ' if doc['url'].startswith('https://docs.qq.com') else 'âŒ æ— æ•ˆ'}")

            # æ£€æŸ¥å¯¹åº”çš„åŸºçº¿æ–‡ä»¶
            baseline_pattern = f"*{doc['name'].split('-')[-1]}*baseline*.csv"
            baseline_files = list(Path("csv_versions/2025_W39/baseline").glob(baseline_pattern))
            if baseline_files:
                print(f"  åŸºçº¿æ–‡ä»¶: {baseline_files[0].name}")
            else:
                print(f"  åŸºçº¿æ–‡ä»¶: âŒ æœªæ‰¾åˆ°")

def main():
    """ä¸»å‡½æ•°"""
    print("=" * 80)
    print("ğŸ”§ ç³»ç»Ÿå…³é”®é—®é¢˜ä¿®å¤ä¸åˆ†æ")
    print("=" * 80)

    # æ‰§è¡Œå„é¡¹æ£€æŸ¥å’Œä¿®å¤
    fix_comprehensive_files_organization()
    analyze_file_matching_logic()
    verify_url_flow()
    check_real_documents_config()

    print("\n" + "=" * 80)
    print("ğŸ” æ·±åº¦åˆ†æç»“è®º")
    print("=" * 80)

    print("""
ç³»ç»Ÿå­˜åœ¨çš„å…³é”®é—®é¢˜ï¼š

1. **æ–‡ä»¶ç»„ç»‡æ··ä¹±** âš ï¸
   - ç»¼åˆæ‰“åˆ†æ–‡ä»¶å­˜å‚¨åœ¨é”™è¯¯ä½ç½®
   - ç¡¬ç¼–ç W39è€Œä¸æ˜¯åŠ¨æ€è·å–å‘¨æ•°
   - UIæ— æ³•æ‰¾åˆ°2025_W39ç›®å½•ä¸­çš„æ–‡ä»¶

2. **æ–‡ä»¶åŒ¹é…é€»è¾‘ç¼ºé™·** âš ï¸
   - ä½¿ç”¨åŒ…å«åˆ¤æ–­è€Œéå®Œå…¨åŒ¹é…
   - å¯èƒ½åŒ¹é…åˆ°é”™è¯¯çš„åŸºçº¿æ–‡ä»¶
   - æ²¡æœ‰æ’é™¤è½¯åˆ é™¤æ–‡ä»¶

3. **URLç®¡ç†é—®é¢˜** âš ï¸
   - åŸºçº¿æ–‡ä»¶ä¸å­˜å‚¨æºURL
   - excel_urlså­˜å‚¨çš„æ˜¯ä¸Šä¼ åçš„URL
   - ä¾èµ–è¡¨æ ¼åç§°è¿›è¡Œæ˜ å°„

4. **è½¯åˆ é™¤æ–‡ä»¶é—®é¢˜** âœ…
   - è½¯åˆ é™¤æ–‡ä»¶ä¸ä¼šè¢«find_baseline_filesè°ƒç”¨
   - ä½†å ç”¨å­˜å‚¨ç©ºé—´ä¸”é€ æˆæ··æ·†

ç»“è®ºï¼šç³»ç»Ÿç¡®å®åœ¨å¤„ç†çœŸå®æ•°æ®ï¼Œä½†å­˜åœ¨ä¸¥é‡çš„æ¶æ„å’Œé€»è¾‘é—®é¢˜ï¼
""")

if __name__ == "__main__":
    main()