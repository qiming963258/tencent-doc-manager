#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ€§èƒ½æµ‹è¯•å·¥å…· - æµ‹è¯•ä¼˜åŒ–åçš„ä¸Šä¼ ä¸‹è½½æ€§èƒ½
å¯¹æ¯”åŸç‰ˆæœ¬ä¸ä¼˜åŒ–ç‰ˆæœ¬çš„æ€§èƒ½å·®å¼‚
"""

import asyncio
import time
import argparse
import logging
import json
from pathlib import Path
from typing import List, Dict, Tuple
from dataclasses import dataclass, asdict
from optimized_download import OptimizedTencentDownloader
from optimized_upload import OptimizedTencentUploader
from performance_monitor import get_performance_monitor, cleanup_performance_monitor


@dataclass
class TestResult:
    """æµ‹è¯•ç»“æœ"""
    test_name: str
    total_time: float
    success_count: int
    failed_count: int
    success_rate: float
    avg_duration_per_task: float
    throughput: float  # ä»»åŠ¡/ç§’
    peak_memory_mb: float
    peak_cpu_percent: float


class PerformanceTestSuite:
    """æ€§èƒ½æµ‹è¯•å¥—ä»¶"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.results: List[TestResult] = []
        
    async def test_download_performance(self, urls: List[str], cookies: str, 
                                      concurrent_levels: List[int] = [1, 2, 3]) -> List[TestResult]:
        """æµ‹è¯•ä¸‹è½½æ€§èƒ½"""
        results = []
        
        for concurrent in concurrent_levels:
            print(f"\n=== æµ‹è¯•ä¸‹è½½æ€§èƒ½ (å¹¶å‘æ•°: {concurrent}) ===")
            
            # å¯åŠ¨æ€§èƒ½ç›‘æ§
            monitor = await get_performance_monitor()
            
            start_time = time.time()
            
            # åˆ›å»ºä¸‹è½½å™¨
            downloader = OptimizedTencentDownloader(max_concurrent=concurrent)
            
            # æ·»åŠ ä»»åŠ¡
            task_ids = []
            for i, url in enumerate(urls):
                task_id = await downloader.add_download_task(url, cookies, priority=i)
                task_ids.append(task_id)
            
            # å¯åŠ¨å¤„ç†
            workers = await downloader.start_processing()
            
            try:
                # ç­‰å¾…å®Œæˆ
                await downloader.wait_for_completion(timeout=300)
                
                # è·å–ç»“æœ
                all_results = await downloader.get_all_results()
                stats = await downloader.get_stats()
                
                total_time = time.time() - start_time
                success_count = sum(1 for r in all_results.values() if r.success)
                failed_count = len(all_results) - success_count
                
                # è·å–æ€§èƒ½æŒ‡æ ‡
                perf_summary = monitor.get_metrics_summary(int(total_time/60) + 1)
                
                result = TestResult(
                    test_name=f"Download_Concurrent_{concurrent}",
                    total_time=total_time,
                    success_count=success_count,
                    failed_count=failed_count,
                    success_rate=success_count / len(urls) * 100,
                    avg_duration_per_task=total_time / len(urls),
                    throughput=len(urls) / total_time,
                    peak_memory_mb=perf_summary.get('memory', {}).get('max_mb', 0),
                    peak_cpu_percent=perf_summary.get('cpu', {}).get('max', 0)
                )
                
                results.append(result)
                self.results.append(result)
                
                print(f"æ€»æ—¶é—´: {total_time:.2f}s")
                print(f"æˆåŠŸç‡: {result.success_rate:.1f}%")
                print(f"ååé‡: {result.throughput:.2f} æ–‡æ¡£/ç§’")
                print(f"å³°å€¼å†…å­˜: {result.peak_memory_mb:.0f}MB")
                print(f"å³°å€¼CPU: {result.peak_cpu_percent:.1f}%")
                
            finally:
                # æ¸…ç†å·¥ä½œè¿›ç¨‹
                for worker in workers:
                    worker.cancel()
                
                await asyncio.sleep(2)  # ç­‰å¾…æ¸…ç†å®Œæˆ
        
        return results
    
    async def test_upload_performance(self, files: List[str], cookies: str,
                                    concurrent_levels: List[int] = [1, 2]) -> List[TestResult]:
        """æµ‹è¯•ä¸Šä¼ æ€§èƒ½"""
        results = []
        
        for concurrent in concurrent_levels:
            print(f"\n=== æµ‹è¯•ä¸Šä¼ æ€§èƒ½ (å¹¶å‘æ•°: {concurrent}) ===")
            
            # å¯åŠ¨æ€§èƒ½ç›‘æ§
            monitor = await get_performance_monitor()
            
            start_time = time.time()
            
            # åˆ›å»ºä¸Šä¼ å™¨
            uploader = OptimizedTencentUploader(max_concurrent=concurrent)
            
            # æ·»åŠ ä»»åŠ¡
            task_ids = []
            for i, file_path in enumerate(files):
                if Path(file_path).exists():
                    task_id = await uploader.add_upload_task(file_path, cookies, priority=i)
                    task_ids.append(task_id)
                else:
                    print(f"è­¦å‘Š: æ–‡ä»¶ä¸å­˜åœ¨ {file_path}")
            
            if not task_ids:
                print("æ²¡æœ‰æœ‰æ•ˆçš„ä¸Šä¼ æ–‡ä»¶")
                continue
            
            # å¯åŠ¨å¤„ç†
            workers = await uploader.start_processing()
            
            try:
                # ç­‰å¾…å®Œæˆ
                await uploader.wait_for_completion(timeout=600)
                
                # è·å–ç»“æœ
                all_results = await uploader.get_all_results()
                stats = await uploader.get_stats()
                
                total_time = time.time() - start_time
                success_count = sum(1 for r in all_results.values() if r.success)
                failed_count = len(all_results) - success_count
                
                # è·å–æ€§èƒ½æŒ‡æ ‡
                perf_summary = monitor.get_metrics_summary(int(total_time/60) + 1)
                
                result = TestResult(
                    test_name=f"Upload_Concurrent_{concurrent}",
                    total_time=total_time,
                    success_count=success_count,
                    failed_count=failed_count,
                    success_rate=success_count / len(task_ids) * 100,
                    avg_duration_per_task=total_time / len(task_ids),
                    throughput=len(task_ids) / total_time,
                    peak_memory_mb=perf_summary.get('memory', {}).get('max_mb', 0),
                    peak_cpu_percent=perf_summary.get('cpu', {}).get('max', 0)
                )
                
                results.append(result)
                self.results.append(result)
                
                print(f"æ€»æ—¶é—´: {total_time:.2f}s")
                print(f"æˆåŠŸç‡: {result.success_rate:.1f}%")
                print(f"ååé‡: {result.throughput:.2f} æ–‡ä»¶/ç§’")
                print(f"å³°å€¼å†…å­˜: {result.peak_memory_mb:.0f}MB")
                print(f"å³°å€¼CPU: {result.peak_cpu_percent:.1f}%")
                
            finally:
                # æ¸…ç†å·¥ä½œè¿›ç¨‹
                for worker in workers:
                    worker.cancel()
                
                await asyncio.sleep(2)  # ç­‰å¾…æ¸…ç†å®Œæˆ
        
        return results
    
    async def test_mixed_workload(self, urls: List[str], files: List[str], cookies: str) -> TestResult:
        """æµ‹è¯•æ··åˆè´Ÿè½½æ€§èƒ½"""
        print(f"\n=== æµ‹è¯•æ··åˆè´Ÿè½½æ€§èƒ½ ===")
        
        monitor = await get_performance_monitor()
        start_time = time.time()
        
        # åŒæ—¶å¯åŠ¨ä¸‹è½½å’Œä¸Šä¼ ä»»åŠ¡
        download_task = asyncio.create_task(
            self._run_download_batch(urls[:3], cookies, 2)
        )
        
        valid_files = [f for f in files if Path(f).exists()]
        upload_task = asyncio.create_task(
            self._run_upload_batch(valid_files[:2], cookies, 1)
        )
        
        try:
            download_results, upload_results = await asyncio.gather(
                download_task, upload_task, return_exceptions=True
            )
            
            total_time = time.time() - start_time
            
            # è®¡ç®—ç»¼åˆç»“æœ
            total_tasks = len(urls[:3]) + len(valid_files[:2])
            download_success = len([r for r in download_results.values() if r.success]) if isinstance(download_results, dict) else 0
            upload_success = len([r for r in upload_results.values() if r.success]) if isinstance(upload_results, dict) else 0
            
            total_success = download_success + upload_success
            
            # è·å–æ€§èƒ½æŒ‡æ ‡
            perf_summary = monitor.get_metrics_summary(int(total_time/60) + 1)
            
            result = TestResult(
                test_name="Mixed_Workload",
                total_time=total_time,
                success_count=total_success,
                failed_count=total_tasks - total_success,
                success_rate=total_success / total_tasks * 100 if total_tasks > 0 else 0,
                avg_duration_per_task=total_time / total_tasks if total_tasks > 0 else 0,
                throughput=total_tasks / total_time if total_time > 0 else 0,
                peak_memory_mb=perf_summary.get('memory', {}).get('max_mb', 0),
                peak_cpu_percent=perf_summary.get('cpu', {}).get('max', 0)
            )
            
            self.results.append(result)
            
            print(f"æ€»æ—¶é—´: {total_time:.2f}s")
            print(f"æˆåŠŸç‡: {result.success_rate:.1f}%")
            print(f"ååé‡: {result.throughput:.2f} ä»»åŠ¡/ç§’")
            print(f"å³°å€¼å†…å­˜: {result.peak_memory_mb:.0f}MB")
            print(f"å³°å€¼CPU: {result.peak_cpu_percent:.1f}%")
            
            return result
            
        except Exception as e:
            print(f"æ··åˆè´Ÿè½½æµ‹è¯•å¼‚å¸¸: {e}")
            return None
    
    async def _run_download_batch(self, urls: List[str], cookies: str, concurrent: int):
        """è¿è¡Œä¸‹è½½æ‰¹æ¬¡"""
        downloader = OptimizedTencentDownloader(max_concurrent=concurrent)
        
        task_ids = []
        for url in urls:
            task_id = await downloader.add_download_task(url, cookies)
            task_ids.append(task_id)
        
        workers = await downloader.start_processing()
        
        try:
            await downloader.wait_for_completion()
            return await downloader.get_all_results()
        finally:
            for worker in workers:
                worker.cancel()
    
    async def _run_upload_batch(self, files: List[str], cookies: str, concurrent: int):
        """è¿è¡Œä¸Šä¼ æ‰¹æ¬¡"""
        uploader = OptimizedTencentUploader(max_concurrent=concurrent)
        
        task_ids = []
        for file_path in files:
            task_id = await uploader.add_upload_task(file_path, cookies)
            task_ids.append(task_id)
        
        workers = await uploader.start_processing()
        
        try:
            await uploader.wait_for_completion()
            return await uploader.get_all_results()
        finally:
            for worker in workers:
                worker.cancel()
    
    def generate_report(self, output_file: str = None):
        """ç”Ÿæˆæ€§èƒ½æµ‹è¯•æŠ¥å‘Š"""
        if not self.results:
            print("æ²¡æœ‰æµ‹è¯•ç»“æœ")
            return
        
        report = {
            "test_time": time.strftime("%Y-%m-%d %H:%M:%S"),
            "total_tests": len(self.results),
            "summary": self._generate_summary(),
            "detailed_results": [asdict(result) for result in self.results],
            "recommendations": self._generate_recommendations()
        }
        
        if output_file:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=2, ensure_ascii=False)
            print(f"\næ€§èƒ½æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_file}")
        
        # æ‰“å°æ‘˜è¦
        print("\n" + "="*60)
        print("æ€§èƒ½æµ‹è¯•æŠ¥å‘Šæ‘˜è¦")
        print("="*60)
        
        for category, data in report["summary"].items():
            print(f"\n{category}:")
            for key, value in data.items():
                if isinstance(value, float):
                    print(f"  {key}: {value:.2f}")
                else:
                    print(f"  {key}: {value}")
        
        print("\næ¨èä¼˜åŒ–æªæ–½:")
        for rec in report["recommendations"]:
            print(f"  - {rec}")
        
        return report
    
    def _generate_summary(self) -> Dict:
        """ç”Ÿæˆæµ‹è¯•æ‘˜è¦"""
        if not self.results:
            return {}
        
        download_results = [r for r in self.results if "Download" in r.test_name]
        upload_results = [r for r in self.results if "Upload" in r.test_name]
        mixed_results = [r for r in self.results if "Mixed" in r.test_name]
        
        summary = {}
        
        if download_results:
            summary["ä¸‹è½½æ€§èƒ½"] = {
                "æœ€é«˜ååé‡": max(r.throughput for r in download_results),
                "å¹³å‡æˆåŠŸç‡": sum(r.success_rate for r in download_results) / len(download_results),
                "æœ€ä½å†…å­˜å³°å€¼": min(r.peak_memory_mb for r in download_results),
                "æœ€é«˜å†…å­˜å³°å€¼": max(r.peak_memory_mb for r in download_results)
            }
        
        if upload_results:
            summary["ä¸Šä¼ æ€§èƒ½"] = {
                "æœ€é«˜ååé‡": max(r.throughput for r in upload_results),
                "å¹³å‡æˆåŠŸç‡": sum(r.success_rate for r in upload_results) / len(upload_results),
                "æœ€ä½å†…å­˜å³°å€¼": min(r.peak_memory_mb for r in upload_results),
                "æœ€é«˜å†…å­˜å³°å€¼": max(r.peak_memory_mb for r in upload_results)
            }
        
        if mixed_results:
            summary["æ··åˆè´Ÿè½½"] = {
                "ååé‡": mixed_results[0].throughput,
                "æˆåŠŸç‡": mixed_results[0].success_rate,
                "å†…å­˜å³°å€¼": mixed_results[0].peak_memory_mb
            }
        
        return summary
    
    def _generate_recommendations(self) -> List[str]:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        recommendations = []
        
        if not self.results:
            return recommendations
        
        # åˆ†æå†…å­˜ä½¿ç”¨
        max_memory = max(r.peak_memory_mb for r in self.results)
        if max_memory > 1600:  # è¶…è¿‡1.6GB
            recommendations.append(f"å†…å­˜å³°å€¼è¾¾åˆ°{max_memory:.0f}MBï¼Œå»ºè®®ä¼˜åŒ–å†…å­˜ä½¿ç”¨æˆ–å¢åŠ æœåŠ¡å™¨å†…å­˜")
        elif max_memory < 800:  # å°äº800MB
            recommendations.append("å†…å­˜ä½¿ç”¨æ•ˆç‡è‰¯å¥½ï¼Œå¯ä»¥è€ƒè™‘å¢åŠ å¹¶å‘æ•°ä»¥æé«˜ååé‡")
        
        # åˆ†æCPUä½¿ç”¨
        max_cpu = max(r.peak_cpu_percent for r in self.results)
        if max_cpu > 90:
            recommendations.append(f"CPUå³°å€¼è¾¾åˆ°{max_cpu:.1f}%ï¼Œå»ºè®®å‡å°‘å¹¶å‘æ•°æˆ–ä¼˜åŒ–CPUä½¿ç”¨")
        elif max_cpu < 60:
            recommendations.append("CPUä½¿ç”¨ç‡è¾ƒä½ï¼Œå¯ä»¥è€ƒè™‘å¢åŠ å¹¶å‘æ•°")
        
        # åˆ†ææˆåŠŸç‡
        avg_success_rate = sum(r.success_rate for r in self.results) / len(self.results)
        if avg_success_rate < 95:
            recommendations.append(f"å¹³å‡æˆåŠŸç‡{avg_success_rate:.1f}%ï¼Œå»ºè®®æ£€æŸ¥ç½‘ç»œè¿æ¥å’Œé”™è¯¯å¤„ç†")
        
        # åˆ†æååé‡
        download_results = [r for r in self.results if "Download" in r.test_name]
        if download_results:
            best_concurrent = max(download_results, key=lambda x: x.throughput)
            recommendations.append(f"ä¸‹è½½æœ€ä¼˜å¹¶å‘æ•°ä¸º{best_concurrent.test_name.split('_')[-1]}ï¼Œååé‡{best_concurrent.throughput:.2f}æ–‡æ¡£/ç§’")
        
        return recommendations


async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    parser = argparse.ArgumentParser(description='è…¾è®¯æ–‡æ¡£æ€§èƒ½æµ‹è¯•å·¥å…·')
    parser.add_argument('-c', '--cookies', required=True, help='ç™»å½•Cookie')
    parser.add_argument('--urls', nargs='+', help='æµ‹è¯•ä¸‹è½½çš„URLåˆ—è¡¨')
    parser.add_argument('--files', nargs='+', help='æµ‹è¯•ä¸Šä¼ çš„æ–‡ä»¶åˆ—è¡¨')
    parser.add_argument('--download-only', action='store_true', help='ä»…æµ‹è¯•ä¸‹è½½')
    parser.add_argument('--upload-only', action='store_true', help='ä»…æµ‹è¯•ä¸Šä¼ ')
    parser.add_argument('--mixed', action='store_true', help='æµ‹è¯•æ··åˆè´Ÿè½½')
    parser.add_argument('--output', default='performance_test_report.json', help='æŠ¥å‘Šè¾“å‡ºæ–‡ä»¶')
    parser.add_argument('--verbose', action='store_true', help='è¯¦ç»†æ—¥å¿—')
    
    args = parser.parse_args()
    
    # é…ç½®æ—¥å¿—
    level = logging.DEBUG if args.verbose else logging.INFO
    logging.basicConfig(level=level, format='%(asctime)s - %(levelname)s - %(message)s')
    
    # é»˜è®¤æµ‹è¯•æ•°æ®
    test_urls = args.urls or [
        "https://docs.qq.com/sheet/DYkNJRlp0cWRWZUlH?tab=BB08J2",
        "https://docs.qq.com/sheet/DYkNJRlp0cWRWZUlH?tab=g2hi7f",
        "https://docs.qq.com/sheet/DYkNJRlp0cWRWZUlH?tab=kgc5ou"
    ]
    
    test_files = args.files or [
        "å‰¯æœ¬-å‡ºå›½é”€å”®çŸ¥è¯†æŠ€èƒ½.xlsx",
        "æµ‹è¯•ç‰ˆæœ¬-å°çº¢ä¹¦éƒ¨é—¨.xlsx"
    ]
    
    test_suite = PerformanceTestSuite()
    
    try:
        print("ğŸš€ å¼€å§‹æ€§èƒ½æµ‹è¯•...")
        print(f"æµ‹è¯•é…ç½®: 4H2GæœåŠ¡å™¨ç¯å¢ƒæ¨¡æ‹Ÿ")
        print(f"Cookie: {'å·²è®¾ç½®' if args.cookies else 'æœªè®¾ç½®'}")
        
        # æ‰§è¡Œæµ‹è¯•
        if not args.upload_only and not args.mixed:
            await test_suite.test_download_performance(test_urls, args.cookies)
        
        if not args.download_only and not args.mixed:
            await test_suite.test_upload_performance(test_files, args.cookies)
        
        if args.mixed or (not args.download_only and not args.upload_only):
            await test_suite.test_mixed_workload(test_urls, test_files, args.cookies)
        
        # ç”ŸæˆæŠ¥å‘Š
        test_suite.generate_report(args.output)
        
    finally:
        # æ¸…ç†ç›‘æ§å™¨
        await cleanup_performance_monitor()


if __name__ == "__main__":
    asyncio.run(main())