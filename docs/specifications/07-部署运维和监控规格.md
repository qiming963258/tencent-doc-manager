# 部署运维和监控规格书

## 1. 部署架构设计

### 1.1 基于4H2G Ubuntu服务器的完整部署方案

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                    生产环境部署架构 (4H2G Ubuntu)                           │
├─────────────────────────────────────────────────────────────────────────────┤
│  外部访问层                                                                 │
│  ┌─────────────────┐  ┌─────────────────┐  ┌─────────────────┐              │
│  │ 域名解析 DNS     │  │ SSL证书 (Let's) │  │ CDN加速 (可选)   │              │
│  │ docs.company.com│  │ HTTPS强制       │  │ 静态资源缓存     │              │
│  └─────────────────┘  └─────────────────┘  └─────────────────┘              │
├─────────────────────────────────────────────────────────────────────────────┤
│  负载均衡和反向代理层                                                        │
│  ┌─────────────────────────────────────────────────────────────────────────┐ │
│  │ Nginx (反向代理 + 负载均衡)                                               │ │
│  │ ┌─────────────────┐  ┌─────────────────┐  ┌─────────────────┐          │ │
│  │ │ HTTP/HTTPS处理   │  │ 静态文件服务     │  │ API路由转发      │          │ │
│  │ │ - 端口: 80/443   │  │ - React构建文件  │  │ - /api/* → Flask │          │ │
│  │ │ - SSL终止        │  │ - 图片/CSS/JS   │  │ - 健康检查       │          │ │
│  │ └─────────────────┘  └─────────────────┘  └─────────────────┘          │ │
│  └─────────────────────────────────────────────────────────────────────────┘ │
├─────────────────────────────────────────────────────────────────────────────┤
│  应用服务层                                                                 │
│  ┌─────────────────────────────────────────────────────────────────────────┐ │
│  │ Docker容器编排                                                           │ │
│  │ ┌─────────────────┐  ┌─────────────────┐  ┌─────────────────┐          │ │
│  │ │ Flask应用容器    │  │ React前端容器    │  │ Worker任务容器   │          │ │
│  │ │ - 端口: 5000     │  │ - Nginx静态服务  │  │ - 批量处理       │          │ │
│  │ │ - 4个工作进程    │  │ - 生产构建       │  │ - Celery队列     │          │ │
│  │ │ - 健康检查       │  │ - 压缩和缓存     │  │ - 定时任务       │          │ │
│  │ └─────────────────┘  └─────────────────┘  └─────────────────┘          │ │
│  └─────────────────────────────────────────────────────────────────────────┘ │
├─────────────────────────────────────────────────────────────────────────────┤
│  数据存储层                                                                 │
│  ┌─────────────────┐  ┌─────────────────┐  ┌─────────────────┐              │
│  │ SQLite数据库     │  │ Redis缓存       │  │ 文件存储         │              │
│  │ - WAL模式        │  │ - 6GB内存配置    │  │ - /data/files    │              │
│  │ - 自动备份       │  │ - 持久化存储     │  │ - 定期清理       │              │
│  │ - 读写分离       │  │ - 集群模式准备   │  │ - 权限控制       │              │
│  └─────────────────┘  └─────────────────┘  └─────────────────┘              │
├─────────────────────────────────────────────────────────────────────────────┤
│  监控和日志层                                                               │
│  ┌─────────────────┐  ┌─────────────────┐  ┌─────────────────┐              │
│  │ 系统监控         │  │ 应用监控         │  │ 日志管理         │              │
│  │ - CPU/内存       │  │ - API响应时间    │  │ - 结构化日志     │              │
│  │ - 磁盘空间       │  │ - 错误率统计     │  │ - 日志轮转       │              │
│  │ - 网络连接       │  │ - 用户活动       │  │ - ELK集成准备    │              │
│  └─────────────────┘  └─────────────────┘  └─────────────────┘              │
└─────────────────────────────────────────────────────────────────────────────┘
```

### 1.2 硬件资源配置优化

```yaml
# 基于4H2G Ubuntu服务器的资源分配
server_specifications:
  cpu: "4核心 (2.4GHz+)"
  memory: "2GB DDR4"
  storage: "40GB SSD"
  network: "5Mbps带宽"
  os: "Ubuntu 20.04 LTS"

resource_allocation:
  system_reserved: "20%"  # 系统保留资源
  applications:
    flask_app:
      cpu_limit: "2核心"
      memory_limit: "1GB"
      processes: 4
      threads_per_process: 2
    
    redis_cache:
      memory_allocation: "256MB"
      max_memory_policy: "allkeys-lru"
      persistence: "rdb + aof"
    
    nginx_proxy:
      worker_processes: "2"
      worker_connections: "1024"
      client_max_body_size: "100MB"
    
    file_storage:
      max_storage: "10GB"
      cleanup_threshold: "80%"
      retention_days: 30

performance_targets:
  concurrent_users: "50+"
  api_response_time: "< 2秒"
  file_upload_time: "< 30秒/10MB"
  system_uptime: "> 99.5%"
```

## 2. Docker容器化部署

### 2.1 完整的Docker Compose配置

```yaml
# docker-compose.yml - 生产环境配置
version: '3.8'

services:
  # Flask后端应用
  app:
    build:
      context: .
      dockerfile: Dockerfile.prod
    container_name: tencent-doc-app
    restart: unless-stopped
    ports:
      - "5000:5000"
    environment:
      - FLASK_ENV=production
      - DATABASE_URL=sqlite:////data/tencent_docs.db
      - REDIS_URL=redis://redis:6379/0
      - SECRET_KEY=${SECRET_KEY}
      - CLAUDE_API_KEY=${CLAUDE_API_KEY}
      - LOG_LEVEL=INFO
    volumes:
      - ./data:/data
      - ./uploads:/app/uploads
      - ./downloads:/app/downloads
      - ./logs:/app/logs
    depends_on:
      - redis
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 1G
        reservations:
          cpus: '1.0'
          memory: 512M
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # Redis缓存服务
  redis:
    image: redis:6.2-alpine
    container_name: tencent-doc-redis
    restart: unless-stopped
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
      - ./redis.conf:/usr/local/etc/redis/redis.conf
    command: redis-server /usr/local/etc/redis/redis.conf
    deploy:
      resources:
        limits:
          memory: 256M
        reservations:
          memory: 128M
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 3s
      retries: 3

  # Nginx反向代理
  nginx:
    image: nginx:1.21-alpine
    container_name: tencent-doc-nginx
    restart: unless-stopped
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/conf.d:/etc/nginx/conf.d:ro
      - ./ssl:/etc/ssl/certs:ro
      - ./static:/var/www/static:ro
    depends_on:
      - app
    deploy:
      resources:
        limits:
          memory: 128M
        reservations:
          memory: 64M
    healthcheck:
      test: ["CMD", "nginx", "-t"]
      interval: 30s
      timeout: 3s
      retries: 3

  # 任务队列Worker (可选)
  worker:
    build:
      context: .
      dockerfile: Dockerfile.worker
    container_name: tencent-doc-worker
    restart: unless-stopped
    environment:
      - CELERY_BROKER=redis://redis:6379/1
      - DATABASE_URL=sqlite:////data/tencent_docs.db
    volumes:
      - ./data:/data
      - ./uploads:/app/uploads
      - ./downloads:/app/downloads
    depends_on:
      - redis
      - app
    deploy:
      resources:
        limits:
          memory: 512M
        reservations:
          memory: 256M

volumes:
  redis_data:
    driver: local

networks:
  default:
    name: tencent-doc-network
    driver: bridge
```

### 2.2 生产环境Dockerfile

```dockerfile
# Dockerfile.prod - Flask应用生产版本
FROM python:3.9-slim

# 设置工作目录
WORKDIR /app

# 安装系统依赖
RUN apt-get update && apt-get install -y \
    curl \
    gnupg \
    wget \
    unzip \
    && rm -rf /var/lib/apt/lists/*

# 安装Playwright浏览器
RUN pip install playwright==1.40.0
RUN playwright install chromium
RUN playwright install-deps chromium

# 复制依赖文件
COPY requirements.prod.txt .
RUN pip install --no-cache-dir -r requirements.prod.txt

# 复制应用代码
COPY . .

# 创建非root用户
RUN useradd --create-home --shell /bin/bash app \
    && chown -R app:app /app
USER app

# 暴露端口
EXPOSE 5000

# 健康检查
HEALTHCHECK --interval=30s --timeout=30s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:5000/api/health || exit 1

# 启动命令
CMD ["gunicorn", "--bind", "0.0.0.0:5000", "--workers", "4", "--threads", "2", "--timeout", "60", "--keepalive", "2", "--max-requests", "1000", "--max-requests-jitter", "100", "app:app"]
```

### 2.3 Nginx配置优化

```nginx
# nginx/nginx.conf - 生产环境配置
user nginx;
worker_processes 2;  # 基于2核CPU
error_log /var/log/nginx/error.log warn;
pid /var/run/nginx.pid;

events {
    worker_connections 1024;
    use epoll;
    multi_accept on;
}

http {
    include /etc/nginx/mime.types;
    default_type application/octet-stream;
    
    # 日志格式
    log_format main '$remote_addr - $remote_user [$time_local] "$request" '
                   '$status $body_bytes_sent "$http_referer" '
                   '"$http_user_agent" "$http_x_forwarded_for" '
                   '$request_time $upstream_response_time';
    
    access_log /var/log/nginx/access.log main;
    
    # 性能优化
    sendfile on;
    tcp_nopush on;
    tcp_nodelay on;
    keepalive_timeout 65;
    types_hash_max_size 2048;
    client_max_body_size 100M;
    
    # Gzip压缩
    gzip on;
    gzip_vary on;
    gzip_min_length 10240;
    gzip_proxied expired no-cache no-store private must-revalidate auth;
    gzip_types
        text/plain
        text/css
        text/xml
        text/javascript
        application/javascript
        application/xml+rss
        application/json;
    
    # 上游服务器配置
    upstream flask_app {
        server app:5000 weight=1 max_fails=3 fail_timeout=30s;
        # 可以添加多个实例进行负载均衡
        keepalive 32;
    }
    
    # 速率限制
    limit_req_zone $binary_remote_addr zone=api:10m rate=10r/s;
    limit_req_zone $binary_remote_addr zone=upload:10m rate=2r/s;
    
    # 主服务器配置
    server {
        listen 80;
        server_name _;
        
        # HTTP重定向到HTTPS
        return 301 https://$server_name$request_uri;
    }
    
    server {
        listen 443 ssl http2;
        server_name docs.company.com;
        
        # SSL配置
        ssl_certificate /etc/ssl/certs/fullchain.pem;
        ssl_certificate_key /etc/ssl/certs/privkey.pem;
        ssl_protocols TLSv1.2 TLSv1.3;
        ssl_ciphers ECDHE-RSA-AES128-GCM-SHA256:ECDHE-RSA-AES256-GCM-SHA384;
        ssl_prefer_server_ciphers off;
        ssl_session_timeout 1d;
        ssl_session_cache shared:MozTLS:10m;
        ssl_session_tickets off;
        
        # 安全标头
        add_header Strict-Transport-Security "max-age=63072000" always;
        add_header X-Frame-Options DENY;
        add_header X-Content-Type-Options nosniff;
        add_header X-XSS-Protection "1; mode=block";
        
        # 静态文件服务
        location /static/ {
            alias /var/www/static/;
            expires 1y;
            add_header Cache-Control "public, immutable";
        }
        
        # API请求代理
        location /api/ {
            limit_req zone=api burst=20 nodelay;
            
            proxy_pass http://flask_app;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
            
            proxy_connect_timeout 30s;
            proxy_send_timeout 60s;
            proxy_read_timeout 60s;
            
            proxy_buffer_size 4k;
            proxy_buffers 8 4k;
            proxy_busy_buffers_size 8k;
        }
        
        # 文件上传特殊处理
        location /api/upload {
            limit_req zone=upload burst=5 nodelay;
            
            client_max_body_size 100M;
            proxy_request_buffering off;
            
            proxy_pass http://flask_app;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
            
            proxy_connect_timeout 30s;
            proxy_send_timeout 300s;  # 上传超时5分钟
            proxy_read_timeout 300s;
        }
        
        # 健康检查
        location /health {
            proxy_pass http://flask_app/api/health;
            access_log off;
        }
        
        # 默认路由
        location / {
            proxy_pass http://flask_app;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
        }
    }
}
```

## 3. 自动化部署脚本

### 3.1 完整部署脚本

```bash
#!/bin/bash
# deploy.sh - 一键部署脚本

set -e  # 遇到错误立即退出

# 配置变量
APP_NAME="tencent-doc-manager"
APP_DIR="/opt/${APP_NAME}"
BACKUP_DIR="/opt/backups/${APP_NAME}"
LOG_FILE="/var/log/deploy-${APP_NAME}.log"

# 日志函数
log() {
    echo "$(date '+%Y-%m-%d %H:%M:%S') - $1" | tee -a "$LOG_FILE"
}

log "开始部署 ${APP_NAME}..."

# 1. 系统环境检查
log "检查系统环境..."
if ! command -v docker &> /dev/null; then
    log "安装Docker..."
    curl -fsSL https://get.docker.com -o get-docker.sh
    sudo sh get-docker.sh
    sudo usermod -aG docker $USER
fi

if ! command -v docker-compose &> /dev/null; then
    log "安装Docker Compose..."
    sudo curl -L "https://github.com/docker/compose/releases/download/1.29.2/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose
    sudo chmod +x /usr/local/bin/docker-compose
fi

# 2. 创建应用目录
log "创建应用目录..."
sudo mkdir -p "$APP_DIR" "$BACKUP_DIR"
sudo chown -R $USER:$USER "$APP_DIR" "$BACKUP_DIR"

# 3. 备份现有数据（如果存在）
if [ -d "$APP_DIR/data" ]; then
    log "备份现有数据..."
    BACKUP_NAME="backup-$(date +%Y%m%d-%H%M%S)"
    tar -czf "$BACKUP_DIR/$BACKUP_NAME.tar.gz" -C "$APP_DIR" data logs
    log "数据已备份到: $BACKUP_DIR/$BACKUP_NAME.tar.gz"
fi

# 4. 部署新版本
log "部署新版本..."
cd "$APP_DIR"

# 拉取最新代码（假设使用Git）
if [ -d ".git" ]; then
    git pull origin main
else
    git clone https://github.com/company/tencent-doc-manager.git .
fi

# 5. 环境变量配置
log "配置环境变量..."
if [ ! -f ".env.prod" ]; then
    cat > .env.prod << EOF
SECRET_KEY=$(openssl rand -hex 32)
CLAUDE_API_KEY=your_claude_api_key_here
FLASK_ENV=production
DATABASE_URL=sqlite:////data/tencent_docs.db
REDIS_URL=redis://redis:6379/0
LOG_LEVEL=INFO
EOF
    log "请编辑 .env.prod 文件，设置正确的API密钥"
fi

# 6. SSL证书设置
log "检查SSL证书..."
mkdir -p ssl
if [ ! -f "ssl/fullchain.pem" ]; then
    log "需要配置SSL证书，当前使用自签名证书用于测试"
    openssl req -x509 -nodes -days 365 -newkey rsa:2048 \
        -keyout ssl/privkey.pem -out ssl/fullchain.pem \
        -subj "/C=CN/ST=Beijing/L=Beijing/O=Company/CN=docs.company.com"
fi

# 7. Docker构建和部署
log "构建Docker镜像..."
docker-compose -f docker-compose.yml build --no-cache

log "停止旧容器..."
docker-compose -f docker-compose.yml down

log "启动新容器..."
docker-compose -f docker-compose.yml up -d

# 8. 健康检查
log "等待服务启动..."
sleep 30

log "检查服务健康状态..."
for i in {1..10}; do
    if curl -f http://localhost/api/health > /dev/null 2>&1; then
        log "服务启动成功！"
        break
    fi
    log "等待服务启动... ($i/10)"
    sleep 10
done

# 9. 部署后清理
log "清理旧镜像..."
docker image prune -f

# 10. 设置日志轮转
log "配置日志轮转..."
sudo tee /etc/logrotate.d/tencent-doc-manager > /dev/null << EOF
/opt/tencent-doc-manager/logs/*.log {
    daily
    rotate 30
    compress
    delaycompress
    missingok
    notifempty
    copytruncate
}
EOF

log "部署完成！"
log "应用访问地址: https://$(hostname -I | awk '{print $1}')"
log "健康检查: curl https://$(hostname -I | awk '{print $1}')/api/health"
```

### 3.2 监控和维护脚本

```bash
#!/bin/bash
# monitor.sh - 系统监控脚本

APP_NAME="tencent-doc-manager"
LOG_FILE="/var/log/monitor-${APP_NAME}.log"
ALERT_EMAIL="admin@company.com"

# 监控函数
check_service_health() {
    local service_name=$1
    local health_url=$2
    
    if curl -f "$health_url" > /dev/null 2>&1; then
        echo "✅ $service_name 健康状态正常"
        return 0
    else
        echo "❌ $service_name 健康检查失败"
        return 1
    fi
}

check_resource_usage() {
    # CPU使用率
    cpu_usage=$(top -bn1 | grep "Cpu(s)" | awk '{print $2}' | cut -d'%' -f1)
    echo "📊 CPU使用率: ${cpu_usage}%"
    
    # 内存使用率  
    memory_usage=$(free | grep Mem | awk '{printf("%.1f", $3/$2 * 100.0)}')
    echo "📊 内存使用率: ${memory_usage}%"
    
    # 磁盘使用率
    disk_usage=$(df -h | grep '/$' | awk '{print $5}' | cut -d'%' -f1)
    echo "📊 磁盘使用率: ${disk_usage}%"
    
    # 检查阈值告警
    if (( $(echo "$cpu_usage > 80" | bc -l) )); then
        echo "⚠️ CPU使用率过高: ${cpu_usage}%" >> "$LOG_FILE"
    fi
    
    if (( $(echo "$memory_usage > 85" | bc -l) )); then
        echo "⚠️ 内存使用率过高: ${memory_usage}%" >> "$LOG_FILE"
    fi
    
    if [ "$disk_usage" -gt 90 ]; then
        echo "⚠️ 磁盘空间不足: ${disk_usage}%" >> "$LOG_FILE"
    fi
}

check_docker_containers() {
    echo "🐳 Docker容器状态:"
    docker-compose ps
    
    # 检查容器健康状态
    unhealthy_containers=$(docker ps --filter "health=unhealthy" --format "table {{.Names}}")
    if [ -n "$unhealthy_containers" ]; then
        echo "❌ 不健康的容器: $unhealthy_containers" >> "$LOG_FILE"
    fi
}

# 执行监控检查
echo "=== $(date) - 系统监控报告 ==="
check_service_health "应用服务" "http://localhost/api/health"
check_resource_usage
check_docker_containers
echo "========================="
```

## 4. 监控和告警系统

### 4.1 应用性能监控

```python
# monitoring/app_monitor.py - 应用性能监控
import psutil
import docker
import redis
import requests
from datetime import datetime
import json

class ApplicationMonitor:
    """应用性能监控器"""
    
    def __init__(self):
        self.docker_client = docker.from_env()
        self.redis_client = redis.Redis(host='localhost', port=6379, decode_responses=True)
        
    def collect_system_metrics(self):
        """收集系统指标"""
        return {
            "timestamp": datetime.now().isoformat(),
            "cpu_percent": psutil.cpu_percent(interval=1),
            "memory_percent": psutil.virtual_memory().percent,
            "disk_usage": psutil.disk_usage('/').percent,
            "network_io": psutil.net_io_counters()._asdict(),
            "load_average": psutil.getloadavg()
        }
    
    def collect_application_metrics(self):
        """收集应用指标"""
        try:
            # 健康检查响应时间
            start_time = datetime.now()
            response = requests.get('http://localhost/api/health', timeout=10)
            response_time = (datetime.now() - start_time).total_seconds()
            
            health_data = response.json() if response.status_code == 200 else {}
            
            return {
                "timestamp": datetime.now().isoformat(),
                "health_status": response.status_code == 200,
                "response_time": response_time,
                "health_data": health_data
            }
        except Exception as e:
            return {
                "timestamp": datetime.now().isoformat(),
                "health_status": False,
                "error": str(e)
            }
    
    def collect_docker_metrics(self):
        """收集Docker容器指标"""
        containers_metrics = []
        
        for container in self.docker_client.containers.list():
            try:
                stats = container.stats(stream=False)
                
                # CPU使用率计算
                cpu_delta = stats['cpu_stats']['cpu_usage']['total_usage'] - \
                           stats['precpu_stats']['cpu_usage']['total_usage']
                system_delta = stats['cpu_stats']['system_cpu_usage'] - \
                              stats['precpu_stats']['system_cpu_usage']
                
                cpu_percent = 0.0
                if system_delta > 0 and cpu_delta > 0:
                    cpu_percent = (cpu_delta / system_delta) * \
                                 len(stats['cpu_stats']['cpu_usage']['percpu_usage']) * 100
                
                # 内存使用率
                memory_usage = stats['memory_stats']['usage']
                memory_limit = stats['memory_stats']['limit']
                memory_percent = (memory_usage / memory_limit) * 100
                
                containers_metrics.append({
                    "name": container.name,
                    "status": container.status,
                    "cpu_percent": round(cpu_percent, 2),
                    "memory_usage_mb": round(memory_usage / 1024 / 1024, 2),
                    "memory_percent": round(memory_percent, 2)
                })
                
            except Exception as e:
                containers_metrics.append({
                    "name": container.name,
                    "status": container.status,
                    "error": str(e)
                })
        
        return {
            "timestamp": datetime.now().isoformat(),
            "containers": containers_metrics
        }
    
    def store_metrics(self, metrics_data):
        """存储监控数据到Redis"""
        key = f"metrics:{datetime.now().strftime('%Y%m%d:%H')}"
        
        # 使用Redis列表存储每小时的监控数据
        self.redis_client.lpush(key, json.dumps(metrics_data))
        
        # 设置过期时间（保留7天）
        self.redis_client.expire(key, 7 * 24 * 3600)
    
    def generate_alert(self, alert_type, message, severity="warning"):
        """生成告警"""
        alert = {
            "timestamp": datetime.now().isoformat(),
            "type": alert_type,
            "message": message,
            "severity": severity,
            "hostname": psutil.os.uname().nodename
        }
        
        # 存储告警到Redis
        self.redis_client.lpush("alerts", json.dumps(alert))
        self.redis_client.expire("alerts", 30 * 24 * 3600)  # 保留30天
        
        return alert
    
    def run_monitoring_cycle(self):
        """运行一个监控周期"""
        # 收集各类指标
        system_metrics = self.collect_system_metrics()
        app_metrics = self.collect_application_metrics()
        docker_metrics = self.collect_docker_metrics()
        
        # 合并指标数据
        all_metrics = {
            "system": system_metrics,
            "application": app_metrics,
            "docker": docker_metrics
        }
        
        # 存储指标
        self.store_metrics(all_metrics)
        
        # 检查告警条件
        self._check_alert_conditions(all_metrics)
        
        return all_metrics
    
    def _check_alert_conditions(self, metrics):
        """检查告警条件"""
        system = metrics["system"]
        app = metrics["application"]
        
        # CPU使用率告警
        if system["cpu_percent"] > 80:
            self.generate_alert(
                "high_cpu_usage",
                f"CPU使用率过高: {system['cpu_percent']:.1f}%",
                "warning"
            )
        
        # 内存使用率告警
        if system["memory_percent"] > 85:
            self.generate_alert(
                "high_memory_usage", 
                f"内存使用率过高: {system['memory_percent']:.1f}%",
                "warning"
            )
        
        # 应用健康状态告警
        if not app["health_status"]:
            self.generate_alert(
                "app_health_failure",
                f"应用健康检查失败: {app.get('error', '未知错误')}",
                "critical"
            )
        
        # 响应时间告警
        if app.get("response_time", 0) > 5:
            self.generate_alert(
                "slow_response",
                f"应用响应时间过长: {app['response_time']:.2f}秒",
                "warning"
            )

# 定时监控脚本
if __name__ == "__main__":
    import time
    import schedule
    
    monitor = ApplicationMonitor()
    
    def run_monitoring():
        try:
            metrics = monitor.run_monitoring_cycle()
            print(f"监控数据收集完成: {datetime.now()}")
        except Exception as e:
            print(f"监控过程出错: {e}")
    
    # 每分钟收集一次指标
    schedule.every(1).minutes.do(run_monitoring)
    
    print("开始监控服务...")
    while True:
        schedule.run_pending()
        time.sleep(1)
```

### 4.2 日志管理和分析

```python
# monitoring/log_analyzer.py - 日志分析器
import re
import json
from datetime import datetime, timedelta
from collections import defaultdict, Counter

class LogAnalyzer:
    """日志分析器"""
    
    def __init__(self, log_file_path):
        self.log_file_path = log_file_path
        self.patterns = {
            "error": re.compile(r'ERROR|Exception|Error'),
            "warning": re.compile(r'WARNING|WARN'),
            "api_request": re.compile(r'"(GET|POST|PUT|DELETE) ([^"]+)" (\d+)'),
            "slow_query": re.compile(r'slow query: ([\d.]+)s'),
            "user_action": re.compile(r'user:(\w+) action:(\w+)')
        }
    
    def parse_log_entries(self, hours_back=24):
        """解析日志条目"""
        entries = []
        cutoff_time = datetime.now() - timedelta(hours=hours_back)
        
        try:
            with open(self.log_file_path, 'r', encoding='utf-8') as f:
                for line_num, line in enumerate(f, 1):
                    try:
                        # 尝试解析时间戳
                        timestamp_match = re.match(r'^(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2})', line)
                        if timestamp_match:
                            timestamp = datetime.strptime(timestamp_match.group(1), '%Y-%m-%d %H:%M:%S')
                            if timestamp >= cutoff_time:
                                entries.append({
                                    'timestamp': timestamp,
                                    'line_number': line_num,
                                    'content': line.strip()
                                })
                    except Exception as e:
                        # 时间戳解析失败，跳过该行
                        continue
        except FileNotFoundError:
            print(f"日志文件未找到: {self.log_file_path}")
        
        return entries
    
    def analyze_error_patterns(self, entries):
        """分析错误模式"""
        error_stats = {
            "total_errors": 0,
            "error_types": Counter(),
            "error_timeline": defaultdict(int),
            "top_errors": []
        }
        
        for entry in entries:
            if self.patterns["error"].search(entry['content']):
                error_stats["total_errors"] += 1
                
                # 按小时统计
                hour_key = entry['timestamp'].strftime('%Y-%m-%d %H:00')
                error_stats["error_timeline"][hour_key] += 1
                
                # 错误类型分类
                if "Exception" in entry['content']:
                    error_stats["error_types"]["Exception"] += 1
                elif "Error" in entry['content']:
                    error_stats["error_types"]["Error"] += 1
                else:
                    error_stats["error_types"]["Other"] += 1
        
        # 获取最频繁的错误
        error_stats["top_errors"] = error_stats["error_types"].most_common(5)
        
        return error_stats
    
    def analyze_api_performance(self, entries):
        """分析API性能"""
        api_stats = {
            "total_requests": 0,
            "status_codes": Counter(),
            "endpoints": Counter(),
            "methods": Counter(),
            "avg_response_time": 0.0,
            "slow_requests": []
        }
        
        response_times = []
        
        for entry in entries:
            api_match = self.patterns["api_request"].search(entry['content'])
            if api_match:
                method, endpoint, status_code = api_match.groups()
                
                api_stats["total_requests"] += 1
                api_stats["methods"][method] += 1
                api_stats["endpoints"][endpoint] += 1
                api_stats["status_codes"][status_code] += 1
                
                # 提取响应时间（如果有）
                time_match = re.search(r'(\d+\.\d+)ms', entry['content'])
                if time_match:
                    response_time = float(time_match.group(1))
                    response_times.append(response_time)
                    
                    # 记录慢请求 (>2000ms)
                    if response_time > 2000:
                        api_stats["slow_requests"].append({
                            "timestamp": entry['timestamp'].isoformat(),
                            "method": method,
                            "endpoint": endpoint,
                            "response_time": response_time
                        })
        
        if response_times:
            api_stats["avg_response_time"] = sum(response_times) / len(response_times)
        
        return api_stats
    
    def generate_report(self, hours_back=24):
        """生成分析报告"""
        entries = self.parse_log_entries(hours_back)
        error_analysis = self.analyze_error_patterns(entries)
        api_analysis = self.analyze_api_performance(entries)
        
        report = {
            "analysis_period": f"{hours_back}小时",
            "total_log_entries": len(entries),
            "analysis_timestamp": datetime.now().isoformat(),
            "error_analysis": error_analysis,
            "api_analysis": api_analysis,
            "summary": self._generate_summary(error_analysis, api_analysis)
        }
        
        return report
    
    def _generate_summary(self, error_analysis, api_analysis):
        """生成摘要信息"""
        summary = []
        
        if error_analysis["total_errors"] > 0:
            summary.append(f"发现 {error_analysis['total_errors']} 个错误")
        
        if api_analysis["total_requests"] > 0:
            error_rate = (error_analysis["total_errors"] / api_analysis["total_requests"]) * 100
            summary.append(f"错误率: {error_rate:.2f}%")
            
            if api_analysis["avg_response_time"] > 0:
                summary.append(f"平均响应时间: {api_analysis['avg_response_time']:.1f}ms")
        
        if len(api_analysis["slow_requests"]) > 0:
            summary.append(f"慢请求数量: {len(api_analysis['slow_requests'])}")
        
        return summary
```

---

**文档版本**: v1.0  
**创建时间**: 2025-01-15  
**基于环境**: 4H2G Ubuntu + Docker + Nginx + SQLite + Redis  
**维护人员**: 运维开发团队