# 自适应表格对比算法规格书

## 1. 算法总体设计

### 1.1 算法概述

自适应表格对比算法是系统的核心技术组件，专门用于解决动态数量的腾讯文档表格在格式差异、列名变异、缺失列等情况下的智能对比分析问题。

### 1.2 核心挑战和解决方案

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                    自适应表格对比算法架构                                      │
├─────────────────────────────────────────────────────────────────────────────┤
│  输入层: N个异构表格（动态配置）                                                 │
│  ┌─────────────────┐  ┌─────────────────┐  ┌─────────────────┐              │
│  │ 表格A (19列)     │  │ 表格B (17列)     │  │ 表格C (20列)     │              │
│  │ - 列名标准       │  │ - 缺失2列        │  │ - 多1列          │              │
│  │ - 格式规范       │  │ - 列名变异       │  │ - 数据格式异常   │              │
│  └─────────────────┘  └─────────────────┘  └─────────────────┘              │
├─────────────────────────────────────────────────────────────────────────────┤
│  算法处理层                                                                   │
│  ┌─────────────────┐  ┌─────────────────┐  ┌─────────────────┐              │
│  │ 智能列匹配引擎   │  │ 数据标准化器     │  │ 语义相似度分析   │              │
│  │ - 文本相似度     │  │ - 格式统一       │  │ - 上下文理解     │              │
│  │ - 位置权重       │  │ - 缺失值填充     │  │ - 业务规则       │              │
│  │ - 历史学习       │  │ - 类型转换       │  │ - 专家知识       │              │
│  └─────────────────┘  └─────────────────┘  └─────────────────┘              │
├─────────────────────────────────────────────────────────────────────────────┤
│  输出层: 标准化N×19动态矩阵                                                    │
│  ┌─────────────────────────────────────────────────────────────────────────┐ │
│  │ 统一格式矩阵 + 置信度评分 + 匹配报告 + 异常标记                            │ │
│  └─────────────────────────────────────────────────────────────────────────┘ │
└─────────────────────────────────────────────────────────────────────────────┘
```

### 1.3 算法性能目标

- **列匹配准确率**: ≥ 95%
- **数据标准化质量**: ≥ 90%  
- **处理速度**: 平均每个表格 < 1秒
- **内存使用**: < 500MB
- **异常处理覆盖率**: ≥ 99%

## 2. 智能列匹配算法

### 2.1 多重匹配策略

#### 2.1.1 基础匹配算法
```python
# 基于现有document_change_analyzer.py扩展的智能列匹配器
class IntelligentColumnMatcher:
    """智能列匹配引擎"""
    
    def __init__(self):
        self.standard_columns = [
            "序号", "项目类型", "来源", "任务发起时间", "目标对齐",
            "关键KR对齐", "具体计划内容", "邓总指导登记", "负责人",
            "协助人", "监督人", "重要程度", "预计完成时间", "完成进度", 
            "完成链接", "最新复盘时间", "对上汇报", "应用情况", "经理分析复盘"
        ]
        
        # 列名变异映射表（基于历史数据学习）
        self.column_variations = {
            "项目类型": ["项目分类", "类型", "项目", "分类"],
            "来源": ["数据来源", "来源部门", "源头", "起源"],
            "任务发起时间": ["发起时间", "开始时间", "任务时间", "发起日期"],
            "目标对齐": ["对齐目标", "目标", "对齐情况", "目标匹配"],
            "关键KR对齐": ["KR对齐", "关键结果", "KR匹配", "关键指标对齐"],
            "具体计划内容": ["计划内容", "计划详情", "具体计划", "内容"],
            "负责人": ["责任人", "主负责人", "项目负责人", "负责"],
            "协助人": ["协助", "协助者", "协助人员", "辅助人"],
            "监督人": ["监督", "监督者", "监督人员", "督导"],
            "重要程度": ["重要性", "优先级", "重要等级", "程度"],
            "预计完成时间": ["完成时间", "计划完成", "预期完成", "截止时间"],
            "完成进度": ["进度", "完成度", "完成状态", "进展"],
            "完成链接": ["计划清单", "清单", "任务清单", "计划列表"],
            "最新复盘时间": ["复盘", "复盘日期", "回顾时间", "总结时间"],
            "对上汇报": ["汇报", "上报", "汇报情况", "汇报状态"],
            "应用情况": ["应用", "应用状态", "使用情况", "执行情况"],
            "经理分析复盘": ["分析总结", "总结", "进度总结", "分析报告"]
        }
        
        # 语义相似度计算器
        self.similarity_calculator = SemanticSimilarityCalculator()
        
        # 匹配历史记录（用于学习和改进）
        self.matching_history = []
        
    def match_columns(self, actual_columns, table_name=None, confidence_threshold=0.6):
        """
        智能列匹配主函数
        
        Args:
            actual_columns: 实际表格的列名列表
            table_name: 表格名称（用于上下文分析）
            confidence_threshold: 匹配置信度阈值
            
        Returns:
            {
                "mapping": {"实际列名": "标准列名"},
                "confidence_scores": {"实际列名": 置信度分数},
                "unmatched_columns": ["未匹配的列名"],
                "missing_columns": ["缺失的标准列名"],
                "matching_report": "详细匹配报告"
            }
        """
        
        matching_result = {
            "mapping": {},
            "confidence_scores": {},
            "unmatched_columns": [],
            "missing_columns": [],
            "matching_report": ""
        }
        
        # 步骤1: 精确匹配
        exact_matches = self._find_exact_matches(actual_columns)
        for actual_col, standard_col in exact_matches.items():
            matching_result["mapping"][actual_col] = standard_col
            matching_result["confidence_scores"][actual_col] = 1.0
        
        # 步骤2: 变异匹配
        remaining_columns = [col for col in actual_columns if col not in exact_matches]
        variation_matches = self._find_variation_matches(remaining_columns)
        for actual_col, (standard_col, confidence) in variation_matches.items():
            if confidence >= confidence_threshold:
                matching_result["mapping"][actual_col] = standard_col
                matching_result["confidence_scores"][actual_col] = confidence
                remaining_columns.remove(actual_col)
        
        # 步骤3: 语义相似度匹配
        semantic_matches = self._find_semantic_matches(
            remaining_columns, table_name, confidence_threshold
        )
        for actual_col, (standard_col, confidence) in semantic_matches.items():
            matching_result["mapping"][actual_col] = standard_col
            matching_result["confidence_scores"][actual_col] = confidence
            remaining_columns.remove(actual_col)
        
        # 步骤4: 位置启发式匹配
        position_matches = self._find_position_based_matches(
            remaining_columns, actual_columns, confidence_threshold
        )
        for actual_col, (standard_col, confidence) in position_matches.items():
            matching_result["mapping"][actual_col] = standard_col
            matching_result["confidence_scores"][actual_col] = confidence
            remaining_columns.remove(actual_col)
        
        # 确定未匹配和缺失的列
        matching_result["unmatched_columns"] = remaining_columns
        matched_standard_columns = set(matching_result["mapping"].values())
        matching_result["missing_columns"] = [
            col for col in self.standard_columns 
            if col not in matched_standard_columns
        ]
        
        # 生成匹配报告
        matching_result["matching_report"] = self._generate_matching_report(
            matching_result, actual_columns, table_name
        )
        
        # 记录匹配历史用于学习
        self._record_matching_history(actual_columns, matching_result, table_name)
        
        return matching_result
    
    def _find_exact_matches(self, actual_columns):
        """精确匹配：完全相同的列名"""
        exact_matches = {}
        for actual_col in actual_columns:
            if actual_col in self.standard_columns:
                exact_matches[actual_col] = actual_col
        return exact_matches
    
    def _find_variation_matches(self, remaining_columns):
        """变异匹配：已知变异形式的列名"""
        variation_matches = {}
        
        for actual_col in remaining_columns:
            best_match = None
            best_confidence = 0.0
            
            for standard_col, variations in self.column_variations.items():
                for variation in variations:
                    # 计算文本相似度
                    similarity = self._calculate_text_similarity(actual_col, variation)
                    if similarity > best_confidence:
                        best_match = standard_col
                        best_confidence = similarity
            
            if best_match and best_confidence > 0.7:  # 变异匹配需要较高置信度
                variation_matches[actual_col] = (best_match, best_confidence)
        
        return variation_matches
    
    def _find_semantic_matches(self, remaining_columns, table_name, threshold):
        """语义匹配：基于语义相似度的匹配"""
        semantic_matches = {}
        
        for actual_col in remaining_columns:
            # 为每个剩余列计算与所有标准列的语义相似度
            similarities = []
            for standard_col in self.standard_columns:
                semantic_score = self.similarity_calculator.calculate_semantic_similarity(
                    actual_col, standard_col, context=table_name
                )
                similarities.append((standard_col, semantic_score))
            
            # 选择最高相似度的匹配
            best_match = max(similarities, key=lambda x: x[1])
            if best_match[1] >= threshold:
                semantic_matches[actual_col] = best_match
        
        return semantic_matches
    
    def _find_position_based_matches(self, remaining_columns, all_columns, threshold):
        """位置启发式匹配：基于列的位置信息进行匹配"""
        position_matches = {}
        
        for i, actual_col in enumerate(all_columns):
            if actual_col in remaining_columns:
                # 基于位置推测可能的标准列
                if i < len(self.standard_columns):
                    expected_standard_col = self.standard_columns[i]
                    
                    # 计算位置匹配置信度
                    position_confidence = self._calculate_position_confidence(
                        actual_col, expected_standard_col, i
                    )
                    
                    if position_confidence >= threshold:
                        position_matches[actual_col] = (expected_standard_col, position_confidence)
        
        return position_matches
    
    def _calculate_text_similarity(self, text1, text2):
        """计算文本相似度（基于编辑距离和字符相似度）"""
        # Levenshtein距离
        edit_distance = self._levenshtein_distance(text1, text2)
        max_len = max(len(text1), len(text2))
        if max_len == 0:
            return 1.0
        
        # 基于编辑距离的相似度
        edit_similarity = 1 - (edit_distance / max_len)
        
        # 字符重叠相似度
        set1 = set(text1)
        set2 = set(text2)
        overlap_similarity = len(set1 & set2) / len(set1 | set2) if set1 | set2 else 0
        
        # 综合相似度
        final_similarity = 0.7 * edit_similarity + 0.3 * overlap_similarity
        
        return final_similarity
    
    def _levenshtein_distance(self, s1, s2):
        """计算Levenshtein编辑距离"""
        if len(s1) < len(s2):
            return self._levenshtein_distance(s2, s1)
        
        if len(s2) == 0:
            return len(s1)
        
        previous_row = list(range(len(s2) + 1))
        for i, c1 in enumerate(s1):
            current_row = [i + 1]
            for j, c2 in enumerate(s2):
                insertions = previous_row[j + 1] + 1
                deletions = current_row[j] + 1
                substitutions = previous_row[j] + (c1 != c2)
                current_row.append(min(insertions, deletions, substitutions))
            previous_row = current_row
        
        return previous_row[-1]
```

#### 2.1.2 语义相似度分析器
```python
class SemanticSimilarityCalculator:
    """语义相似度计算器"""
    
    def __init__(self):
        # 业务领域词汇映射
        self.domain_vocabulary = {
            "时间": ["时间", "日期", "期限", "截止", "开始", "结束"],
            "人员": ["人", "员", "者", "负责", "主管", "经理"],
            "进度": ["进度", "完成", "状态", "情况", "程度"],
            "内容": ["内容", "详情", "描述", "说明", "计划"],
            "重要": ["重要", "优先", "关键", "核心", "主要"]
        }
        
        # 上下文关键词
        self.context_keywords = {
            "项目管理": ["项目", "管理", "计划", "执行", "监控"],
            "任务追踪": ["任务", "跟踪", "追踪", "监督", "检查"],
            "绩效评估": ["绩效", "评估", "考核", "评价", "分析"]
        }
    
    def calculate_semantic_similarity(self, actual_column, standard_column, context=None):
        """
        计算语义相似度
        
        Args:
            actual_column: 实际列名
            standard_column: 标准列名
            context: 上下文信息（如表格名称）
        
        Returns:
            float: 语义相似度分数 (0-1)
        """
        
        # 基础词汇匹配分数
        word_similarity = self._calculate_word_similarity(actual_column, standard_column)
        
        # 领域语义分数
        domain_similarity = self._calculate_domain_similarity(actual_column, standard_column)
        
        # 上下文增强分数
        context_boost = self._calculate_context_boost(
            actual_column, standard_column, context
        ) if context else 0
        
        # 综合语义相似度
        semantic_score = (
            0.4 * word_similarity + 
            0.4 * domain_similarity + 
            0.2 * context_boost
        )
        
        return min(1.0, semantic_score)
    
    def _calculate_word_similarity(self, actual, standard):
        """计算词汇级别的相似度"""
        actual_words = self._extract_meaningful_words(actual)
        standard_words = self._extract_meaningful_words(standard)
        
        if not actual_words or not standard_words:
            return 0.0
        
        # 计算词汇重叠度
        overlap = len(set(actual_words) & set(standard_words))
        total = len(set(actual_words) | set(standard_words))
        
        return overlap / total if total > 0 else 0.0
    
    def _calculate_domain_similarity(self, actual, standard):
        """计算领域语义相似度"""
        actual_domains = self._identify_domains(actual)
        standard_domains = self._identify_domains(standard)
        
        if not actual_domains or not standard_domains:
            return 0.0
        
        # 领域重叠度
        domain_overlap = len(actual_domains & standard_domains)
        domain_total = len(actual_domains | standard_domains)
        
        return domain_overlap / domain_total if domain_total > 0 else 0.0
    
    def _identify_domains(self, column_name):
        """识别列名所属的业务领域"""
        identified_domains = set()
        
        for domain, keywords in self.domain_vocabulary.items():
            for keyword in keywords:
                if keyword in column_name:
                    identified_domains.add(domain)
        
        return identified_domains
```

### 2.2 学习和改进机制

#### 2.2.1 匹配历史学习
```python
class MatchingHistoryLearner:
    """匹配历史学习器"""
    
    def __init__(self, history_file="matching_history.json"):
        self.history_file = history_file
        self.matching_patterns = self._load_history()
        
    def record_successful_matching(self, actual_column, standard_column, 
                                 confidence, context=None):
        """记录成功的匹配案例"""
        pattern = {
            "actual_column": actual_column,
            "standard_column": standard_column,
            "confidence": confidence,
            "context": context,
            "timestamp": datetime.now().isoformat(),
            "success": True
        }
        
        self.matching_patterns.append(pattern)
        self._save_history()
        
        # 更新变异映射表
        self._update_variation_mappings(actual_column, standard_column)
    
    def record_failed_matching(self, actual_column, attempted_matches, context=None):
        """记录失败的匹配尝试"""
        pattern = {
            "actual_column": actual_column,
            "attempted_matches": attempted_matches,
            "context": context,
            "timestamp": datetime.now().isoformat(),
            "success": False
        }
        
        self.matching_patterns.append(pattern)
        self._save_history()
    
    def get_learned_variations(self, standard_column):
        """获取学习到的列名变异"""
        variations = []
        for pattern in self.matching_patterns:
            if (pattern.get("success") and 
                pattern.get("standard_column") == standard_column and
                pattern.get("confidence", 0) > 0.8):
                variations.append(pattern["actual_column"])
        
        return list(set(variations))
    
    def _update_variation_mappings(self, actual_column, standard_column):
        """动态更新变异映射表"""
        if standard_column in self.column_variations:
            if actual_column not in self.column_variations[standard_column]:
                self.column_variations[standard_column].append(actual_column)
        else:
            self.column_variations[standard_column] = [actual_column]
```

## 3. 数据标准化算法

### 3.1 多维数据标准化

#### 3.1.1 数据标准化器
```python
class AdaptiveDataStandardizer:
    """自适应数据标准化器"""
    
    def __init__(self):
        self.standard_data_types = {
            "序号": int,
            "项目类型": str,
            "来源": str,
            "任务发起时间": "datetime",
            "目标对齐": str,
            "关键KR对齐": str,
            "具体计划内容": str,
            "邓总指导登记": str,
            "负责人": str,
            "协助人": str,
            "监督人": str,
            "重要程度": str,  # 可能是数值或文本
            "预计完成时间": "datetime",
            "完成进度": "percentage",  # 0-1或0-100%
            "完成链接": str,
            "最新复盘时间": "datetime",
            "对上汇报": str,
            "应用情况": str,
            "经理分析复盘": str
        }
        
        self.standardization_rules = {
            "datetime": self._standardize_datetime,
            "percentage": self._standardize_percentage,
            int: self._standardize_integer,
            str: self._standardize_string
        }
        
    def standardize_table_data(self, raw_data, column_mapping, table_metadata=None):
        """
        标准化表格数据
        
        Args:
            raw_data: 原始表格数据 (list of lists)
            column_mapping: 列名映射 {"实际列名": "标准列名"}
            table_metadata: 表格元数据
            
        Returns:
            {
                "standardized_data": 标准化后的数据,
                "standardization_report": 标准化报告,
                "data_quality_score": 数据质量分数,
                "issues": 发现的问题列表
            }
        """
        
        if not raw_data:
            return self._empty_standardization_result()
        
        # 提取标题行和数据行
        headers = raw_data[0] if raw_data else []
        data_rows = raw_data[1:] if len(raw_data) > 1 else []
        
        # 创建标准化后的数据结构
        standardized_data = []
        standardization_issues = []
        
        # 处理每一行数据
        for row_index, row in enumerate(data_rows):
            standardized_row = self._standardize_single_row(
                row, headers, column_mapping, row_index, standardization_issues
            )
            standardized_data.append(standardized_row)
        
        # 填充缺失列
        standardized_data = self._fill_missing_columns(
            standardized_data, column_mapping, standardization_issues
        )
        
        # 计算数据质量分数
        quality_score = self._calculate_data_quality_score(
            standardized_data, standardization_issues, len(data_rows)
        )
        
        return {
            "standardized_data": standardized_data,
            "standardization_report": self._generate_standardization_report(
                standardization_issues, quality_score
            ),
            "data_quality_score": quality_score,
            "issues": standardization_issues
        }
    
    def _standardize_single_row(self, row, headers, column_mapping, row_index, issues):
        """标准化单行数据"""
        standardized_row = {}
        
        # 处理每个标准列
        for standard_column in self.standard_columns:
            value = None
            
            # 查找对应的实际列值
            for actual_column, mapped_standard in column_mapping.items():
                if mapped_standard == standard_column:
                    actual_column_index = headers.index(actual_column) if actual_column in headers else -1
                    if actual_column_index != -1 and actual_column_index < len(row):
                        raw_value = row[actual_column_index]
                        
                        # 应用标准化规则
                        try:
                            value = self._apply_standardization_rule(
                                raw_value, standard_column, row_index, actual_column
                            )
                        except Exception as e:
                            issues.append({
                                "type": "standardization_error",
                                "row": row_index,
                                "column": standard_column,
                                "raw_value": raw_value,
                                "error": str(e)
                            })
                            value = self._get_default_value(standard_column)
                    break
            
            # 如果没有找到对应值，使用默认值
            if value is None:
                value = self._get_default_value(standard_column)
                issues.append({
                    "type": "missing_value",
                    "row": row_index,
                    "column": standard_column,
                    "default_used": value
                })
            
            standardized_row[standard_column] = value
        
        return standardized_row
    
    def _apply_standardization_rule(self, raw_value, standard_column, row_index, actual_column):
        """应用标准化规则"""
        expected_type = self.standard_data_types.get(standard_column, str)
        
        if expected_type in self.standardization_rules:
            return self.standardization_rules[expected_type](raw_value, standard_column)
        else:
            return str(raw_value) if raw_value is not None else ""
    
    def _standardize_datetime(self, value, column_name):
        """标准化日期时间"""
        if not value or pd.isna(value):
            return None
        
        # 尝试多种日期格式
        date_formats = [
            "%Y-%m-%d",
            "%Y/%m/%d", 
            "%Y年%m月%d日",
            "%Y-%m-%d %H:%M:%S",
            "%Y/%m/%d %H:%M:%S",
            "%m/%d/%Y",
            "%d/%m/%Y"
        ]
        
        for fmt in date_formats:
            try:
                parsed_date = datetime.strptime(str(value), fmt)
                return parsed_date.isoformat()
            except ValueError:
                continue
        
        # 如果所有格式都失败，返回原值
        return str(value)
    
    def _standardize_percentage(self, value, column_name):
        """标准化百分比"""
        if not value or pd.isna(value):
            return 0.0
        
        str_value = str(value).strip()
        
        # 处理百分号格式
        if '%' in str_value:
            numeric_part = str_value.replace('%', '').strip()
            try:
                return float(numeric_part) / 100.0
            except ValueError:
                return 0.0
        
        # 处理小数格式
        try:
            numeric_value = float(str_value)
            if numeric_value > 1.0:
                return numeric_value / 100.0  # 假设是百分数形式
            return numeric_value
        except ValueError:
            return 0.0
    
    def _standardize_integer(self, value, column_name):
        """标准化整数"""
        if not value or pd.isna(value):
            return 0
        
        try:
            return int(float(str(value)))
        except ValueError:
            return 0
    
    def _standardize_string(self, value, column_name):
        """标准化字符串"""
        if not value or pd.isna(value):
            return ""
        
        # 清理字符串
        cleaned = str(value).strip()
        
        # 移除多余的空白字符
        cleaned = ' '.join(cleaned.split())
        
        return cleaned
```

### 3.2 缺失值处理策略

#### 3.2.1 智能填充算法
```python
class MissingValueHandler:
    """缺失值处理器"""
    
    def __init__(self):
        self.filling_strategies = {
            "序号": self._fill_sequence,
            "项目类型": self._fill_categorical,
            "来源": self._fill_categorical,
            "任务发起时间": self._fill_datetime,
            "目标对齐": self._fill_text_context,
            "关键KR对齐": self._fill_text_context,
            "具体计划内容": self._fill_text_context,
            "负责人": self._fill_categorical,
            "重要程度": self._fill_importance,
            "完成进度": self._fill_progress,
            # ... 其他列的填充策略
        }
    
    def fill_missing_values(self, standardized_data, column_mapping, table_context):
        """填充缺失值"""
        filled_data = []
        filling_report = []
        
        for row_index, row in enumerate(standardized_data):
            filled_row = {}
            
            for column, value in row.items():
                if self._is_missing_value(value):
                    # 应用相应的填充策略
                    if column in self.filling_strategies:
                        filled_value = self.filling_strategies[column](
                            value, row, standardized_data, row_index, column
                        )
                        filled_row[column] = filled_value
                        
                        filling_report.append({
                            "row": row_index,
                            "column": column,
                            "original_value": value,
                            "filled_value": filled_value,
                            "strategy": self.filling_strategies[column].__name__
                        })
                    else:
                        # 默认填充策略
                        filled_value = self._default_fill(column, value)
                        filled_row[column] = filled_value
                else:
                    filled_row[column] = value
            
            filled_data.append(filled_row)
        
        return filled_data, filling_report
    
    def _fill_sequence(self, value, row, all_data, row_index, column):
        """填充序号"""
        return row_index + 1
    
    def _fill_categorical(self, value, row, all_data, row_index, column):
        """填充分类型数据"""
        # 统计该列最常见的值
        column_values = [r[column] for r in all_data if not self._is_missing_value(r[column])]
        if column_values:
            from collections import Counter
            most_common = Counter(column_values).most_common(1)
            return most_common[0][0] if most_common else "未知"
        return "未知"
    
    def _fill_datetime(self, value, row, all_data, row_index, column):
        """填充日期时间"""
        # 使用相邻行的日期或当前日期
        if row_index > 0:
            prev_value = all_data[row_index - 1][column]
            if not self._is_missing_value(prev_value):
                return prev_value
        
        return datetime.now().isoformat()
    
    def _fill_progress(self, value, row, all_data, row_index, column):
        """填充进度数据"""
        # 基于其他相关列推测进度
        if "预计完成时间" in row and "任务发起时间" in row:
            # 基于时间推测进度
            return self._estimate_progress_by_time(
                row["任务发起时间"], row["预计完成时间"]
            )
        
        return 0.0  # 默认未开始
```

## 4. 容错和异常处理机制

### 4.1 异常检测和恢复

#### 4.1.1 异常检测器
```python
class TableAnomalyDetector:
    """表格异常检测器"""
    
    def __init__(self):
        self.anomaly_thresholds = {
            "missing_columns_ratio": 0.3,  # 缺失列比例阈值
            "data_quality_minimum": 0.6,   # 最低数据质量要求
            "row_completeness_minimum": 0.5, # 行完整性最低要求
            "type_consistency_minimum": 0.8   # 类型一致性最低要求
        }
        
    def detect_anomalies(self, table_data, standardization_result):
        """检测表格异常"""
        anomalies = []
        
        # 检测结构异常
        structural_anomalies = self._detect_structural_anomalies(table_data)
        anomalies.extend(structural_anomalies)
        
        # 检测数据质量异常
        quality_anomalies = self._detect_quality_anomalies(standardization_result)
        anomalies.extend(quality_anomalies)
        
        # 检测业务逻辑异常
        business_anomalies = self._detect_business_anomalies(table_data)
        anomalies.extend(business_anomalies)
        
        return {
            "anomalies": anomalies,
            "severity_summary": self._categorize_anomalies(anomalies),
            "recovery_suggestions": self._generate_recovery_suggestions(anomalies)
        }
    
    def _detect_structural_anomalies(self, table_data):
        """检测结构异常"""
        anomalies = []
        
        if not table_data:
            anomalies.append({
                "type": "empty_table",
                "severity": "critical",
                "message": "表格为空"
            })
            return anomalies
        
        # 检测列数异常
        expected_columns = len(self.standard_columns)
        actual_columns = len(table_data[0]) if table_data else 0
        
        if actual_columns < expected_columns * 0.7:
            anomalies.append({
                "type": "insufficient_columns",
                "severity": "high",
                "message": f"列数不足：期望{expected_columns}列，实际{actual_columns}列"
            })
        
        return anomalies
    
    def _detect_quality_anomalies(self, standardization_result):
        """检测数据质量异常"""
        anomalies = []
        
        quality_score = standardization_result.get("data_quality_score", 0)
        if quality_score < self.anomaly_thresholds["data_quality_minimum"]:
            anomalies.append({
                "type": "low_data_quality",
                "severity": "high",
                "message": f"数据质量过低：{quality_score:.2f}",
                "details": standardization_result.get("issues", [])
            })
        
        return anomalies
    
    def _generate_recovery_suggestions(self, anomalies):
        """生成恢复建议"""
        suggestions = []
        
        for anomaly in anomalies:
            if anomaly["type"] == "empty_table":
                suggestions.append({
                    "anomaly_type": anomaly["type"],
                    "suggestion": "检查表格URL是否正确，或表格是否需要认证访问",
                    "action": "retry_with_authentication"
                })
            elif anomaly["type"] == "insufficient_columns":
                suggestions.append({
                    "anomaly_type": anomaly["type"],
                    "suggestion": "尝试使用宽松的列匹配策略，或手动补充缺失列",
                    "action": "relaxed_column_matching"
                })
            elif anomaly["type"] == "low_data_quality":
                suggestions.append({
                    "anomaly_type": anomaly["type"],
                    "suggestion": "启用数据清理和修复功能，或降低质量要求阈值",
                    "action": "enable_data_repair"
                })
        
        return suggestions
```

### 4.2 性能监控和优化

#### 4.2.1 算法性能监控
```python
class AlgorithmPerformanceMonitor:
    """算法性能监控器"""
    
    def __init__(self):
        self.performance_metrics = {
            "column_matching_time": [],
            "data_standardization_time": [],
            "missing_value_fill_time": [],
            "total_processing_time": [],
            "memory_usage": [],
            "accuracy_scores": []
        }
        
    def monitor_column_matching(self, func):
        """监控列匹配性能"""
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            start_time = time.time()
            start_memory = self._get_memory_usage()
            
            result = func(*args, **kwargs)
            
            end_time = time.time()
            end_memory = self._get_memory_usage()
            
            # 记录性能指标
            self.performance_metrics["column_matching_time"].append(end_time - start_time)
            self.performance_metrics["memory_usage"].append(end_memory - start_memory)
            
            # 计算匹配准确率
            if result and "confidence_scores" in result:
                avg_confidence = sum(result["confidence_scores"].values()) / len(result["confidence_scores"])
                self.performance_metrics["accuracy_scores"].append(avg_confidence)
            
            return result
        
        return wrapper
    
    def generate_performance_report(self):
        """生成性能报告"""
        report = {
            "average_processing_time": {
                "column_matching": np.mean(self.performance_metrics["column_matching_time"]),
                "data_standardization": np.mean(self.performance_metrics["data_standardization_time"]),
                "total": np.mean(self.performance_metrics["total_processing_time"])
            },
            "accuracy_metrics": {
                "average_confidence": np.mean(self.performance_metrics["accuracy_scores"]),
                "confidence_std": np.std(self.performance_metrics["accuracy_scores"])
            },
            "resource_usage": {
                "average_memory_mb": np.mean(self.performance_metrics["memory_usage"]),
                "peak_memory_mb": np.max(self.performance_metrics["memory_usage"])
            },
            "performance_trends": {
                "processing_time_trend": self._calculate_trend(
                    self.performance_metrics["total_processing_time"]
                ),
                "accuracy_trend": self._calculate_trend(
                    self.performance_metrics["accuracy_scores"]
                )
            }
        }
        
        return report
```

## 5. 算法验证和测试

### 5.1 测试框架设计

#### 5.1.1 算法测试套件
```python
class AdaptiveComparisonTestSuite:
    """自适应对比算法测试套件"""
    
    def __init__(self):
        self.test_cases = self._load_test_cases()
        self.performance_benchmarks = self._load_benchmarks()
        
    def run_comprehensive_tests(self):
        """运行完整的测试套件"""
        test_results = {
            "unit_tests": self._run_unit_tests(),
            "integration_tests": self._run_integration_tests(),
            "performance_tests": self._run_performance_tests(),
            "accuracy_tests": self._run_accuracy_tests()
        }
        
        return self._generate_test_report(test_results)
    
    def _run_accuracy_tests(self):
        """运行准确性测试"""
        accuracy_results = []
        
        for test_case in self.test_cases:
            matcher = IntelligentColumnMatcher()
            result = matcher.match_columns(
                test_case["input_columns"],
                test_case.get("table_name"),
                0.6
            )
            
            # 计算准确率
            expected_mapping = test_case["expected_mapping"]
            actual_mapping = result["mapping"]
            
            correct_matches = sum(
                1 for k, v in actual_mapping.items()
                if expected_mapping.get(k) == v
            )
            
            accuracy = correct_matches / len(expected_mapping) if expected_mapping else 0
            
            accuracy_results.append({
                "test_case": test_case["name"],
                "accuracy": accuracy,
                "correct_matches": correct_matches,
                "total_expected": len(expected_mapping),
                "false_positives": len(actual_mapping) - correct_matches,
                "details": result
            })
        
        return accuracy_results
    
    def _run_performance_tests(self):
        """运行性能测试"""
        performance_results = []
        
        # 测试不同规模的数据集
        test_sizes = [5, 10, 20, 30, 50]
        
        for size in test_sizes:
            test_data = self._generate_test_data(size)
            
            start_time = time.time()
            start_memory = self._get_memory_usage()
            
            # 运行算法
            comparator = AdaptiveTableComparator()
            result = comparator.adaptive_compare_tables(
                test_data["current"], test_data["reference"]
            )
            
            end_time = time.time()
            end_memory = self._get_memory_usage()
            
            performance_results.append({
                "table_count": size,
                "processing_time": end_time - start_time,
                "memory_usage": end_memory - start_memory,
                "throughput": size / (end_time - start_time),
                "meets_sla": (end_time - start_time) < (size * 1.0)  # 1秒/表格SLA
            })
        
        return performance_results
```

---

**文档版本**: v1.0  
**创建时间**: 2025-01-15  
**基于系统**: document_change_analyzer.py + 智能匹配算法  
**维护人员**: 算法开发团队