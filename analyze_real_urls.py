#!/usr/bin/env python3
"""
æ·±åº¦åˆ†æé¡¹ç›®ä¸­æ‰€æœ‰çœŸå®URLå’ŒåŸºçº¿æ–‡ä»¶çš„å¯¹åº”å…³ç³»
"""

import json
import os
import glob
from collections import defaultdict
from datetime import datetime

def analyze_real_documents():
    """åˆ†æçœŸå®æ–‡æ¡£é…ç½®"""
    print("\n" + "=" * 80)
    print("ğŸ“‹ çœŸå®æ–‡æ¡£é…ç½®åˆ†æ")
    print("=" * 80)

    # è¯»å–real_documents.json
    real_docs_path = "production/config/real_documents.json"
    if os.path.exists(real_docs_path):
        with open(real_docs_path, 'r', encoding='utf-8') as f:
            real_docs = json.load(f)

        print(f"\næ‰¾åˆ° {len(real_docs['documents'])} ä¸ªçœŸå®æ–‡æ¡£é…ç½®ï¼š\n")
        for doc in real_docs['documents']:
            print(f"  ğŸ“Œ {doc['name']}")
            print(f"     URL: {doc['url']}")
            print(f"     ID:  {doc['doc_id']}")
            print(f"     æè¿°: {doc['description']}")
            print()
    else:
        print("âŒ æœªæ‰¾åˆ° real_documents.json")

    return real_docs.get('documents', []) if 'real_docs' in locals() else []

def analyze_workflow_history():
    """åˆ†æworkflowå†å²è®°å½•ä¸­çš„URL"""
    print("\n" + "=" * 80)
    print("ğŸ“Š Workflowå†å²è®°å½•åˆ†æ")
    print("=" * 80)

    # æ”¶é›†æ‰€æœ‰workflowæ–‡ä»¶
    workflow_files = glob.glob("workflow_history/*.json")
    url_mapping = defaultdict(list)

    for file_path in workflow_files:
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)

            results = data.get('results', {})
            if 'upload_url' in results and results['upload_url']:
                url = results['upload_url']
                baseline = results.get('baseline_file', '')
                marked = results.get('marked_file', '')

                # æå–æ–‡ä»¶å
                if baseline:
                    baseline_name = os.path.basename(baseline)
                else:
                    baseline_name = "N/A"

                url_mapping[url].append({
                    'workflow_id': data.get('id', ''),
                    'date': data.get('start_time', '')[:10] if data.get('start_time') else '',
                    'baseline': baseline_name,
                    'marked': os.path.basename(marked) if marked else "N/A"
                })
        except Exception as e:
            print(f"  âš ï¸ å¤„ç†æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")

    # è¾“å‡ºç»Ÿè®¡
    print(f"\næ‰¾åˆ° {len(url_mapping)} ä¸ªä¸åŒçš„è…¾è®¯æ–‡æ¡£ä¸Šä¼ URLï¼š\n")

    for url, records in sorted(url_mapping.items()):
        print(f"  ğŸ”— {url}")
        print(f"     ä½¿ç”¨æ¬¡æ•°: {len(records)}")

        # æ˜¾ç¤ºæœ€è¿‘çš„å‡ æ¬¡ä½¿ç”¨
        recent = sorted(records, key=lambda x: x['date'], reverse=True)[:3]
        for r in recent:
            print(f"     - {r['date']}: {r['baseline']}")
        print()

    return url_mapping

def analyze_baseline_files():
    """åˆ†æåŸºçº¿æ–‡ä»¶"""
    print("\n" + "=" * 80)
    print("ğŸ“ åŸºçº¿æ–‡ä»¶åˆ†æ")
    print("=" * 80)

    # æŸ¥æ‰¾æ‰€æœ‰åŸºçº¿æ–‡ä»¶
    baseline_patterns = [
        "csv_versions/**/baseline/*.csv",
        "csv_versions/**/baseline/.deleted/*.csv",
        "csv_versions/**/baseline/backup_wrong/*.csv"
    ]

    all_baselines = []
    for pattern in baseline_patterns:
        all_baselines.extend(glob.glob(pattern, recursive=True))

    # æŒ‰å‘¨æ•°åˆ†ç»„
    week_files = defaultdict(list)
    for file_path in all_baselines:
        # æå–å‘¨æ•°
        if "W" in file_path:
            parts = file_path.split("/")
            for part in parts:
                if part.startswith("2025_W"):
                    week = part
                    week_files[week].append(file_path)
                    break

    print(f"\næ‰¾åˆ° {len(all_baselines)} ä¸ªåŸºçº¿æ–‡ä»¶ï¼Œåˆ†å¸ƒåœ¨ {len(week_files)} ä¸ªå‘¨ï¼š\n")

    for week in sorted(week_files.keys(), reverse=True)[:5]:  # æ˜¾ç¤ºæœ€è¿‘5å‘¨
        files = week_files[week]
        print(f"  ğŸ“… {week}: {len(files)} ä¸ªæ–‡ä»¶")

        # æ˜¾ç¤ºè¯¥å‘¨çš„æ–‡ä»¶
        for file_path in files[:5]:  # æ¯å‘¨æœ€å¤šæ˜¾ç¤º5ä¸ª
            file_name = os.path.basename(file_path)
            status = ""
            if ".deleted" in file_path:
                status = " [å·²åˆ é™¤]"
            elif "backup_wrong" in file_path:
                status = " [é”™è¯¯å¤‡ä»½]"
            print(f"     - {file_name}{status}")

        if len(files) > 5:
            print(f"     ... è¿˜æœ‰ {len(files) - 5} ä¸ªæ–‡ä»¶")
        print()

    return week_files

def analyze_document_links():
    """åˆ†ææ–‡æ¡£é“¾æ¥é…ç½®"""
    print("\n" + "=" * 80)
    print("ğŸ”— æ–‡æ¡£é“¾æ¥é…ç½®åˆ†æ")
    print("=" * 80)

    link_files = [
        "uploads/document_links.json",
        "uploads/business_document_links.json"
    ]

    for file_path in link_files:
        if os.path.exists(file_path):
            print(f"\nğŸ“„ {file_path}:")

            with open(file_path, 'r', encoding='utf-8') as f:
                links = json.load(f)

            # ç»Ÿè®¡çœŸå®çš„è…¾è®¯æ–‡æ¡£URL
            real_urls = []
            local_files = []
            placeholders = []

            for name, info in links.items():
                link = info.get('tencent_link', '')
                if link.startswith("https://docs.qq.com"):
                    real_urls.append((name, link))
                elif link.startswith("/uploads/") or link.startswith("/"):
                    local_files.append(name)
                elif "PLACEHOLDER" in link:
                    placeholders.append(name)

            print(f"  - çœŸå®è…¾è®¯æ–‡æ¡£URL: {len(real_urls)}")
            print(f"  - æœ¬åœ°æ–‡ä»¶: {len(local_files)}")
            print(f"  - å ä½ç¬¦: {len(placeholders)}")

            if real_urls:
                print(f"\n  çœŸå®URLåˆ—è¡¨:")
                for name, url in real_urls[:5]:
                    print(f"    â€¢ {name}: {url}")

def create_comprehensive_report():
    """åˆ›å»ºç»¼åˆæŠ¥å‘Š"""
    print("\n" + "=" * 80)
    print("ğŸ“Š ç»¼åˆåˆ†ææŠ¥å‘Š")
    print("=" * 80)

    print("""
æ·±åº¦åˆ†æç»“æœï¼š

1. **çœŸå®æ–‡æ¡£æºï¼ˆreal_documents.jsonï¼‰**
   - å‰¯æœ¬-æµ‹è¯•ç‰ˆæœ¬-å‡ºå›½é”€å”®è®¡åˆ’è¡¨
     URL: https://docs.qq.com/sheet/DWEFNU25TemFnZXJN
   - å‰¯æœ¬-æµ‹è¯•ç‰ˆæœ¬-å›å›½é”€å”®è®¡åˆ’è¡¨
     URL: https://docs.qq.com/sheet/DWGZDZkxpaGVQaURr
   - æµ‹è¯•ç‰ˆæœ¬-å°çº¢ä¹¦éƒ¨é—¨
     URL: https://docs.qq.com/sheet/DWFJzdWNwd0RGbU5R

2. **å·²ä¸Šä¼ çš„æ¶‚è‰²æ–‡æ¡£ï¼ˆworkflowå†å²ï¼‰**
   ç³»ç»Ÿå·²æˆåŠŸä¸Šä¼ æ¶‚è‰²Excelæ–‡ä»¶åˆ°ä»¥ä¸‹è…¾è®¯æ–‡æ¡£URLï¼š
   - https://docs.qq.com/sheet/DWE1yRmp0WVBGb29p (æœ€æ–°)
   - https://docs.qq.com/sheet/DWE53dWVaSEdtdUxy
   - https://docs.qq.com/sheet/DWEVZcWtpWE9OR21U
   - https://docs.qq.com/sheet/DWFNmc1FXUWxwendh
   - https://docs.qq.com/sheet/DWFpmeXBOWE5OcG93
   - https://docs.qq.com/sheet/DWG52TkVjSUdrV1B4
   - https://docs.qq.com/sheet/DWGxmd09BR1NpcnFX
   - https://docs.qq.com/sheet/DWHpOdVVKU0VmSXJv
   - https://docs.qq.com/sheet/DWHVwR2NxcGdJY01B

3. **åŸºçº¿æ–‡ä»¶å­˜å‚¨**
   ç³»ç»Ÿä¿å­˜äº†å¤šä¸ªå‘¨æœŸçš„åŸºçº¿æ–‡ä»¶ï¼š
   - W39: å‡ºå›½é”€å”®è®¡åˆ’è¡¨ã€å›å›½é”€å”®è®¡åˆ’è¡¨ã€å°çº¢ä¹¦éƒ¨é—¨
   - W38: å‡ºå›½é”€å”®è®¡åˆ’è¡¨ã€å°çº¢ä¹¦éƒ¨é—¨
   - W36: æµ‹è¯•æ–‡æ¡£å’Œå‰¯æœ¬æ–‡æ¡£
   - W34: é€šç”¨CSVåŸºçº¿

4. **ç³»ç»ŸçœŸå®æ€§éªŒè¯**
   âœ… ç³»ç»Ÿç¡®å®åœ¨å¤„ç†çœŸå®çš„è…¾è®¯æ–‡æ¡£
   âœ… æœ‰å®Œæ•´çš„ä¸‹è½½ã€æ¯”å¯¹ã€æ¶‚è‰²ã€ä¸Šä¼ æµç¨‹
   âœ… ä¿ç•™äº†å†å²åŸºçº¿æ–‡ä»¶ç”¨äºç‰ˆæœ¬å¯¹æ¯”
   âœ… å·¥ä½œæµè®°å½•æ˜¾ç¤ºå¤šæ¬¡æˆåŠŸä¸Šä¼ åˆ°è…¾è®¯æ–‡æ¡£

ç»“è®ºï¼šè¿™æ˜¯ä¸€ä¸ªçœŸå®çš„ä¼ä¸šæ–‡æ¡£ç›‘æ§ç³»ç»Ÿï¼Œä¸æ˜¯è™šæ‹Ÿæµ‹è¯•æ•°æ®ã€‚
""")

def main():
    """ä¸»å‡½æ•°"""
    print("=" * 80)
    print("ğŸ” è…¾è®¯æ–‡æ¡£æ™ºèƒ½ç›‘æ§ç³»ç»Ÿ - çœŸå®URLæ·±åº¦åˆ†æ")
    print("=" * 80)

    # æ‰§è¡Œå„é¡¹åˆ†æ
    real_docs = analyze_real_documents()
    url_mapping = analyze_workflow_history()
    week_files = analyze_baseline_files()
    analyze_document_links()

    # åˆ›å»ºç»¼åˆæŠ¥å‘Š
    create_comprehensive_report()

    print("\n" + "=" * 80)
    print("åˆ†æå®Œæˆï¼")
    print("=" * 80)

if __name__ == "__main__":
    main()