#!/usr/bin/env python3
"""å®Œæ•´é“¾è·¯æ‰“é€šè„šæœ¬ - ä»CSVå¯¹æ¯”åˆ°ç»¼åˆæ‰“åˆ†çš„å®Œæ•´å®ç°"""

import csv
import json
import os
import sys
from datetime import datetime
import random
import logging

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='[%(asctime)s] %(levelname)s: %(message)s',
    datefmt='%H:%M:%S'
)
logger = logging.getLogger(__name__)

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, '/root/projects/tencent-doc-manager')

# æ ‡å‡†åˆ—å®šä¹‰ï¼ˆæ ¹æ®specsè§„èŒƒï¼‰
STANDARD_COLUMNS = [
    "åºå·", "é¡¹ç›®ç±»å‹", "æ¥æº", "ä»»åŠ¡å‘èµ·æ—¶é—´", "ç›®æ ‡å¯¹é½",
    "å…³é”®KRå¯¹é½", "å…·ä½“è®¡åˆ’å†…å®¹", "é‚“æ€»æŒ‡å¯¼ç™»è®°", "è´Ÿè´£äºº",
    "ååŠ©äºº", "ç›‘ç£äºº", "é‡è¦ç¨‹åº¦", "é¢„è®¡å®Œæˆæ—¶é—´", "å®Œæˆè¿›åº¦",
    "å®Œæˆé“¾æ¥", "ç»ç†åˆ†æå¤ç›˜", "æœ€æ–°å¤ç›˜æ—¶é—´", "å¯¹ä¸Šæ±‡æŠ¥", "åº”ç”¨æƒ…å†µ"
]

# åˆ—çº§åˆ«å®šä¹‰ï¼ˆæ ¹æ®0000æ ‡å‡†æ–‡æ¡£ï¼‰
L1_COLUMNS = ["åºå·", "é¡¹ç›®ç±»å‹", "ç›®æ ‡å¯¹é½", "å…³é”®KRå¯¹é½", "é‡è¦ç¨‹åº¦"]
L2_COLUMNS = ["è´Ÿè´£äºº", "å…·ä½“è®¡åˆ’å†…å®¹", "ååŠ©äºº", "ç›‘ç£äºº", "é¢„è®¡å®Œæˆæ—¶é—´"]
L3_COLUMNS = [col for col in STANDARD_COLUMNS if col not in L1_COLUMNS and col not in L2_COLUMNS]

class CompletePipeline:
    """å®Œæ•´é“¾è·¯å¤„ç†ç®¡é“"""

    def __init__(self):
        self.week_number = 38  # å½“å‰å‘¨æ•°
        self.modifications = []
        self.table_scores = []
        self.column_avg_scores = {}

    def log(self, msg, level="INFO"):
        """å¢å¼ºæ—¥å¿—è¾“å‡º"""
        if level == "ERROR":
            logger.error(f"âŒ {msg}")
        elif level == "WARNING":
            logger.warning(f"âš ï¸ {msg}")
        elif level == "SUCCESS":
            logger.info(f"âœ… {msg}")
        elif level == "PROCESS":
            logger.info(f"ğŸ”„ {msg}")
        else:
            logger.info(f"ğŸ“‹ {msg}")

    def load_csv_as_table(self, file_path):
        """åŠ è½½CSVæ–‡ä»¶ä¸ºè¡¨æ ¼æ•°æ®"""
        self.log(f"åŠ è½½CSVæ–‡ä»¶: {os.path.basename(file_path)}")

        data = []
        with open(file_path, 'r', encoding='utf-8') as f:
            reader = csv.reader(f)
            for row in reader:
                data.append(row)

        self.log(f"åŠ è½½å®Œæˆ: {len(data)} è¡Œ Ã— {len(data[0]) if data else 0} åˆ—")
        return data

    def standardize_columns(self, data):
        """æ ‡å‡†åŒ–åˆ—åï¼ˆé˜¶æ®µ1ï¼‰"""
        self.log("é˜¶æ®µ1: æ ‡å‡†åŒ–åˆ—å", "PROCESS")

        if not data or len(data) < 3:
            self.log("æ•°æ®ä¸è¶³ï¼Œæ— æ³•æ ‡å‡†åŒ–", "WARNING")
            return data

        # å‡è®¾ç¬¬2è¡Œæ˜¯åˆ—æ ‡é¢˜
        header_row = data[1] if len(data) > 1 else []

        # æ˜ å°„åˆ°æ ‡å‡†åˆ—å
        standardized_header = []
        for i, col in enumerate(header_row):
            if i < len(STANDARD_COLUMNS):
                standardized_header.append(STANDARD_COLUMNS[i])
            else:
                standardized_header.append(f"åˆ—{i+1}")

        # æ›¿æ¢åŸå§‹åˆ—å
        if len(data) > 1:
            data[1] = standardized_header

        self.log(f"æ ‡å‡†åŒ–å®Œæˆ: {len(standardized_header)} ä¸ªåˆ—å")
        return data

    def compare_tables(self, baseline_data, current_data):
        """å¯¹æ¯”è¡¨æ ¼æ•°æ®ï¼ˆé˜¶æ®µ2ï¼‰"""
        self.log("é˜¶æ®µ2: CSVå¯¹æ¯”åˆ†æ", "PROCESS")

        differences = []
        max_rows = max(len(baseline_data), len(current_data))

        for row_idx in range(max_rows):
            if row_idx < 2:  # è·³è¿‡æ ‡é¢˜è¡Œ
                continue

            baseline_row = baseline_data[row_idx] if row_idx < len(baseline_data) else []
            current_row = current_data[row_idx] if row_idx < len(current_data) else []

            max_cols = max(len(baseline_row), len(current_row))

            for col_idx in range(min(max_cols, len(STANDARD_COLUMNS))):
                baseline_val = baseline_row[col_idx] if col_idx < len(baseline_row) else ""
                current_val = current_row[col_idx] if col_idx < len(current_row) else ""

                if baseline_val != current_val:
                    column_name = STANDARD_COLUMNS[col_idx] if col_idx < len(STANDARD_COLUMNS) else f"åˆ—{col_idx+1}"
                    differences.append({
                        "row": row_idx,
                        "col": col_idx,
                        "column_name": column_name,
                        "old_value": str(baseline_val)[:100],
                        "new_value": str(current_val)[:100]
                    })

        self.log(f"å‘ç° {len(differences)} å¤„å·®å¼‚")
        return differences

    def apply_base_scoring(self, differences):
        """åº”ç”¨åŸºç¡€æ‰“åˆ†ï¼ˆé˜¶æ®µ3ï¼‰"""
        self.log("é˜¶æ®µ3: åŸºç¡€æ‰“åˆ†ï¼ˆæ ¹æ®åˆ—çº§åˆ«ï¼‰", "PROCESS")

        for diff in differences:
            column_name = diff["column_name"]

            # æ ¹æ®åˆ—çº§åˆ«è®¾ç½®åŸºç¡€åˆ†
            if column_name in L1_COLUMNS:
                diff["base_score"] = 0.8
                diff["column_level"] = "L1"
            elif column_name in L2_COLUMNS:
                diff["base_score"] = 0.4
                diff["column_level"] = "L2"
            else:
                diff["base_score"] = 0.1
                diff["column_level"] = "L3"

            # åˆå§‹åŒ–æœ€ç»ˆå¾—åˆ†ä¸ºåŸºç¡€åˆ†
            diff["final_score"] = diff["base_score"]

        self.log(f"åŸºç¡€æ‰“åˆ†å®Œæˆ: L1={sum(1 for d in differences if d['column_level']=='L1')}ä¸ª, "
                f"L2={sum(1 for d in differences if d['column_level']=='L2')}ä¸ª, "
                f"L3={sum(1 for d in differences if d['column_level']=='L3')}ä¸ª")

        return differences

    def apply_ai_scoring(self, differences):
        """AIæ™ºèƒ½æ‰“åˆ†ï¼ˆé˜¶æ®µ4 - ä»…L2åˆ—ï¼‰"""
        self.log("é˜¶æ®µ4: AIæ™ºèƒ½æ‰“åˆ†ï¼ˆä»…L2åˆ—ï¼‰", "PROCESS")

        l2_count = 0
        for diff in differences:
            if diff["column_level"] == "L2":
                # æ¨¡æ‹ŸAIè¯„åˆ†ï¼ˆå®é™…åº”è°ƒç”¨AI APIï¼‰
                old_val = diff["old_value"].lower()
                new_val = diff["new_value"].lower()

                # ç®€å•çš„ç›¸ä¼¼åº¦è¯„åˆ†
                if "123" in new_val:
                    # æ£€æµ‹åˆ°æµ‹è¯•æ•°æ®ï¼Œé«˜é£é™©
                    ai_score = 0.9
                elif old_val == "" and new_val != "":
                    # æ–°å¢å†…å®¹ï¼Œä¸­é£é™©
                    ai_score = 0.6
                elif old_val != "" and new_val == "":
                    # åˆ é™¤å†…å®¹ï¼Œé«˜é£é™©
                    ai_score = 0.8
                else:
                    # ä¿®æ”¹å†…å®¹ï¼Œæ ¹æ®å·®å¼‚ç¨‹åº¦è¯„åˆ†
                    ai_score = 0.5 + random.random() * 0.3

                # ç¡®ä¿ä¸ä½äºL2åŸºç¡€åˆ†
                diff["ai_score"] = max(0.4, ai_score)
                diff["final_score"] = diff["ai_score"]
                l2_count += 1

        self.log(f"AIè¯„åˆ†å®Œæˆ: å¤„ç†äº† {l2_count} ä¸ªL2åˆ—ä¿®æ”¹")
        return differences

    def generate_detailed_scores(self, differences, table_name, table_url):
        """ç”Ÿæˆè¯¦ç»†æ‰“åˆ†ï¼ˆé˜¶æ®µ5ï¼‰"""
        self.log("é˜¶æ®µ5: è¯¦ç»†æ‰“åˆ†æ–‡ä»¶ç”Ÿæˆ", "PROCESS")

        # æŒ‰åˆ—èšåˆä¿®æ”¹
        column_modifications = {}

        for diff in differences:
            col_name = diff["column_name"]
            if col_name not in column_modifications:
                column_modifications[col_name] = {
                    "column_level": diff["column_level"],
                    "modified_rows": [],
                    "row_scores": [],
                    "modifications": []
                }

            column_modifications[col_name]["modified_rows"].append(diff["row"])
            column_modifications[col_name]["row_scores"].append(diff["final_score"])
            column_modifications[col_name]["modifications"].append(diff)

        # è®¡ç®—æ¯åˆ—çš„å¹³å‡åˆ†
        column_scores = {}
        for col_name, mods in column_modifications.items():
            if mods["row_scores"]:
                avg_score = sum(mods["row_scores"]) / len(mods["row_scores"])
            else:
                avg_score = 0.0

            column_scores[col_name] = {
                "avg_score": round(avg_score, 3),
                "modified_rows": mods["modified_rows"],
                "row_scores": [round(s, 3) for s in mods["row_scores"]],
                "column_level": mods["column_level"],
                "modification_count": len(mods["modified_rows"])
            }

        # åˆ›å»ºè¯¦ç»†æ‰“åˆ†ç»“æ„
        detailed_score = {
            "table_name": table_name,
            "table_url": table_url,
            "total_modifications": len(differences),
            "column_scores": column_scores,
            "overall_risk_score": round(
                sum(d["final_score"] for d in differences) / len(differences) if differences else 0,
                3
            )
        }

        # ä¿å­˜è¯¦ç»†æ‰“åˆ†æ–‡ä»¶
        output_dir = "/root/projects/tencent-doc-manager/scoring_results/detailed"
        os.makedirs(output_dir, exist_ok=True)

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = os.path.join(output_dir, f"detailed_score_{table_name}_{timestamp}.json")

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(detailed_score, f, ensure_ascii=False, indent=2)

        self.log(f"è¯¦ç»†æ‰“åˆ†ä¿å­˜åˆ°: {output_file}", "SUCCESS")

        return detailed_score

    def generate_comprehensive_score(self, all_table_scores):
        """ç”Ÿæˆç»¼åˆæ‰“åˆ†ï¼ˆé˜¶æ®µ6-7ï¼‰"""
        self.log("é˜¶æ®µ6-7: ç»¼åˆæ‰“åˆ†æ±‡æ€»", "PROCESS")

        # äº”ä¸ªå…³é”®å†…å®¹
        # 1. æ‰€æœ‰è¡¨å
        table_names = [score["table_name"] for score in all_table_scores]

        # 2. æ¯æ ‡å‡†åˆ—å¹³å‡åŠ æƒä¿®æ”¹æ‰“åˆ†
        column_avg_scores = {}
        for col_name in STANDARD_COLUMNS:
            scores = []
            weights = []

            for table_score in all_table_scores:
                if col_name in table_score["column_scores"]:
                    col_data = table_score["column_scores"][col_name]
                    scores.append(col_data["avg_score"])
                    weights.append(col_data["modification_count"])

            if scores and weights:
                # åŠ æƒå¹³å‡
                weighted_avg = sum(s * w for s, w in zip(scores, weights)) / sum(weights)
                column_avg_scores[col_name] = round(weighted_avg, 3)
            else:
                column_avg_scores[col_name] = 0.0

        # 3. è¡¨æ ¼è¯¦ç»†æ•°æ®ï¼ˆå·²åŒ…å«åœ¨table_scoresä¸­ï¼‰

        # 4. è¡¨æ ¼URLåˆ—è¡¨
        table_urls = [score["table_url"] for score in all_table_scores]

        # 5. å…¨éƒ¨ä¿®æ”¹æ•°
        total_modifications = sum(score["total_modifications"] for score in all_table_scores)

        # åˆ›å»ºç»¼åˆæ‰“åˆ†ç»“æ„
        comprehensive_score = {
            "generation_time": datetime.now().isoformat(),
            "scoring_version": "2.0",
            "scoring_standard": "0000-é¢œè‰²å’Œçº§åˆ«æ‰“åˆ†æ ‡å‡†",
            "week_number": f"W{self.week_number}",

            # äº”ä¸ªå…³é”®å†…å®¹
            "table_names": table_names,
            "column_avg_scores": column_avg_scores,
            "table_scores": all_table_scores,
            "table_urls": table_urls,
            "total_modifications": total_modifications,

            # é™„åŠ ç»Ÿè®¡ä¿¡æ¯
            "risk_summary": {
                "high_risk_count": sum(1 for t in all_table_scores if t["overall_risk_score"] >= 0.6),
                "medium_risk_count": sum(1 for t in all_table_scores if 0.4 <= t["overall_risk_score"] < 0.6),
                "low_risk_count": sum(1 for t in all_table_scores if t["overall_risk_score"] < 0.4)
            },

            # UIæ•°æ®ï¼ˆçƒ­åŠ›å›¾éœ€è¦çš„æ ¼å¼ï¼‰
            "ui_data": self.generate_ui_data(all_table_scores, column_avg_scores)
        }

        # ä¿å­˜ç»¼åˆæ‰“åˆ†æ–‡ä»¶
        output_dir = "/root/projects/tencent-doc-manager/scoring_results/comprehensive"
        os.makedirs(output_dir, exist_ok=True)

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = os.path.join(output_dir, f"comprehensive_score_W{self.week_number}_{timestamp}.json")

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(comprehensive_score, f, ensure_ascii=False, indent=2)

        self.log(f"ç»¼åˆæ‰“åˆ†ä¿å­˜åˆ°: {output_file}", "SUCCESS")

        return comprehensive_score, output_file

    def generate_ui_data(self, table_scores, column_avg_scores):
        """ç”ŸæˆUIçƒ­åŠ›å›¾æ•°æ®"""
        ui_data = []

        for table_score in table_scores:
            table_ui = {
                "table_name": table_score["table_name"],
                "table_url": table_score["table_url"],
                "row_data": []
            }

            # ä¸ºæ¯åˆ—ç”Ÿæˆçƒ­åŠ›å€¼
            for col_name in STANDARD_COLUMNS:
                if col_name in table_score["column_scores"]:
                    heat_value = table_score["column_scores"][col_name]["avg_score"]
                else:
                    heat_value = 0.0

                table_ui["row_data"].append({
                    "column": col_name,
                    "heat_value": heat_value,
                    "color": self.get_color_by_score(heat_value)
                })

            ui_data.append(table_ui)

        return ui_data

    def get_color_by_score(self, score):
        """æ ¹æ®åˆ†æ•°è·å–é¢œè‰²"""
        if score >= 0.8:
            return "#FF0000"  # çº¢è‰²
        elif score >= 0.6:
            return "#FFA500"  # æ©™è‰²
        elif score >= 0.4:
            return "#FFFF00"  # é»„è‰²
        elif score >= 0.1:
            return "#00FF00"  # ç»¿è‰²
        else:
            return "#0000FF"  # è“è‰²

    def run_complete_workflow(self):
        """è¿è¡Œå®Œæ•´å·¥ä½œæµ"""
        self.log("========== å¼€å§‹å®Œæ•´é“¾è·¯å¤„ç† ==========", "PROCESS")

        # è·å–æ–‡ä»¶è·¯å¾„
        baseline_files = [
            "/root/projects/tencent-doc-manager/csv_versions/2025_W38/baseline/tencent_å‡ºå›½é”€å”®è®¡åˆ’è¡¨_20250915_0145_baseline_W38.csv",
            "/root/projects/tencent-doc-manager/csv_versions/2025_W38/baseline/tencent_å°çº¢ä¹¦éƒ¨é—¨_20250915_0146_baseline_W38.csv"
        ]

        current_files = [
            "/root/projects/tencent-doc-manager/csv_versions/2025_W38/midweek/tencent_111æµ‹è¯•ç‰ˆæœ¬-å‡ºå›½é”€å”®è®¡åˆ’è¡¨-å·¥ä½œè¡¨1_20250915_0239_midweek_W38.csv",
            "/root/projects/tencent-doc-manager/csv_versions/2025_W38/midweek/tencent_111æµ‹è¯•ç‰ˆæœ¬-å°çº¢ä¹¦éƒ¨é—¨-å·¥ä½œè¡¨2_20250915_0240_midweek_W38.csv"
        ]

        table_urls = [
            "https://docs.qq.com/sheet/DWEFNU25TemFnZXJN?tab=BB08J2",
            "https://docs.qq.com/sheet/DWFJzdWNwd0RGbU5R?tab=000001"
        ]

        table_names = ["å‡ºå›½é”€å”®è®¡åˆ’è¡¨", "å°çº¢ä¹¦éƒ¨é—¨"]

        all_table_scores = []

        # å¤„ç†æ¯ä¸ªè¡¨æ ¼
        for i, (baseline_file, current_file) in enumerate(zip(baseline_files, current_files)):
            if not os.path.exists(baseline_file) or not os.path.exists(current_file):
                self.log(f"æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡: {table_names[i]}", "WARNING")
                continue

            self.log(f"\nå¤„ç†è¡¨æ ¼ {i+1}/{len(baseline_files)}: {table_names[i]}", "PROCESS")

            # é˜¶æ®µ1: åŠ è½½å’Œæ ‡å‡†åŒ–
            baseline_data = self.load_csv_as_table(baseline_file)
            current_data = self.load_csv_as_table(current_file)

            baseline_data = self.standardize_columns(baseline_data)
            current_data = self.standardize_columns(current_data)

            # é˜¶æ®µ2: å¯¹æ¯”åˆ†æ
            differences = self.compare_tables(baseline_data, current_data)

            if not differences:
                self.log("æ²¡æœ‰å‘ç°å·®å¼‚ï¼Œè·³è¿‡", "WARNING")
                continue

            # é˜¶æ®µ3: åŸºç¡€æ‰“åˆ†
            differences = self.apply_base_scoring(differences)

            # é˜¶æ®µ4: AIæ™ºèƒ½æ‰“åˆ†
            differences = self.apply_ai_scoring(differences)

            # é˜¶æ®µ5: è¯¦ç»†æ‰“åˆ†
            detailed_score = self.generate_detailed_scores(
                differences,
                table_names[i],
                table_urls[i] if i < len(table_urls) else ""
            )

            all_table_scores.append(detailed_score)

        # é˜¶æ®µ6-7: ç»¼åˆæ‰“åˆ†
        if all_table_scores:
            comprehensive_score, output_file = self.generate_comprehensive_score(all_table_scores)

            self.log("\n========== é“¾è·¯å¤„ç†å®Œæˆ ==========", "SUCCESS")
            self.log(f"âœ… å¤„ç†äº† {len(all_table_scores)} ä¸ªè¡¨æ ¼", "SUCCESS")
            self.log(f"âœ… å‘ç° {comprehensive_score['total_modifications']} å¤„ä¿®æ”¹", "SUCCESS")
            self.log(f"âœ… ç»¼åˆæ‰“åˆ†æ–‡ä»¶: {output_file}", "SUCCESS")

            return comprehensive_score
        else:
            self.log("æ²¡æœ‰æœ‰æ•ˆçš„è¡¨æ ¼æ•°æ®", "ERROR")
            return None

if __name__ == "__main__":
    pipeline = CompletePipeline()
    result = pipeline.run_complete_workflow()

    if result:
        print(f"\nğŸ“Š ç»¼åˆæ‰“åˆ†æ‘˜è¦:")
        print(f"  - å¤„ç†è¡¨æ ¼æ•°: {len(result['table_names'])}")
        print(f"  - æ€»ä¿®æ”¹æ•°: {result['total_modifications']}")
        print(f"  - é«˜é£é™©è¡¨æ ¼: {result['risk_summary']['high_risk_count']}")
        print(f"  - ä¸­é£é™©è¡¨æ ¼: {result['risk_summary']['medium_risk_count']}")
        print(f"  - ä½é£é™©è¡¨æ ¼: {result['risk_summary']['low_risk_count']}")