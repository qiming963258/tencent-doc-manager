#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è…¾è®¯æ–‡æ¡£ç›‘æ§ç³»ç»Ÿ - ç‹¬ç«‹åç«¯APIæœåŠ¡
æä¾›å®Œæ•´çš„RESTful APIæ¥å£ä¾›å‰ç«¯è°ƒç”¨
"""

from flask import Flask, request, jsonify, send_file
from flask_cors import CORS
import os
import sys
import json
import time
import threading
import traceback
from datetime import datetime
from pathlib import Path
from queue import Queue
import logging

# å¯¼å…¥æ ¸å¿ƒåŠŸèƒ½æ¨¡å—
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
from production.core_modules.week_time_manager import WeekTimeManager
from production.core_modules.baseline_manager import BaselineManager
from production.core_modules.auto_comparison_task import AutoComparisonTask
from production.core_modules.document_change_analyzer import DocumentChangeAnalyzer
from unified_csv_comparator import UnifiedCSVComparator
from tencent_doc_downloader_new import TencentDocDownloader
from intelligent_excel_marker import IntelligentExcelMarker
from tencent_doc_uploader_ultimate import upload_to_tencent_docs

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# åˆå§‹åŒ–Flaskåº”ç”¨
app = Flask(__name__)
CORS(app, origins="*", supports_credentials=True)

# å…¨å±€çŠ¶æ€ç®¡ç†
class WorkflowState:
    def __init__(self):
        self.status = "idle"
        self.current_task = None
        self.progress = 0
        self.total_steps = 0
        self.logs = []
        self.results = {}
        self.lock = threading.Lock()
        self.error = None

    def reset(self):
        with self.lock:
            self.status = "idle"
            self.current_task = None
            self.progress = 0
            self.total_steps = 0
            self.logs = []
            self.results = {}
            self.error = None

    def add_log(self, message, level="info"):
        with self.lock:
            log_entry = {
                "timestamp": datetime.now().strftime("%H:%M:%S"),
                "message": message,
                "level": level
            }
            self.logs.append(log_entry)
            logger.info(f"[{log_entry['timestamp']}] {message}")

    def update_progress(self, task, progress, total=None):
        with self.lock:
            self.current_task = task
            self.progress = progress
            if total:
                self.total_steps = total

    def set_status(self, status):
        with self.lock:
            self.status = status

    def get_state(self):
        with self.lock:
            return {
                "status": self.status,
                "current_task": self.current_task,
                "progress": self.progress,
                "total_steps": self.total_steps,
                "logs": self.logs[-100:],  # åªè¿”å›æœ€è¿‘100æ¡æ—¥å¿—
                "error": self.error
            }

# å…¨å±€å·¥ä½œæµçŠ¶æ€
workflow_state = WorkflowState()

# æ ¸å¿ƒå·¥ä½œæµå¤„ç†å‡½æ•°
def process_workflow(baseline_url, target_url, cookie, advanced_settings=None):
    """æ‰§è¡Œå®Œæ•´çš„æ–‡æ¡£å¤„ç†å·¥ä½œæµ"""
    try:
        workflow_state.reset()
        workflow_state.set_status("running")
        workflow_state.add_log("ğŸš€ å¼€å§‹æ‰§è¡Œæ–‡æ¡£å¤„ç†å·¥ä½œæµ", "info")

        # åˆå§‹åŒ–ç®¡ç†å™¨
        week_manager = WeekTimeManager()
        baseline_manager = BaselineManager()

        # ç¡®å®šæ–‡ä»¶å­˜å‚¨è·¯å¾„
        current_week = week_manager.get_current_week()
        weekday = datetime.now().weekday()
        hour = datetime.now().hour

        # æ ¹æ®æ—¶é—´å†³å®šç‰ˆæœ¬ç±»å‹
        if weekday == 1:  # Tuesday
            version_type = "baseline"
        elif weekday == 5 and hour >= 19:  # Saturday after 7pm
            version_type = "weekend"
        else:
            version_type = "midweek"

        workflow_state.add_log(f"ğŸ“… å½“å‰å‘¨: W{current_week}, ç‰ˆæœ¬ç±»å‹: {version_type}", "info")

        # æ­¥éª¤1: å¤„ç†åŸºçº¿æ–‡æ¡£
        workflow_state.update_progress("å¤„ç†åŸºçº¿æ–‡æ¡£", 1, 10)

        if baseline_url:
            # ä¸‹è½½æ–°çš„åŸºçº¿æ–‡æ¡£
            workflow_state.add_log("ğŸ“¥ ä¸‹è½½åŸºçº¿æ–‡æ¡£...", "info")
            downloader = TencentDocDownloader()

            # ä»URLæå–æ–‡æ¡£å
            doc_name = extract_doc_name_from_url(baseline_url)
            baseline_filename = week_manager.generate_filename(
                doc_name,
                version_type="baseline",
                extension="csv"
            )
            baseline_path = os.path.join(
                week_manager.get_version_dir(current_week, "baseline"),
                baseline_filename
            )

            # ç¡®ä¿ç›®å½•å­˜åœ¨
            os.makedirs(os.path.dirname(baseline_path), exist_ok=True)

            # ä¸‹è½½æ–‡æ¡£
            success = downloader.download_as_csv(
                baseline_url,
                baseline_path,
                cookie
            )

            if not success:
                raise Exception("åŸºçº¿æ–‡æ¡£ä¸‹è½½å¤±è´¥")

            workflow_state.add_log(f"âœ… åŸºçº¿æ–‡æ¡£å·²ä¿å­˜: {baseline_filename}", "success")
        else:
            # ä½¿ç”¨ç°æœ‰åŸºçº¿
            workflow_state.add_log("ğŸ“‚ ä½¿ç”¨ç°æœ‰åŸºçº¿æ–‡æ¡£", "info")
            doc_name = extract_doc_name_from_url(target_url)
            baseline_path = baseline_manager.get_latest_baseline(doc_name)

            if not baseline_path:
                raise Exception(f"æœªæ‰¾åˆ°æ–‡æ¡£ {doc_name} çš„åŸºçº¿æ–‡ä»¶")

            workflow_state.add_log(f"âœ… æ‰¾åˆ°åŸºçº¿æ–‡æ¡£: {os.path.basename(baseline_path)}", "success")

        # æ­¥éª¤2: ä¸‹è½½ç›®æ ‡æ–‡æ¡£
        workflow_state.update_progress("ä¸‹è½½ç›®æ ‡æ–‡æ¡£", 2, 10)
        workflow_state.add_log("ğŸ“¥ ä¸‹è½½ç›®æ ‡æ–‡æ¡£...", "info")

        downloader = TencentDocDownloader()
        target_filename = week_manager.generate_filename(
            doc_name,
            version_type=version_type,
            extension="csv"
        )
        target_path = os.path.join(
            week_manager.get_version_dir(current_week, version_type),
            target_filename
        )

        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(target_path), exist_ok=True)

        # ä¸‹è½½æ–‡æ¡£
        success = downloader.download_as_csv(
            target_url,
            target_path,
            cookie,
            force_download=True  # æ€»æ˜¯å¼ºåˆ¶ä¸‹è½½æœ€æ–°ç‰ˆæœ¬
        )

        if not success:
            raise Exception("ç›®æ ‡æ–‡æ¡£ä¸‹è½½å¤±è´¥")

        workflow_state.add_log(f"âœ… ç›®æ ‡æ–‡æ¡£å·²ä¿å­˜: {target_filename}", "success")

        # æ­¥éª¤3: CSVå¯¹æ¯”åˆ†æ
        workflow_state.update_progress("æ‰§è¡ŒCSVå¯¹æ¯”åˆ†æ", 3, 10)
        workflow_state.add_log("ğŸ” å¼€å§‹CSVå¯¹æ¯”åˆ†æ...", "info")

        comparator = UnifiedCSVComparator()
        comparison_result = comparator.compare(baseline_path, target_path)

        total_modifications = comparison_result['statistics']['total_modifications']
        similarity = comparison_result['statistics']['similarity']

        workflow_state.add_log(
            f"ğŸ“Š å¯¹æ¯”å®Œæˆ: ä¿®æ”¹æ•°={total_modifications}, ç›¸ä¼¼åº¦={similarity:.1%}",
            "success"
        )

        # æ­¥éª¤4: ç”Ÿæˆå˜æ›´åˆ†ææŠ¥å‘Š
        workflow_state.update_progress("ç”Ÿæˆå˜æ›´åˆ†ææŠ¥å‘Š", 4, 10)
        workflow_state.add_log("ğŸ“ ç”Ÿæˆå˜æ›´åˆ†ææŠ¥å‘Š...", "info")

        analyzer = DocumentChangeAnalyzer()
        analysis_result = analyzer.analyze_changes(comparison_result)

        workflow_state.add_log(f"âœ… å˜æ›´åˆ†æå®Œæˆ", "success")

        # æ­¥éª¤5: åˆ›å»ºExcelæ ‡è®°æ–‡ä»¶
        workflow_state.update_progress("åˆ›å»ºExcelæ ‡è®°æ–‡ä»¶", 5, 10)
        workflow_state.add_log("ğŸ“‘ åˆ›å»ºExcelæ ‡è®°æ–‡ä»¶...", "info")

        marker = IntelligentExcelMarker()
        excel_filename = target_filename.replace('.csv', '_marked.xlsx')
        excel_path = os.path.join(
            week_manager.get_version_dir(current_week, version_type),
            excel_filename
        )

        marker.create_marked_excel(
            target_path,
            comparison_result,
            analysis_result,
            excel_path
        )

        workflow_state.add_log(f"âœ… Excelæ–‡ä»¶å·²åˆ›å»º: {excel_filename}", "success")

        # æ­¥éª¤6: ä¸Šä¼ åˆ°è…¾è®¯æ–‡æ¡£ï¼ˆå¯é€‰ï¼‰
        if advanced_settings and advanced_settings.get('auto_upload'):
            workflow_state.update_progress("ä¸Šä¼ åˆ°è…¾è®¯æ–‡æ¡£", 6, 10)
            workflow_state.add_log("â˜ï¸ ä¸Šä¼ åˆ°è…¾è®¯æ–‡æ¡£...", "info")

            upload_result = upload_to_tencent_docs(excel_path, cookie)

            if upload_result['success']:
                workflow_state.add_log(
                    f"âœ… å·²ä¸Šä¼ åˆ°è…¾è®¯æ–‡æ¡£: {upload_result.get('url', 'N/A')}",
                    "success"
                )
            else:
                workflow_state.add_log(
                    f"âš ï¸ ä¸Šä¼ å¤±è´¥: {upload_result.get('error', 'æœªçŸ¥é”™è¯¯')}",
                    "warning"
                )

        # å®Œæˆ
        workflow_state.update_progress("å®Œæˆ", 10, 10)
        workflow_state.set_status("completed")
        workflow_state.add_log("ğŸ‰ å·¥ä½œæµæ‰§è¡Œå®Œæˆ!", "success")

        # ä¿å­˜ç»“æœ
        workflow_state.results = {
            "baseline_path": baseline_path,
            "target_path": target_path,
            "excel_path": excel_path,
            "comparison": comparison_result,
            "analysis": analysis_result
        }

    except Exception as e:
        logger.error(f"å·¥ä½œæµæ‰§è¡Œå¤±è´¥: {str(e)}")
        logger.error(traceback.format_exc())
        workflow_state.error = str(e)
        workflow_state.set_status("error")
        workflow_state.add_log(f"âŒ é”™è¯¯: {str(e)}", "error")

def extract_doc_name_from_url(url):
    """ä»URLæå–æ–‡æ¡£åç§°"""
    # ç®€å•å®ç°ï¼Œå®é™…åº”è¯¥ä»URLæŸ¥è¯¢æ–‡æ¡£ä¿¡æ¯
    if "å‡ºå›½é”€å”®" in url:
        return "å‡ºå›½é”€å”®è®¡åˆ’è¡¨"
    elif "å›å›½é”€å”®" in url:
        return "å›å›½é”€å”®è®¡åˆ’è¡¨"
    elif "å°çº¢ä¹¦" in url:
        return "å°çº¢ä¹¦éƒ¨é—¨"
    else:
        # ä»URLæå–IDä½œä¸ºåç§°
        parts = url.split("/")
        return parts[-1] if parts else "unknown"

# APIè·¯ç”±
@app.route('/api/status', methods=['GET'])
def get_status():
    """è·å–æœåŠ¡çŠ¶æ€"""
    return jsonify({
        "status": "running",
        "version": "2.0",
        "service": "backend-api",
        "timestamp": datetime.now().isoformat()
    })

@app.route('/api/workflow/start', methods=['POST'])
def start_workflow():
    """å¯åŠ¨å·¥ä½œæµ"""
    try:
        data = request.get_json()

        # éªŒè¯å¿…è¦å‚æ•°
        if not data.get('target_url'):
            return jsonify({"error": "ç¼ºå°‘target_urlå‚æ•°"}), 400

        if not data.get('cookie'):
            return jsonify({"error": "ç¼ºå°‘cookieå‚æ•°"}), 400

        # æ£€æŸ¥æ˜¯å¦æœ‰æ­£åœ¨è¿è¡Œçš„å·¥ä½œæµ
        if workflow_state.status == "running":
            return jsonify({"error": "å·²æœ‰å·¥ä½œæµæ­£åœ¨è¿è¡Œ"}), 409

        # åœ¨åå°çº¿ç¨‹ä¸­å¯åŠ¨å·¥ä½œæµ
        thread = threading.Thread(
            target=process_workflow,
            args=(
                data.get('baseline_url'),
                data.get('target_url'),
                data.get('cookie'),
                data.get('advanced_settings', {})
            )
        )
        thread.daemon = True
        thread.start()

        return jsonify({
            "success": True,
            "message": "å·¥ä½œæµå·²å¯åŠ¨"
        })

    except Exception as e:
        logger.error(f"å¯åŠ¨å·¥ä½œæµå¤±è´¥: {str(e)}")
        return jsonify({"error": str(e)}), 500

@app.route('/api/workflow/status', methods=['GET'])
def get_workflow_status():
    """è·å–å·¥ä½œæµçŠ¶æ€"""
    return jsonify(workflow_state.get_state())

@app.route('/api/workflow/logs', methods=['GET'])
def get_workflow_logs():
    """è·å–å·¥ä½œæµæ—¥å¿—"""
    limit = request.args.get('limit', 100, type=int)
    return jsonify({
        "logs": workflow_state.logs[-limit:],
        "total": len(workflow_state.logs)
    })

@app.route('/api/workflow/stop', methods=['POST'])
def stop_workflow():
    """åœæ­¢å·¥ä½œæµ"""
    workflow_state.set_status("stopped")
    workflow_state.add_log("â›” å·¥ä½œæµå·²åœæ­¢", "warning")
    return jsonify({"success": True})

@app.route('/api/baseline/list', methods=['GET'])
def list_baselines():
    """åˆ—å‡ºæ‰€æœ‰åŸºçº¿æ–‡ä»¶"""
    try:
        manager = BaselineManager()
        baselines = manager.list_all_baselines()
        return jsonify({"baselines": baselines})
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route('/api/baseline/upload', methods=['POST'])
def upload_baseline():
    """ä¸Šä¼ åŸºçº¿æ–‡ä»¶"""
    try:
        data = request.get_json()
        url = data.get('url')
        cookie = data.get('cookie')

        if not url or not cookie:
            return jsonify({"error": "ç¼ºå°‘å¿…è¦å‚æ•°"}), 400

        # ä¸‹è½½å¹¶ä¿å­˜ä¸ºåŸºçº¿
        downloader = TencentDocDownloader()
        week_manager = WeekTimeManager()

        doc_name = extract_doc_name_from_url(url)
        filename = week_manager.generate_filename(
            doc_name,
            version_type="baseline",
            extension="csv"
        )

        current_week = week_manager.get_current_week()
        filepath = os.path.join(
            week_manager.get_version_dir(current_week, "baseline"),
            filename
        )

        os.makedirs(os.path.dirname(filepath), exist_ok=True)

        success = downloader.download_as_csv(url, filepath, cookie)

        if success:
            return jsonify({
                "success": True,
                "filename": filename,
                "path": filepath
            })
        else:
            return jsonify({"error": "ä¸‹è½½å¤±è´¥"}), 500

    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route('/api/files/csv/<week>/<version>', methods=['GET'])
def list_csv_files(week, version):
    """åˆ—å‡ºæŒ‡å®šå‘¨å’Œç‰ˆæœ¬çš„CSVæ–‡ä»¶"""
    try:
        week_manager = WeekTimeManager()
        directory = week_manager.get_version_dir(week, version)

        if not os.path.exists(directory):
            return jsonify({"files": []})

        files = []
        for f in os.listdir(directory):
            if f.endswith('.csv'):
                filepath = os.path.join(directory, f)
                stat = os.stat(filepath)
                files.append({
                    "name": f,
                    "size": stat.st_size,
                    "modified": datetime.fromtimestamp(stat.st_mtime).isoformat()
                })

        return jsonify({"files": files})

    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route('/api/comparison/execute', methods=['POST'])
def execute_comparison():
    """æ‰§è¡ŒCSVå¯¹æ¯”"""
    try:
        data = request.get_json()
        baseline_path = data.get('baseline_path')
        target_path = data.get('target_path')

        if not baseline_path or not target_path:
            return jsonify({"error": "ç¼ºå°‘æ–‡ä»¶è·¯å¾„"}), 400

        comparator = UnifiedCSVComparator()
        result = comparator.compare(baseline_path, target_path)

        return jsonify(result)

    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route('/api/health', methods=['GET'])
def health_check():
    """å¥åº·æ£€æŸ¥"""
    return jsonify({
        "status": "healthy",
        "uptime": time.time(),
        "memory_usage": get_memory_usage()
    })

def get_memory_usage():
    """è·å–å†…å­˜ä½¿ç”¨æƒ…å†µ"""
    try:
        import psutil
        process = psutil.Process()
        return {
            "rss": process.memory_info().rss,
            "percent": process.memory_percent()
        }
    except:
        return {}

if __name__ == '__main__':
    print(f"ğŸš€ è…¾è®¯æ–‡æ¡£ç›‘æ§ç³»ç»Ÿåç«¯APIæœåŠ¡")
    print(f"ğŸ“ æœåŠ¡åœ°å€: http://0.0.0.0:8093")
    print(f"ğŸ“Š APIæ–‡æ¡£: http://0.0.0.0:8093/api/status")
    print(f"=" * 50)

    app.run(
        host='0.0.0.0',
        port=8093,
        debug=False,
        threaded=True
    )