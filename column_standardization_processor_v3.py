#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
CSVåˆ—åæ™ºèƒ½æ ‡å‡†åŒ–å¤„ç†å™¨ V3
é€‚é…simplified_csv_comparatorçš„æ–°è¾“å‡ºæ ¼å¼
æ”¯æŒè¶…è¿‡19åˆ—æ—¶çš„æ™ºèƒ½ç­›é€‰
"""

import json
import os
import asyncio
import logging
from typing import Dict, List, Tuple, Any, Optional
from datetime import datetime
from pathlib import Path

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# å¯¼å…¥DeepSeekå®¢æˆ·ç«¯
from deepseek_client import DeepSeekClient

class ColumnStandardizationProcessorV3:
    """åˆ—åæ ‡å‡†åŒ–å¤„ç†å™¨V3 - é€‚é…ç®€åŒ–è¾“å‡ºæ ¼å¼ï¼Œæ”¯æŒæ™ºèƒ½ç­›é€‰"""
    
    def __init__(self, api_key: str):
        """åˆå§‹åŒ–å¤„ç†å™¨"""
        self.api_key = api_key
        self.deepseek_client = DeepSeekClient(api_key)
        
        # 19ä¸ªæ ‡å‡†åˆ—å
        self.standard_columns = [
            "åºå·", "é¡¹ç›®ç±»å‹", "æ¥æº", "ä»»åŠ¡å‘èµ·æ—¶é—´", "ç›®æ ‡å¯¹é½",
            "å…³é”®KRå¯¹é½", "å…·ä½“è®¡åˆ’å†…å®¹", "é‚“æ€»æŒ‡å¯¼ç™»è®°", "è´Ÿè´£äºº",
            "ååŠ©äºº", "ç›‘ç£äºº", "é‡è¦ç¨‹åº¦", "é¢„è®¡å®Œæˆæ—¶é—´", "å®Œæˆè¿›åº¦",
            "å½¢æˆè®¡åˆ’æ¸…å•", "å¤ç›˜æ—¶é—´", "å¯¹ä¸Šæ±‡æŠ¥", "åº”ç”¨æƒ…å†µ", "è¿›åº¦åˆ†ææ€»ç»“"
        ]
        
        logger.info("åˆ—åæ ‡å‡†åŒ–å¤„ç†å™¨V3åˆå§‹åŒ–å®Œæˆ - æ”¯æŒç®€åŒ–æ ¼å¼å’Œæ™ºèƒ½ç­›é€‰")
    
    def extract_columns_from_simplified_comparison(self, comparison_file_path: str) -> Dict[str, str]:
        """
        ä»ç®€åŒ–çš„CSVå¯¹æ¯”ç»“æœä¸­æå–ä¿®æ”¹åˆ—ä¿¡æ¯
        
        Args:
            comparison_file_path: ç®€åŒ–CSVå¯¹æ¯”ç»“æœæ–‡ä»¶è·¯å¾„
            
        Returns:
            column_mapping: {Excelåˆ—æ ‡è¯†: åˆ—å} å¦‚ {"C": "é¡¹ç›®ç±»å‹", "D": "æ¥æº"}
        """
        with open(comparison_file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # ç›´æ¥ä»modified_columnså­—æ®µè·å–
        column_mapping = data.get('modified_columns', {})
        
        logger.info(f"ä»ç®€åŒ–æ ¼å¼æå–åˆ° {len(column_mapping)} ä¸ªä¿®æ”¹åˆ—")
        logger.info(f"åˆ—æ˜ å°„: {column_mapping}")
        
        return column_mapping
    
    def build_smart_standardization_prompt(self, column_mapping: Dict[str, str]) -> str:
        """
        æ„å»ºæ™ºèƒ½æ ‡å‡†åŒ–æç¤ºè¯
        æ”¯æŒè¶…è¿‡19åˆ—æ—¶çš„æ™ºèƒ½ç­›é€‰
        
        Args:
            column_mapping: {Excelåˆ—æ ‡è¯†: åˆ—å} çš„å­—å…¸
        """
        column_count = len(column_mapping)
        
        # åˆ¤æ–­æ˜¯å¦éœ€è¦æ™ºèƒ½ç­›é€‰
        needs_filtering = column_count > 19
        
        prompt = f"""ä½ æ˜¯ä¸“ä¸šçš„CSVåˆ—åæ ‡å‡†åŒ–ä¸“å®¶ã€‚è¯·å°†Excelåˆ—æ ‡è¯†å¯¹åº”çš„åˆ—åæ˜ å°„åˆ°19ä¸ªæ ‡å‡†åˆ—åã€‚

## ğŸ¯ æ ¸å¿ƒä»»åŠ¡
{"ã€æ™ºèƒ½ç­›é€‰æ¨¡å¼ã€‘ä»" + str(column_count) + "ä¸ªä¿®æ”¹åˆ—ä¸­é€‰æ‹©æœ€ç›¸ä¼¼çš„19ä¸ªè¿›è¡Œæ˜ å°„" if needs_filtering else "å°†æ‰€æœ‰ä¿®æ”¹åˆ—æ˜ å°„åˆ°æ ‡å‡†åˆ—å"}

## ğŸ“‹ 19ä¸ªæ ‡å‡†åˆ—åï¼ˆå›ºå®šé¡ºåºï¼‰
{json.dumps(self.standard_columns, ensure_ascii=False, indent=2)}

## ğŸ“ éœ€è¦æ ‡å‡†åŒ–çš„åˆ—ï¼ˆå…±{column_count}ä¸ªä¿®æ”¹åˆ—ï¼‰
"""
        for col_id, col_name in sorted(column_mapping.items()):
            prompt += f"åˆ—{col_id}: {col_name}\n"
        
        if needs_filtering:
            prompt += f"""
## âš ï¸ æ™ºèƒ½ç­›é€‰è§„åˆ™ï¼ˆå½“å‰æœ‰{column_count}ä¸ªåˆ—ï¼Œéœ€ç­›é€‰åˆ°19ä¸ªï¼‰
1. **ä¼˜å…ˆçº§1**: å®Œå…¨åŒ¹é…çš„åˆ—åï¼ˆå¦‚"åºå·"="åºå·"ï¼‰
2. **ä¼˜å…ˆçº§2**: é«˜åº¦ç›¸ä¼¼çš„å˜å¼‚å½¢å¼ï¼ˆå¦‚"ç¼–å·"â†’"åºå·"ï¼‰
3. **ä¼˜å…ˆçº§3**: è¯­ä¹‰ç›¸å…³çš„åˆ—ï¼ˆå¦‚"æ‰§è¡Œäºº"â†’"è´Ÿè´£äºº"ï¼‰
4. **ä¼˜å…ˆçº§4**: é‡è¦çš„ä¸šåŠ¡åˆ—ï¼ˆL1å’ŒL2çº§åˆ«çš„åˆ—ï¼‰
5. **ä¸¢å¼ƒåŸåˆ™**: 
   - æ— æ³•æ˜ å°„åˆ°ä»»ä½•æ ‡å‡†åˆ—çš„
   - ç›¸ä¼¼åº¦æä½çš„
   - å¤šä¸ªç±»ä¼¼åˆ—æ—¶é€‰æ‹©æœ€ç›¸ä¼¼çš„ä¸€ä¸ª

## ğŸ¯ ç­›é€‰è¦æ±‚
- å¿…é¡»é€‰å‡ºæ°å¥½19ä¸ªæœ€ç›¸ä¼¼çš„åˆ—è¿›è¡Œæ˜ å°„
- æ¯ä¸ªæ ‡å‡†åˆ—æœ€å¤šæ˜ å°„ä¸€ä¸ªå®é™…åˆ—
- è®°å½•è¢«ä¸¢å¼ƒçš„åˆ—åŠåŸå› 
"""
        
        prompt += f"""
## ğŸ“Š è¾“å‡ºæ ¼å¼ï¼ˆä¸¥æ ¼JSONï¼‰
{{
    "success": true,
    "column_mapping": {{
        // Excelåˆ—æ ‡è¯† â†’ æ ‡å‡†åˆ—åçš„æ˜ å°„
        "C": "æ˜ å°„åçš„æ ‡å‡†åˆ—å",
        "D": "æ˜ å°„åçš„æ ‡å‡†åˆ—å"
        // ... {"æœ€å¤š19ä¸ªæ˜ å°„" if needs_filtering else "æ‰€æœ‰åˆ—çš„æ˜ å°„"}
    }},
    "confidence_scores": {{
        // æ¯ä¸ªæ˜ å°„çš„ç½®ä¿¡åº¦
        "C": 0.95,
        "D": 0.88
    }},
    {'filtered_out' if needs_filtering else 'unmapped_columns'}: {{
        // {'è¢«ç­›é€‰æ‰çš„åˆ—ï¼ˆè¶…è¿‡19ä¸ªçš„éƒ¨åˆ†ï¼‰' if needs_filtering else 'æ— æ³•æ˜ å°„çš„åˆ—'}
        "åˆ—æ ‡è¯†": {{
            "original_name": "åŸå§‹åˆ—å",
            "reason": "{'ç­›é€‰åŸå› ï¼šç›¸ä¼¼åº¦å¤ªä½/å·²æœ‰æ›´å¥½çš„æ˜ å°„/æ— æ³•åŒ¹é…ç­‰' if needs_filtering else 'æ— æ³•æ˜ å°„åŸå› '}"
        }}
    }},
    "statistics": {{
        "total_columns": {column_count},
        "mapped_count": æ•°å­—,  // {'åº”è¯¥â‰¤19' if needs_filtering else 'æˆåŠŸæ˜ å°„æ•°'}
        {'filtered_count": æ•°å­—,  // è¢«ç­›é€‰æ‰çš„åˆ—æ•°' if needs_filtering else '"unmapped_count": æ•°å­—  // æ— æ³•æ˜ å°„æ•°'}
        "average_confidence": æ•°å­—  // å¹³å‡ç½®ä¿¡åº¦
    }}
}}

è¯·ç«‹å³åˆ†æå¹¶è¿”å›{'æ™ºèƒ½ç­›é€‰åçš„' if needs_filtering else ''}æ ‡å‡†åŒ–æ˜ å°„ç»“æœã€‚"""
        
        return prompt
    
    async def standardize_column_names(self, column_mapping: Dict[str, str]) -> Dict[str, Any]:
        """
        è°ƒç”¨AIè¿›è¡Œåˆ—åæ ‡å‡†åŒ–ï¼ˆæ”¯æŒæ™ºèƒ½ç­›é€‰ï¼‰
        
        Args:
            column_mapping: {Excelåˆ—æ ‡è¯†: åˆ—å} çš„å­—å…¸
            
        Returns:
            æ ‡å‡†åŒ–ç»“æœ
        """
        prompt = self.build_smart_standardization_prompt(column_mapping)
        
        messages = [
            {
                "role": "system",
                "content": "ä½ æ˜¯ä¸“ä¸šçš„æ•°æ®åˆ†æä¸“å®¶ï¼Œç²¾é€šCSVåˆ—åæ ‡å‡†åŒ–å’Œæ™ºèƒ½ç­›é€‰ã€‚æ€»æ˜¯è¿”å›æœ‰æ•ˆçš„JSONæ ¼å¼ã€‚"
            },
            {
                "role": "user",
                "content": prompt
            }
        ]
        
        result = await self.deepseek_client.chat_completion(messages, temperature=0.1)
        
        if result["success"]:
            try:
                content = result["content"]
                # æå–JSONéƒ¨åˆ†
                json_start = content.find('{')
                json_end = content.rfind('}') + 1
                
                if json_start != -1 and json_end > 0:
                    json_str = content[json_start:json_end]
                    parsed = json.loads(json_str)
                    
                    # éªŒè¯æ˜ å°„æ•°é‡ï¼ˆæœ€å¤š19ä¸ªï¼‰
                    mapping_count = len(parsed.get("column_mapping", {}))
                    if mapping_count > 19:
                        logger.warning(f"AIè¿”å›äº†{mapping_count}ä¸ªæ˜ å°„ï¼Œè¶…è¿‡19ä¸ªé™åˆ¶")
                        # åªä¿ç•™ç½®ä¿¡åº¦æœ€é«˜çš„19ä¸ª
                        scores = parsed.get("confidence_scores", {})
                        sorted_mappings = sorted(scores.items(), key=lambda x: x[1], reverse=True)[:19]
                        filtered_mapping = {k: parsed["column_mapping"][k] for k, _ in sorted_mappings}
                        parsed["column_mapping"] = filtered_mapping
                        parsed["statistics"]["mapped_count"] = 19
                    
                    return {
                        "success": True,
                        "result": parsed,
                        "original_mapping": column_mapping
                    }
            except Exception as e:
                logger.error(f"è§£æAIå“åº”å¤±è´¥: {e}")
                return {"success": False, "error": str(e)}
        
        return result
    
    def apply_standardization_to_simplified_file(self, 
                                                comparison_file_path: str,
                                                standardization_result: Dict[str, Any],
                                                output_path: str = None) -> Dict[str, Any]:
        """
        å°†æ ‡å‡†åŒ–ç»“æœåº”ç”¨åˆ°ç®€åŒ–çš„CSVå¯¹æ¯”æ–‡ä»¶
        
        Args:
            comparison_file_path: åŸå§‹ç®€åŒ–CSVå¯¹æ¯”ç»“æœæ–‡ä»¶è·¯å¾„
            standardization_result: AIæ ‡å‡†åŒ–ç»“æœ
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            å¤„ç†ç»“æœ
        """
        # è¯»å–åŸå§‹æ–‡ä»¶
        with open(comparison_file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        if not standardization_result.get("success"):
            return {"success": False, "error": "æ ‡å‡†åŒ–å¤±è´¥"}
        
        column_name_mapping = standardization_result["result"].get("column_mapping", {})
        
        # åˆ›å»ºæ ‡å‡†åŒ–åçš„åˆ—æ˜ å°„
        standardized_columns = {}
        for col_id, original_name in data.get("modified_columns", {}).items():
            if col_id in column_name_mapping:
                standardized_columns[col_id] = column_name_mapping[col_id]
            else:
                # ä¿ç•™æœªæ˜ å°„çš„åˆ—ï¼ˆå¦‚æœè¢«ç­›é€‰æ‰ï¼‰
                standardized_columns[col_id] = original_name
        
        # æ›´æ–°æ•°æ®
        data["standardized_columns"] = standardized_columns
        data["original_modified_columns"] = data.get("modified_columns", {})
        data["modified_columns"] = standardized_columns
        
        # æ·»åŠ æ ‡å‡†åŒ–å…ƒæ•°æ®
        data["standardization_metadata"] = {
            "standardized_at": datetime.now().isoformat(),
            "column_mapping": column_name_mapping,
            "statistics": standardization_result["result"].get("statistics", {}),
            "filtered_out": standardization_result["result"].get("filtered_out", {}),
            "confidence_scores": standardization_result["result"].get("confidence_scores", {})
        }
        
        # ä¿å­˜ç»“æœ
        if output_path is None:
            base_name = os.path.splitext(comparison_file_path)[0]
            output_path = f"{base_name}_standardized.json"
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"æ ‡å‡†åŒ–ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
        
        return {
            "success": True,
            "input_file": comparison_file_path,
            "output_file": output_path,
            "column_mapping": column_name_mapping,
            "statistics": standardization_result["result"].get("statistics", {}),
            "filtered_count": len(standardization_result["result"].get("filtered_out", {}))
        }
    
    async def process_simplified_comparison_file(self, comparison_file_path: str) -> Dict[str, Any]:
        """
        å¤„ç†ç®€åŒ–CSVå¯¹æ¯”ç»“æœæ–‡ä»¶çš„å®Œæ•´æµç¨‹
        
        Args:
            comparison_file_path: ç®€åŒ–CSVå¯¹æ¯”ç»“æœæ–‡ä»¶è·¯å¾„
            
        Returns:
            å¤„ç†ç»“æœ
        """
        logger.info(f"å¼€å§‹å¤„ç†ç®€åŒ–æ ¼å¼æ–‡ä»¶: {comparison_file_path}")
        
        try:
            # æ­¥éª¤1ï¼šæå–ä¿®æ”¹åˆ—ä¿¡æ¯
            column_mapping = self.extract_columns_from_simplified_comparison(
                comparison_file_path
            )
            
            if not column_mapping:
                return {"success": False, "error": "æœªæ‰¾åˆ°ä¿®æ”¹åˆ—"}
            
            # æ­¥éª¤2ï¼šè°ƒç”¨AIæ ‡å‡†åŒ–ï¼ˆæ”¯æŒæ™ºèƒ½ç­›é€‰ï¼‰
            standardization_result = await self.standardize_column_names(column_mapping)
            
            if not standardization_result.get("success"):
                return standardization_result
            
            # æ­¥éª¤3ï¼šåº”ç”¨æ ‡å‡†åŒ–
            result = self.apply_standardization_to_simplified_file(
                comparison_file_path,
                standardization_result
            )
            
            return result
            
        except Exception as e:
            logger.error(f"å¤„ç†æ–‡ä»¶å¤±è´¥: {e}")
            return {"success": False, "error": str(e)}
    
    def sync_process_file(self, comparison_file_path: str) -> Dict[str, Any]:
        """åŒæ­¥ç‰ˆæœ¬çš„æ–‡ä»¶å¤„ç†"""
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            return loop.run_until_complete(
                self.process_simplified_comparison_file(comparison_file_path)
            )
        finally:
            loop.close()


# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    import sys
    
    # æµ‹è¯•ç®€åŒ–æ ¼å¼æ–‡ä»¶
    test_file = "/root/projects/tencent-doc-manager/comparison_results/simplified_å‰¯æœ¬-æµ‹è¯•ç‰ˆæœ¬-å‡ºå›½é”€å”®è®¡åˆ’è¡¨-å·¥ä½œè¡¨1_vs_å‰¯æœ¬-å‰¯æœ¬-æµ‹è¯•ç‰ˆæœ¬-å‡ºå›½é”€å”®è®¡åˆ’è¡¨-å·¥ä½œè¡¨1_20250905_224147.json"
    
    if len(sys.argv) > 1:
        test_file = sys.argv[1]
    
    if not Path(test_file).exists():
        print(f"æ–‡ä»¶ä¸å­˜åœ¨: {test_file}")
        sys.exit(1)
    
    api_key = "sk-onzowexyblsrgltveejlnajoutrjqwbrqkwzcccskwmzonvb"
    
    processor = ColumnStandardizationProcessorV3(api_key)
    
    # å…ˆæµ‹è¯•åˆ—æå–
    columns = processor.extract_columns_from_simplified_comparison(test_file)
    print(f"\nğŸ“Š æå–åˆ°çš„ä¿®æ”¹åˆ— ({len(columns)}ä¸ª):")
    for col_id, col_name in sorted(columns.items()):
        print(f"  {col_id}: {col_name}")
    
    # æµ‹è¯•å®Œæ•´å¤„ç†
    print("\nğŸš€ å¼€å§‹æ ‡å‡†åŒ–å¤„ç†...")
    result = processor.sync_process_file(test_file)
    
    if result.get("success"):
        print("\nâœ… å¤„ç†æˆåŠŸï¼")
        print(f"è¾“å…¥æ–‡ä»¶: {result['input_file']}")
        print(f"è¾“å‡ºæ–‡ä»¶: {result['output_file']}")
        print(f"\nç»Ÿè®¡ä¿¡æ¯:")
        print(f"  æ˜ å°„åˆ—æ•°: {result['statistics'].get('mapped_count', 0)}")
        print(f"  ç­›é€‰æ‰åˆ—æ•°: {result.get('filtered_count', 0)}")
        print(f"\nåˆ—æ˜ å°„:")
        for col_id, standard_name in result['column_mapping'].items():
            print(f"  åˆ—{col_id} â†’ {standard_name}")
    else:
        print(f"\nâŒ å¤„ç†å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")