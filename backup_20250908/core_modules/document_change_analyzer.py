#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è…¾è®¯æ–‡æ¡£å˜æ›´å¯¹æ¯”åˆ†æç¨‹åº
ç”¨äºæ£€æµ‹CSVè¡¨æ ¼çš„ä¿®æ”¹å†…å®¹å¹¶ç”Ÿæˆé£é™©è¯„ä¼°æŠ¥å‘Š
"""

import pandas as pd
import difflib
import json
from datetime import datetime
from typing import Dict, List, Tuple, Any
import re
import sys
import os

# å¯¼å…¥è‡ªé€‚åº”è¡¨æ ¼å¯¹æ¯”å™¨
try:
    from adaptive_table_comparator import AdaptiveTableComparator, IntelligentColumnMatcher
except ImportError:
    # å¦‚æœå¯¼å…¥å¤±è´¥ï¼Œæ·»åŠ å½“å‰ç›®å½•åˆ°è·¯å¾„
    sys.path.append(os.path.dirname(os.path.abspath(__file__)))
    from adaptive_table_comparator import AdaptiveTableComparator, IntelligentColumnMatcher

class DocumentChangeAnalyzer:
    """æ–‡æ¡£å˜æ›´åˆ†æå™¨ - å¢å¼ºç‰ˆæœ¬ï¼Œé›†æˆè‡ªé€‚åº”è¡¨æ ¼å¯¹æ¯”åŠŸèƒ½"""
    
    def __init__(self):
        # åˆ—é£é™©ç­‰çº§é…ç½®ï¼ˆåŸºäºè§„æ ¼è¯´æ˜ä¹¦ï¼‰
        self.column_risk_levels = {
            "åºå·": "L3",                    # å¯è‡ªç”±ç¼–è¾‘
            "é¡¹ç›®ç±»å‹": "L2",                # éœ€è¦è¯­ä¹‰å®¡æ ¸  
            "æ¥æº": "L1",                    # ç»å¯¹ä¸èƒ½ä¿®æ”¹
            "ä»»åŠ¡å‘èµ·æ—¶é—´": "L1",            # ç»å¯¹ä¸èƒ½ä¿®æ”¹
            "ç›®æ ‡å¯¹é½": "L1",                # ç»å¯¹ä¸èƒ½ä¿®æ”¹ - ç”¨æˆ·ç‰¹åˆ«å¼ºè°ƒ
            "å…³é”®KRå¯¹é½": "L1",              # ç»å¯¹ä¸èƒ½ä¿®æ”¹
            "å…·ä½“è®¡åˆ’å†…å®¹": "L2",            # éœ€è¦è¯­ä¹‰å®¡æ ¸
            "é‚“æ€»æŒ‡å¯¼ç™»è®°": "L2",            # éœ€è¦è¯­ä¹‰å®¡æ ¸
            "è´Ÿè´£äºº": "L2",                  # éœ€è¦è¯­ä¹‰å®¡æ ¸
            "ååŠ©äºº": "L2",                  # éœ€è¦è¯­ä¹‰å®¡æ ¸
            "ç›‘ç£äºº": "L2",                  # éœ€è¦è¯­ä¹‰å®¡æ ¸
            "é‡è¦ç¨‹åº¦": "L1",                # ç»å¯¹ä¸èƒ½ä¿®æ”¹
            "é¢„è®¡å®Œæˆæ—¶é—´": "L1",            # ç»å¯¹ä¸èƒ½ä¿®æ”¹
            "å®Œæˆè¿›åº¦": "L1",                # ç»å¯¹ä¸èƒ½ä¿®æ”¹
            "å½¢æˆè®¡åˆ’æ¸…å•,å®Œæˆé™„ä»¶ã€é“¾æ¥ã€æˆªå›¾ä¸Šä¼ ": "L2",  # éœ€è¦è¯­ä¹‰å®¡æ ¸
            "å¤ç›˜å‘¨æœŸ": "L3",                # å¯è‡ªç”±ç¼–è¾‘
            "å¤ç›˜æ—¶é—´": "L3",                # å¯è‡ªç”±ç¼–è¾‘
            "å¯¹ä¸Šæ±‡æŠ¥": "L3",                # å¯è‡ªç”±ç¼–è¾‘
            "åº”ç”¨æƒ…å†µ": "L3",                # å¯è‡ªç”±ç¼–è¾‘
            "è¿›åº¦åˆ†æä¸æ€»ç»“": "L3"           # å¯è‡ªç”±ç¼–è¾‘
        }
        
        # é£é™©è¯„åˆ†é…ç½®
        self.risk_scores = {
            "L1": 1.0,    # æœ€é«˜é£é™©
            "L2": 0.6,    # ä¸­ç­‰é£é™©
            "L3": 0.2     # æœ€ä½é£é™©
        }
        
        # é›†æˆè‡ªé€‚åº”è¡¨æ ¼å¯¹æ¯”å™¨
        self.adaptive_comparator = AdaptiveTableComparator()
        self.column_matcher = IntelligentColumnMatcher()
        
        # å¤„ç†æ¨¡å¼ï¼š'legacy' ä¼ ç»Ÿæ¨¡å¼ | 'adaptive' è‡ªé€‚åº”æ¨¡å¼ | 'hybrid' æ··åˆæ¨¡å¼
        self.processing_mode = 'hybrid'

    def adaptive_compare_tables(self, current_file: str, reference_file: str = None) -> Dict[str, Any]:
        """
        è‡ªé€‚åº”è¡¨æ ¼å¯¹æ¯”åˆ†æ - æ–°å¢æ–¹æ³•
        æ”¯æŒä¸åŒæ ¼å¼è¡¨æ ¼çš„æ™ºèƒ½å¯¹æ¯”å’Œåˆ†æ
        
        Args:
            current_file: å½“å‰è¡¨æ ¼æ–‡ä»¶è·¯å¾„
            reference_file: å‚è€ƒè¡¨æ ¼æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            å®Œæ•´çš„å¯¹æ¯”åˆ†æç»“æœï¼ŒåŒ…æ‹¬æ™ºèƒ½åŒ¹é…å’Œé£é™©è¯„ä¼°
        """
        print(f"ğŸ” å¯åŠ¨è‡ªé€‚åº”è¡¨æ ¼å¯¹æ¯”åˆ†æ")
        print(f"å½“å‰æ–‡ä»¶: {current_file}")
        if reference_file:
            print(f"å‚è€ƒæ–‡ä»¶: {reference_file}")
        
        try:
            # åŠ è½½å½“å‰è¡¨æ ¼æ•°æ®
            current_df = self.load_csv_data(current_file)
            if current_df.empty:
                return {"error": f"æ— æ³•åŠ è½½å½“å‰æ–‡ä»¶: {current_file}"}
            
            # è½¬æ¢ä¸ºè‡ªé€‚åº”å¯¹æ¯”å™¨éœ€è¦çš„æ ¼å¼
            current_table_data = {
                "name": os.path.basename(current_file),
                "data": [current_df.columns.tolist()] + current_df.values.tolist()
            }
            
            tables_to_compare = [current_table_data]
            reference_tables = None
            
            # å¦‚æœæœ‰å‚è€ƒæ–‡ä»¶ï¼ŒåŠ è½½å‚è€ƒæ•°æ®
            if reference_file:
                reference_df = self.load_csv_data(reference_file)
                if not reference_df.empty:
                    reference_table_data = {
                        "name": os.path.basename(reference_file),
                        "data": [reference_df.columns.tolist()] + reference_df.values.tolist()
                    }
                    reference_tables = [reference_table_data]
                    print(f"âœ… å‚è€ƒæ•°æ®åŠ è½½æˆåŠŸ: {len(reference_df)}è¡Œ Ã— {len(reference_df.columns)}åˆ—")
            
            print(f"âœ… å½“å‰æ•°æ®åŠ è½½æˆåŠŸ: {len(current_df)}è¡Œ Ã— {len(current_df.columns)}åˆ—")
            
            # æ‰§è¡Œè‡ªé€‚åº”å¯¹æ¯”åˆ†æ
            adaptive_result = self.adaptive_comparator.adaptive_compare_tables(
                tables_to_compare, reference_tables
            )
            
            # å¦‚æœæ˜¯æ··åˆæ¨¡å¼ï¼Œè¿˜è¦æ‰§è¡Œä¼ ç»Ÿå¯¹æ¯”ï¼ˆç”¨äºå…¼å®¹æ€§ï¼‰
            legacy_result = None
            if self.processing_mode in ['legacy', 'hybrid'] and reference_file and not reference_df.empty:
                legacy_result = self.compare_dataframes(reference_df, current_df)
                print("âœ… ä¼ ç»Ÿå¯¹æ¯”åˆ†æå®Œæˆ")
            
            # åˆå¹¶å’Œå¢å¼ºç»“æœ
            enhanced_result = self._merge_analysis_results(
                adaptive_result, legacy_result, current_df.columns.tolist()
            )
            
            print(f"âœ… è‡ªé€‚åº”å¯¹æ¯”åˆ†æå®Œæˆ")
            return enhanced_result
            
        except Exception as e:
            error_msg = f"è‡ªé€‚åº”å¯¹æ¯”åˆ†æå¤±è´¥: {str(e)}"
            print(f"âŒ {error_msg}")
            return {"error": error_msg, "traceback": str(e)}
    
    def _merge_analysis_results(self, adaptive_result: Dict[str, Any], 
                               legacy_result: Dict[str, Any] = None,
                               current_columns: List[str] = None) -> Dict[str, Any]:
        """
        åˆå¹¶è‡ªé€‚åº”åˆ†æç»“æœå’Œä¼ ç»Ÿåˆ†æç»“æœ
        
        Args:
            adaptive_result: è‡ªé€‚åº”å¯¹æ¯”å™¨çš„ç»“æœ
            legacy_result: ä¼ ç»Ÿå¯¹æ¯”å™¨çš„ç»“æœï¼ˆå¯é€‰ï¼‰
            current_columns: å½“å‰è¡¨æ ¼çš„åˆ—å
            
        Returns:
            åˆå¹¶åçš„å¢å¼ºç»“æœ
        """
        
        enhanced_result = {
            "analysis_mode": self.processing_mode,
            "analysis_timestamp": datetime.now().isoformat(),
            "adaptive_analysis": adaptive_result,
            "legacy_analysis": legacy_result,
            "enhanced_summary": {},
            "column_intelligence": {},
            "recommendations": []
        }
        
        # æå–è‡ªé€‚åº”åˆ†æçš„å…³é”®ä¿¡æ¯
        if adaptive_result and "comparison_results" in adaptive_result:
            first_table_result = adaptive_result["comparison_results"][0] if adaptive_result["comparison_results"] else {}
            
            # åˆ—åŒ¹é…æ™ºèƒ½åˆ†æ
            if "matching_result" in first_table_result:
                matching_result = first_table_result["matching_result"]
                enhanced_result["column_intelligence"] = {
                    "total_input_columns": len(current_columns) if current_columns else 0,
                    "successfully_matched": len(matching_result.get("mapping", {})),
                    "match_confidence": {
                        col: conf for col, conf in matching_result.get("confidence_scores", {}).items()
                    },
                    "unmatched_columns": matching_result.get("unmatched_columns", []),
                    "missing_standard_columns": matching_result.get("missing_columns", []),
                    "matching_report": matching_result.get("matching_report", "")
                }
                
                # ç”Ÿæˆåˆ—åŒ¹é…å»ºè®®
                if matching_result.get("unmatched_columns"):
                    enhanced_result["recommendations"].append({
                        "type": "column_matching",
                        "priority": "high",
                        "message": f"å‘ç° {len(matching_result['unmatched_columns'])} ä¸ªæœªåŒ¹é…çš„åˆ—ï¼Œå»ºè®®æ£€æŸ¥åˆ—åè§„èŒƒ",
                        "details": matching_result["unmatched_columns"]
                    })
                
                # ä½ç½®ä¿¡åº¦åŒ¹é…è­¦å‘Š
                low_confidence_matches = [
                    col for col, conf in matching_result.get("confidence_scores", {}).items()
                    if conf < 0.7
                ]
                if low_confidence_matches:
                    enhanced_result["recommendations"].append({
                        "type": "low_confidence_matching",
                        "priority": "medium", 
                        "message": f"å‘ç° {len(low_confidence_matches)} ä¸ªä½ç½®ä¿¡åº¦åŒ¹é…ï¼Œå»ºè®®äººå·¥ç¡®è®¤",
                        "details": low_confidence_matches
                    })
            
            # æ•°æ®è´¨é‡åˆ†æ
            if "standardization_result" in first_table_result:
                std_result = first_table_result["standardization_result"]
                quality_score = std_result.get("data_quality_score", 0.0)
                
                enhanced_result["enhanced_summary"]["data_quality_score"] = quality_score
                enhanced_result["enhanced_summary"]["standardization_issues"] = len(std_result.get("issues", []))
                
                if quality_score < 0.7:
                    enhanced_result["recommendations"].append({
                        "type": "data_quality",
                        "priority": "high",
                        "message": f"æ•°æ®è´¨é‡è¾ƒä½ ({quality_score:.2f})ï¼Œå»ºè®®æ£€æŸ¥åŸå§‹æ•°æ®",
                        "details": std_result.get("issues", [])[:5]  # åªæ˜¾ç¤ºå‰5ä¸ªé—®é¢˜
                    })
        
        # åˆå¹¶ä¼ ç»Ÿåˆ†æç»“æœï¼ˆå¦‚æœæœ‰ï¼‰
        if legacy_result:
            enhanced_result["enhanced_summary"].update({
                "legacy_total_changes": legacy_result["summary"]["total_changes"],
                "legacy_l1_violations": legacy_result["summary"]["l1_violations"],
                "legacy_l2_modifications": legacy_result["summary"]["l2_modifications"],
                "legacy_l3_modifications": legacy_result["summary"]["l3_modifications"],
                "legacy_risk_score": legacy_result["summary"]["risk_score"]
            })
            
            # L1è¿è§„ä¸¥é‡è­¦å‘Š
            if legacy_result["summary"]["l1_violations"] > 0:
                enhanced_result["recommendations"].append({
                    "type": "l1_violations",
                    "priority": "critical",
                    "message": f"æ£€æµ‹åˆ° {legacy_result['summary']['l1_violations']} ä¸ªL1çº§ä¸¥é‡è¿è§„ä¿®æ”¹",
                    "details": [
                        change for change in legacy_result["detailed_changes"] 
                        if change["risk_level"] == "L1"
                    ][:3]  # åªæ˜¾ç¤ºå‰3ä¸ª
                })
        
        # å¤„ç†ç»Ÿè®¡æ±‡æ€»
        processing_stats = adaptive_result.get("processing_stats", {})
        enhanced_result["enhanced_summary"].update({
            "tables_processed": processing_stats.get("tables_processed", 0),
            "average_match_confidence": processing_stats.get("average_match_confidence", 0.0),
            "total_processing_time": processing_stats.get("total_processing_time", 0.0)
        })
        
        # æ•´ä½“é£é™©è¯„ä¼°
        overall_risk = self._assess_overall_risk(enhanced_result)
        enhanced_result["enhanced_summary"]["overall_risk_level"] = overall_risk
        
        return enhanced_result
    
    def _assess_overall_risk(self, enhanced_result: Dict[str, Any]) -> str:
        """è¯„ä¼°æ•´ä½“é£é™©ç­‰çº§"""
        
        # æ£€æŸ¥L1è¿è§„
        l1_violations = enhanced_result["enhanced_summary"].get("legacy_l1_violations", 0)
        if l1_violations > 0:
            return "CRITICAL"  # æœ‰L1è¿è§„å°±æ˜¯ä¸¥é‡é£é™©
        
        # æ£€æŸ¥æ•°æ®è´¨é‡
        data_quality = enhanced_result["enhanced_summary"].get("data_quality_score", 1.0)
        if data_quality < 0.6:
            return "HIGH"  # æ•°æ®è´¨é‡å¤ªä½
        
        # æ£€æŸ¥åŒ¹é…è´¨é‡
        column_intel = enhanced_result.get("column_intelligence", {})
        match_rate = 0
        if column_intel.get("total_input_columns", 0) > 0:
            match_rate = column_intel.get("successfully_matched", 0) / column_intel["total_input_columns"]
        
        if match_rate < 0.7:
            return "HIGH"  # åŒ¹é…ç‡å¤ªä½
        
        # æ£€æŸ¥L2ä¿®æ”¹æ•°é‡
        l2_modifications = enhanced_result["enhanced_summary"].get("legacy_l2_modifications", 0)
        if l2_modifications > 5:
            return "MEDIUM"  # L2ä¿®æ”¹è¾ƒå¤š
        
        return "LOW"  # å…¶ä»–æƒ…å†µè®¤ä¸ºæ˜¯ä½é£é™©
    
    def load_csv_data(self, file_path: str) -> pd.DataFrame:
        """åŠ è½½CSVæ•°æ®"""
        try:
            # ä½¿ç”¨UTF-8ç¼–ç è¯»å–ï¼Œè·³è¿‡BOMï¼Œè·³è¿‡ç¬¬ä¸€è¡Œæ ‡é¢˜
            df = pd.read_csv(file_path, encoding='utf-8-sig', skiprows=1)
            return df
        except Exception as e:
            print(f"è¯»å–æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            return pd.DataFrame()

    def compare_dataframes(self, df_original: pd.DataFrame, df_modified: pd.DataFrame) -> Dict[str, Any]:
        """å¯¹æ¯”ä¸¤ä¸ªDataFrameï¼Œæ£€æµ‹å˜æ›´"""
        changes = {
            "summary": {
                "total_changes": 0,
                "l1_violations": 0,
                "l2_modifications": 0, 
                "l3_modifications": 0,
                "risk_score": 0.0
            },
            "detailed_changes": [],
            "risk_matrix": {}
        }
        
        # ç¡®ä¿ä¸¤ä¸ªDataFrameæœ‰ç›¸åŒçš„å½¢çŠ¶è¿›è¡Œå¯¹æ¯”
        max_rows = max(len(df_original), len(df_modified))
        max_cols = max(len(df_original.columns) if not df_original.empty else 0, 
                      len(df_modified.columns) if not df_modified.empty else 0)
        
        # é‡æ–°ç´¢å¼•ä»¥ç¡®ä¿å¯¹é½
        if not df_original.empty:
            df_original = df_original.reindex(range(max_rows))
        if not df_modified.empty:
            df_modified = df_modified.reindex(range(max_rows))
        
        # é€åˆ—å¯¹æ¯”
        for col_idx, col_name in enumerate(df_modified.columns if not df_modified.empty else []):
            if col_name in df_original.columns:
                original_col = df_original[col_name].fillna("")
                modified_col = df_modified[col_name].fillna("")
                
                # é€è¡Œå¯¹æ¯”è¯¥åˆ—
                for row_idx in range(max_rows):
                    original_value = str(original_col.iloc[row_idx]) if row_idx < len(original_col) else ""
                    modified_value = str(modified_col.iloc[row_idx]) if row_idx < len(modified_col) else ""
                    
                    if original_value != modified_value:
                        change = self._analyze_change(
                            row_idx, col_name, original_value, modified_value
                        )
                        changes["detailed_changes"].append(change)
                        changes["summary"]["total_changes"] += 1
                        
                        # ç»Ÿè®¡ä¸åŒé£é™©ç­‰çº§çš„ä¿®æ”¹
                        risk_level = change["risk_level"]
                        if risk_level == "L1":
                            changes["summary"]["l1_violations"] += 1
                        elif risk_level == "L2":
                            changes["summary"]["l2_modifications"] += 1
                        elif risk_level == "L3":
                            changes["summary"]["l3_modifications"] += 1
                        
                        # ç´¯è®¡é£é™©åˆ†æ•°
                        changes["summary"]["risk_score"] += change["risk_score"]
        
        # ç”Ÿæˆé£é™©çŸ©é˜µï¼ˆç”¨äºçƒ­åŠ›å›¾ï¼‰
        changes["risk_matrix"] = self._generate_risk_matrix(df_modified, changes["detailed_changes"])
        
        return changes

    def _analyze_change(self, row_idx: int, col_name: str, original: str, modified: str) -> Dict[str, Any]:
        """åˆ†æå•ä¸ªå˜æ›´çš„è¯¦ç»†ä¿¡æ¯"""
        # è·å–åˆ—çš„é£é™©ç­‰çº§
        risk_level = self.column_risk_levels.get(col_name, "L2")
        risk_score = self.risk_scores[risk_level]
        
        # æ£€æµ‹ä¿®æ”¹ç±»å‹
        change_type = "content_update"
        if "ã€ä¿®æ”¹ã€‘" in modified:
            change_type = "explicit_modification"
            risk_score *= 1.2  # æ˜ç¡®æ ‡è®°çš„ä¿®æ”¹å¢åŠ é£é™©åˆ†æ•°
        elif "ã€æ–°å¢ã€‘" in modified:
            change_type = "content_addition"
            risk_score *= 1.1
        elif original and not modified:
            change_type = "deletion"
            risk_score *= 1.3
        elif not original and modified:
            change_type = "insertion"
            risk_score *= 1.1
            
        # è®¡ç®—ä¿®æ”¹å¼ºåº¦ï¼ˆåŸºäºæ–‡æœ¬å·®å¼‚ç¨‹åº¦ï¼‰
        similarity = self._calculate_similarity(original, modified)
        modification_intensity = 1.0 - similarity
        
        return {
            "row": row_idx,
            "column": col_name,
            "original_value": original[:100] + "..." if len(original) > 100 else original,
            "modified_value": modified[:100] + "..." if len(modified) > 100 else modified,
            "change_type": change_type,
            "risk_level": risk_level,
            "risk_score": risk_score,
            "modification_intensity": modification_intensity,
            "severity": self._assess_severity(risk_level, modification_intensity),
            "timestamp": datetime.now().isoformat()
        }

    def _calculate_similarity(self, text1: str, text2: str) -> float:
        """è®¡ç®—ä¸¤ä¸ªæ–‡æœ¬çš„ç›¸ä¼¼åº¦"""
        if not text1 and not text2:
            return 1.0
        if not text1 or not text2:
            return 0.0
            
        # ä½¿ç”¨SequenceMatcherè®¡ç®—ç›¸ä¼¼åº¦
        matcher = difflib.SequenceMatcher(None, text1, text2)
        return matcher.ratio()

    def _assess_severity(self, risk_level: str, modification_intensity: float) -> str:
        """è¯„ä¼°ä¿®æ”¹ä¸¥é‡ç¨‹åº¦"""
        if risk_level == "L1":
            return "critical"  # L1åˆ—çš„ä»»ä½•ä¿®æ”¹éƒ½æ˜¯ä¸¥é‡çš„
        elif risk_level == "L2":
            if modification_intensity > 0.7:
                return "high"
            elif modification_intensity > 0.4:
                return "medium"
            else:
                return "low"
        else:  # L3
            if modification_intensity > 0.8:
                return "medium"
            else:
                return "low"

    def _generate_risk_matrix(self, df: pd.DataFrame, changes: List[Dict]) -> Dict[str, Any]:
        """ç”Ÿæˆç”¨äºçƒ­åŠ›å›¾çš„é£é™©çŸ©é˜µ"""
        if df.empty:
            return {}
            
        matrix = {}
        rows, cols = df.shape
        
        # åˆå§‹åŒ–çŸ©é˜µ
        for col_name in df.columns:
            matrix[col_name] = [0.0] * rows
        
        # å¡«å…¥é£é™©åˆ†æ•°
        for change in changes:
            col_name = change["column"]
            row_idx = change["row"]
            if col_name in matrix and row_idx < len(matrix[col_name]):
                matrix[col_name][row_idx] = change["modification_intensity"] * change["risk_score"]
        
        return matrix

    def generate_report(self, changes: Dict[str, Any]) -> str:
        """ç”Ÿæˆå˜æ›´åˆ†ææŠ¥å‘Š"""
        report = []
        report.append("=" * 60)
        report.append("è…¾è®¯æ–‡æ¡£å˜æ›´æ£€æµ‹æŠ¥å‘Š")
        report.append("=" * 60)
        report.append(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append("")
        
        # æ¦‚è¦ç»Ÿè®¡
        summary = changes["summary"]
        report.append("ğŸ“Š å˜æ›´æ¦‚è¦:")
        report.append(f"  æ€»å˜æ›´æ•°é‡: {summary['total_changes']}")
        report.append(f"  ğŸ”´ L1çº§ä¸¥é‡è¿è§„: {summary['l1_violations']} (ç»å¯¹ä¸èƒ½ä¿®æ”¹)")
        report.append(f"  ğŸŸ¡ L2çº§å¼‚å¸¸ä¿®æ”¹: {summary['l2_modifications']} (éœ€è¦è¯­ä¹‰å®¡æ ¸)")
        report.append(f"  ğŸŸ¢ L3çº§å¸¸è§„ä¿®æ”¹: {summary['l3_modifications']} (å¯è‡ªç”±ç¼–è¾‘)")
        report.append(f"  ç»¼åˆé£é™©è¯„åˆ†: {summary['risk_score']:.2f}")
        
        # é£é™©ç­‰çº§è¯„ä¼°
        if summary['l1_violations'] > 0:
            report.append(f"  âš ï¸  é£é™©ç­‰çº§: ğŸ”´ ä¸¥é‡ (å‘ç°L1çº§åˆ«è¿è§„)")
        elif summary['l2_modifications'] > 3:
            report.append(f"  âš ï¸  é£é™©ç­‰çº§: ğŸŸ¡ ä¸­ç­‰ (L2ä¿®æ”¹è¾ƒå¤š)")
        else:
            report.append(f"  âš ï¸  é£é™©ç­‰çº§: ğŸŸ¢ æ­£å¸¸")
        
        report.append("")
        
        # è¯¦ç»†å˜æ›´åˆ—è¡¨
        if changes["detailed_changes"]:
            report.append("ğŸ“‹ è¯¦ç»†å˜æ›´è®°å½•:")
            for i, change in enumerate(changes["detailed_changes"][:10], 1):  # åªæ˜¾ç¤ºå‰10ä¸ª
                report.append(f"\n  [{i}] ç¬¬{change['row'] + 1}è¡Œ - {change['column']}")
                report.append(f"      é£é™©ç­‰çº§: {change['risk_level']} | ä¸¥é‡ç¨‹åº¦: {change['severity']}")
                report.append(f"      ä¿®æ”¹å¼ºåº¦: {change['modification_intensity']:.2f} | é£é™©åˆ†æ•°: {change['risk_score']:.2f}")
                report.append(f"      åŸå†…å®¹: {change['original_value']}")
                report.append(f"      æ–°å†…å®¹: {change['modified_value']}")
                report.append(f"      ä¿®æ”¹ç±»å‹: {change['change_type']}")
                
            if len(changes["detailed_changes"]) > 10:
                report.append(f"\n  ... è¿˜æœ‰ {len(changes['detailed_changes']) - 10} ä¸ªå˜æ›´æœªæ˜¾ç¤º")
        
        report.append("")
        report.append("=" * 60)
        
        return "\n".join(report)

def main():
    """ä¸»å‡½æ•°ï¼šæ‰§è¡Œå¯¹æ¯”åˆ†ææµ‹è¯• - å¢å¼ºç‰ˆæœ¬ï¼Œæ”¯æŒè‡ªé€‚åº”å’Œä¼ ç»Ÿä¸¤ç§æ¨¡å¼"""
    print("ğŸ” å¯åŠ¨è…¾è®¯æ–‡æ¡£å˜æ›´æ£€æµ‹ç³»ç»Ÿ - å¢å¼ºç‰ˆæœ¬")
    print("æ”¯æŒè‡ªé€‚åº”è¡¨æ ¼å¯¹æ¯”å’Œä¼ ç»Ÿå¯¹æ¯”ä¸¤ç§æ¨¡å¼")
    
    analyzer = DocumentChangeAnalyzer()
    
    # æ–‡ä»¶è·¯å¾„
    original_file = "/root/projects/tencent-doc-manager/refer/æµ‹è¯•ç‰ˆæœ¬-å°çº¢ä¹¦éƒ¨é—¨-å·¥ä½œè¡¨2.csv"
    modified_file = "/root/projects/tencent-doc-manager/æµ‹è¯•ç‰ˆæœ¬-å°çº¢ä¹¦éƒ¨é—¨-å·¥ä½œè¡¨2-ä¿®æ”¹ç‰ˆ.csv"
    
    print(f"\n=== æ¨¡å¼é€‰æ‹© ===")
    print(f"å½“å‰å¤„ç†æ¨¡å¼: {analyzer.processing_mode}")
    print("æ¨¡å¼è¯´æ˜:")
    print("  - hybrid: æ··åˆæ¨¡å¼ï¼ˆæ¨èï¼‰- åŒæ—¶è¿è¡Œè‡ªé€‚åº”å’Œä¼ ç»Ÿå¯¹æ¯”")
    print("  - adaptive: ä»…è‡ªé€‚åº”æ¨¡å¼ - æ™ºèƒ½åˆ—åŒ¹é…å’Œæ ‡å‡†åŒ–")
    print("  - legacy: ä»…ä¼ ç»Ÿæ¨¡å¼ - åŸæœ‰çš„é€åˆ—å¯¹æ¯”")
    
    # æ‰§è¡Œè‡ªé€‚åº”å¯¹æ¯”åˆ†æ
    print(f"\nğŸš€ æ‰§è¡Œè‡ªé€‚åº”å¯¹æ¯”åˆ†æ...")
    adaptive_result = analyzer.adaptive_compare_tables(modified_file, original_file)
    
    if "error" in adaptive_result:
        print(f"âŒ è‡ªé€‚åº”åˆ†æå¤±è´¥: {adaptive_result['error']}")
        return
    
    # æ˜¾ç¤ºå¢å¼ºåˆ†æç»“æœ
    print("\n" + "="*80)
    print("ğŸ“Š è‡ªé€‚åº”è¡¨æ ¼å¯¹æ¯”åˆ†ææŠ¥å‘Š")
    print("="*80)
    
    enhanced_summary = adaptive_result.get("enhanced_summary", {})
    column_intelligence = adaptive_result.get("column_intelligence", {})
    recommendations = adaptive_result.get("recommendations", [])
    
    # åŸºæœ¬ä¿¡æ¯
    print(f"åˆ†ææ¨¡å¼: {adaptive_result.get('analysis_mode', 'unknown')}")
    print(f"åˆ†ææ—¶é—´: {adaptive_result.get('analysis_timestamp', 'unknown')}")
    print(f"å¤„ç†è€—æ—¶: {enhanced_summary.get('total_processing_time', 0):.2f}ç§’")
    
    # åˆ—åŒ¹é…æ™ºèƒ½åˆ†æ
    print(f"\nğŸ“‹ åˆ—åŒ¹é…æ™ºèƒ½åˆ†æ:")
    print(f"  è¾“å…¥åˆ—æ•°: {column_intelligence.get('total_input_columns', 0)}")
    print(f"  æˆåŠŸåŒ¹é…: {column_intelligence.get('successfully_matched', 0)}")
    print(f"  åŒ¹é…æˆåŠŸç‡: {column_intelligence.get('successfully_matched', 0) / max(1, column_intelligence.get('total_input_columns', 1)) * 100:.1f}%")
    print(f"  å¹³å‡åŒ¹é…ç½®ä¿¡åº¦: {enhanced_summary.get('average_match_confidence', 0):.3f}")
    
    if column_intelligence.get("unmatched_columns"):
        print(f"  æœªåŒ¹é…åˆ—: {column_intelligence['unmatched_columns']}")
    
    if column_intelligence.get("missing_standard_columns"):
        print(f"  ç¼ºå¤±æ ‡å‡†åˆ—: {column_intelligence['missing_standard_columns']}")
    
    # æ•°æ®è´¨é‡åˆ†æ
    print(f"\nğŸ“ˆ æ•°æ®è´¨é‡åˆ†æ:")
    data_quality = enhanced_summary.get("data_quality_score")
    if data_quality is not None:
        print(f"  æ•°æ®è´¨é‡åˆ†æ•°: {data_quality:.3f}")
        quality_status = "ä¼˜ç§€" if data_quality >= 0.9 else "è‰¯å¥½" if data_quality >= 0.7 else "å¾…æ”¹è¿›"
        print(f"  è´¨é‡ç­‰çº§: {quality_status}")
        
    standardization_issues = enhanced_summary.get("standardization_issues", 0)
    if standardization_issues > 0:
        print(f"  æ ‡å‡†åŒ–é—®é¢˜æ•°: {standardization_issues}")
    
    # ä¼ ç»Ÿå¯¹æ¯”ç»“æœï¼ˆå¦‚æœæœ‰ï¼‰
    if "legacy_analysis" in adaptive_result and adaptive_result["legacy_analysis"]:
        print(f"\nğŸ” ä¼ ç»Ÿå¯¹æ¯”åˆ†æç»“æœ:")
        print(f"  æ€»å˜æ›´æ•°: {enhanced_summary.get('legacy_total_changes', 0)}")
        print(f"  ğŸ”´ L1çº§ä¸¥é‡è¿è§„: {enhanced_summary.get('legacy_l1_violations', 0)}")
        print(f"  ğŸŸ¡ L2çº§å¼‚å¸¸ä¿®æ”¹: {enhanced_summary.get('legacy_l2_modifications', 0)}")
        print(f"  ğŸŸ¢ L3çº§å¸¸è§„ä¿®æ”¹: {enhanced_summary.get('legacy_l3_modifications', 0)}")
        print(f"  ç»¼åˆé£é™©è¯„åˆ†: {enhanced_summary.get('legacy_risk_score', 0):.2f}")
    
    # æ•´ä½“é£é™©è¯„ä¼°
    overall_risk = enhanced_summary.get("overall_risk_level", "UNKNOWN")
    risk_icons = {
        "CRITICAL": "ğŸ”´", "HIGH": "ğŸŸ ", "MEDIUM": "ğŸŸ¡", "LOW": "ğŸŸ¢"
    }
    print(f"\nâš ï¸  æ•´ä½“é£é™©ç­‰çº§: {risk_icons.get(overall_risk, 'â“')} {overall_risk}")
    
    # å»ºè®®å’Œæ¨è
    if recommendations:
        print(f"\nğŸ’¡ ç³»ç»Ÿå»ºè®® ({len(recommendations)}æ¡):")
        for i, rec in enumerate(recommendations, 1):
            priority_icons = {
                "critical": "ğŸ”´", "high": "ğŸŸ ", "medium": "ğŸŸ¡", "low": "ğŸŸ¢"
            }
            icon = priority_icons.get(rec.get("priority", "medium"), "ğŸ’¡")
            print(f"  [{i}] {icon} {rec.get('message', '')}")
            if rec.get("details") and len(rec["details"]) <= 3:
                for detail in rec["details"]:
                    if isinstance(detail, dict):
                        print(f"      - {detail.get('column', 'unknown')}: {detail.get('change_type', 'unknown')}")
                    else:
                        print(f"      - {detail}")
    
    # ä¿å­˜è¯¦ç»†ç»“æœåˆ°JSONæ–‡ä»¶
    output_file = "/root/projects/tencent-doc-manager/è‡ªé€‚åº”åˆ†æç»“æœ.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(adaptive_result, f, ensure_ascii=False, indent=2, default=str)
    
    print(f"\nğŸ’¾ è¯¦ç»†åˆ†æç»“æœå·²ä¿å­˜è‡³: {output_file}")
    
    # æ˜¾ç¤ºè¯¦ç»†åŒ¹é…æŠ¥å‘Šï¼ˆå¦‚æœæœ‰ï¼‰
    if column_intelligence.get("matching_report"):
        print(f"\nğŸ“‹ è¯¦ç»†åŒ¹é…æŠ¥å‘Š:")
        print(column_intelligence["matching_report"])
    
    # ä¼ ç»Ÿæ¨¡å¼çš„éªŒè¯æ£€æµ‹ï¼ˆå…¼å®¹æ€§ä¿æŒï¼‰
    if enhanced_summary.get("legacy_total_changes", 0) > 0:
        print(f"\nğŸ” ä¼ ç»Ÿæ¨¡å¼éªŒè¯æ£€æµ‹å‡†ç¡®æ€§:")
        detected_l1 = enhanced_summary.get("legacy_l1_violations", 0)
        detected_l2 = enhanced_summary.get("legacy_l2_modifications", 0) 
        detected_l3 = enhanced_summary.get("legacy_l3_modifications", 0)
        
        print(f"  é¢„æœŸL1è¿è§„: 4ä¸ª, æ£€æµ‹åˆ°: {detected_l1}ä¸ª")
        print(f"  é¢„æœŸL2ä¿®æ”¹: 1ä¸ª, æ£€æµ‹åˆ°: {detected_l2}ä¸ª")
        print(f"  é¢„æœŸL3ä¿®æ”¹: 1ä¸ª, æ£€æµ‹åˆ°: {detected_l3}ä¸ª")
        
        if detected_l1 >= 4 and detected_l2 >= 1:
            print("âœ… ä¼ ç»Ÿæ£€æµ‹å‡†ç¡®æ€§è‰¯å¥½: æˆåŠŸè¯†åˆ«å…³é”®é£é™©ä¿®æ”¹")
        else:
            print("âš ï¸  ä¼ ç»Ÿæ£€æµ‹ç»“æœéœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–")
    
    print(f"\nâœ… è‡ªé€‚åº”è¡¨æ ¼å¯¹æ¯”åˆ†æå®Œæˆ")
    print("="*80)

if __name__ == "__main__":
    main()