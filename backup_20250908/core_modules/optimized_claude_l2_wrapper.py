#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Stage 6: ä¼˜åŒ–Claude APIå°è£… - L2è¯­ä¹‰å˜åŒ–æ£€æµ‹ä¸“ç”¨
ç¡®ä¿Claude APIè¾¾åˆ°ç¨³å®šå¯ç”¨ï¼Œæ­£ç¡®è¯†åˆ«L2è¯­ä¹‰å˜åŒ–
"""

import sys
import os
sys.path.append('/root/projects/tencent-doc-manager/production/core_modules')

import asyncio
import aiohttp
import json
import time
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass
from collections import deque
import hashlib
import statistics

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class L2SemanticAnalysisRequest:
    """L2è¯­ä¹‰åˆ†æè¯·æ±‚"""
    change_id: str
    column_name: str
    original_value: str
    new_value: str
    table_context: str
    business_context: str = ""
    priority: str = "normal"  # high, normal, low


@dataclass
class L2SemanticAnalysisResult:
    """L2è¯­ä¹‰åˆ†æç»“æœ"""
    change_id: str
    semantic_decision: str  # APPROVE, REJECT, REVIEW
    confidence_score: float  # 0.0 - 1.0
    semantic_reasoning: str
    risk_level: str  # L1, L2, L3
    business_impact: str  # HIGH, MEDIUM, LOW
    approval_required: bool
    auto_processable: bool
    l2_specific_factors: List[str]
    processing_time: float
    api_model_used: str
    analysis_timestamp: str


class ClaudeAPIStabilityMonitor:
    """Claude APIç¨³å®šæ€§ç›‘æ§å™¨"""
    
    def __init__(self, window_size: int = 100):
        self.window_size = window_size
        self.response_times = deque(maxlen=window_size)
        self.success_history = deque(maxlen=window_size)
        self.error_counts = {}
        self.health_metrics = {
            'total_requests': 0,
            'successful_requests': 0,
            'failed_requests': 0,
            'avg_response_time': 0.0,
            'success_rate': 100.0,
            'error_rate': 0.0,
            'stability_score': 100.0,
            'last_check': datetime.now().isoformat()
        }
    
    def record_request(self, success: bool, response_time: float, error_type: str = None):
        """è®°å½•è¯·æ±‚ç»“æœ"""
        self.response_times.append(response_time)
        self.success_history.append(success)
        
        self.health_metrics['total_requests'] += 1
        
        if success:
            self.health_metrics['successful_requests'] += 1
        else:
            self.health_metrics['failed_requests'] += 1
            if error_type:
                self.error_counts[error_type] = self.error_counts.get(error_type, 0) + 1
        
        # æ›´æ–°æŒ‡æ ‡
        self._update_metrics()
    
    def _update_metrics(self):
        """æ›´æ–°å¥åº·æŒ‡æ ‡"""
        if self.response_times:
            self.health_metrics['avg_response_time'] = statistics.mean(self.response_times)
        
        if self.health_metrics['total_requests'] > 0:
            self.health_metrics['success_rate'] = (
                self.health_metrics['successful_requests'] / 
                self.health_metrics['total_requests'] * 100
            )
            self.health_metrics['error_rate'] = (
                self.health_metrics['failed_requests'] / 
                self.health_metrics['total_requests'] * 100
            )
        
        # è®¡ç®—ç¨³å®šæ€§è¯„åˆ†
        success_rate = self.health_metrics['success_rate']
        avg_response_time = self.health_metrics['avg_response_time']
        
        # åŸºç¡€è¯„åˆ†åŸºäºæˆåŠŸç‡
        stability_score = success_rate
        
        # å“åº”æ—¶é—´æƒ©ç½š
        if avg_response_time > 5.0:
            stability_score *= 0.8
        elif avg_response_time > 10.0:
            stability_score *= 0.6
        
        # è¿‘æœŸè¡¨ç°åŠ æƒ
        if len(self.success_history) >= 10:
            recent_success_rate = sum(self.success_history[-10:]) / 10 * 100
            stability_score = (stability_score * 0.7) + (recent_success_rate * 0.3)
        
        self.health_metrics['stability_score'] = round(stability_score, 1)
        self.health_metrics['last_check'] = datetime.now().isoformat()
    
    def get_health_status(self) -> Dict[str, Any]:
        """è·å–å¥åº·çŠ¶æ€"""
        status = "healthy"
        if self.health_metrics['stability_score'] < 80:
            status = "degraded"
        if self.health_metrics['stability_score'] < 60:
            status = "unhealthy"
        
        return {
            **self.health_metrics,
            'status': status,
            'error_distribution': dict(self.error_counts),
            'recommendations': self._generate_recommendations()
        }
    
    def _generate_recommendations(self) -> List[str]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        recommendations = []
        
        if self.health_metrics['success_rate'] < 95:
            recommendations.append("æé«˜é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶")
        
        if self.health_metrics['avg_response_time'] > 5.0:
            recommendations.append("ä¼˜åŒ–APIè¯·æ±‚æ€§èƒ½")
        
        if len(self.error_counts) > 3:
            recommendations.append("åˆ†æé”™è¯¯æ¨¡å¼å¹¶ä¼˜åŒ–")
        
        return recommendations


class OptimizedClaudeL2Wrapper:
    """ä¼˜åŒ–çš„Claude L2è¯­ä¹‰åˆ†æåŒ…è£…å™¨"""
    
    def __init__(self, base_url: str = "http://localhost:8081"):
        self.base_url = base_url
        self.session = None
        self.stability_monitor = ClaudeAPIStabilityMonitor()
        
        # é‡è¯•é…ç½®
        self.retry_config = {
            'max_retries': 3,
            'base_delay': 1.0,
            'max_delay': 8.0,
            'backoff_multiplier': 2.0
        }
        
        # L2è¯­ä¹‰åˆ†æä¸“ç”¨é…ç½®
        self.l2_config = {
            'enable_context_enhancement': True,
            'enable_business_rules_validation': True,
            'enable_semantic_similarity_check': True,
            'confidence_threshold': 0.7,
            'auto_approve_threshold': 0.9
        }
        
        # L2ä¸“ç”¨æç¤ºè¯æ¨¡æ¿
        self.l2_prompt_template = """
ä½ æ˜¯ä¸€ä¸ªä¸“é—¨å¤„ç†L2çº§åˆ«è¯­ä¹‰å˜æ›´çš„AIåˆ†æå¸ˆã€‚L2çº§åˆ«çš„ä¿®æ”¹éœ€è¦è¯­ä¹‰å®¡æ ¸ï¼Œä¸æ˜¯ç»å¯¹ç¦æ­¢çš„L1çº§åˆ«ï¼Œä¹Ÿä¸æ˜¯å¯ä»¥è‡ªç”±ç¼–è¾‘çš„L3çº§åˆ«ã€‚

è¯·åˆ†æä»¥ä¸‹è¡¨æ ¼å­—æ®µä¿®æ”¹çš„è¯­ä¹‰åˆç†æ€§ï¼š

## ä¿®æ”¹è¯¦æƒ…
è¡¨æ ¼èƒŒæ™¯: {table_context}
å­—æ®µåç§°: {column_name}
åŸå§‹å€¼: {original_value}
æ–°å€¼: {new_value}
ä¸šåŠ¡èƒŒæ™¯: {business_context}

## L2è¯­ä¹‰å®¡æ ¸è¦ç‚¹
1. **è¯­ä¹‰ä¸€è‡´æ€§**: ä¿®æ”¹æ˜¯å¦ä¿æŒåŸæœ‰è¯­ä¹‰æ„å›¾
2. **ä¸šåŠ¡é€»è¾‘æ€§**: ä¿®æ”¹æ˜¯å¦ç¬¦åˆä¸šåŠ¡æµç¨‹é€»è¾‘
3. **å½±å“èŒƒå›´è¯„ä¼°**: ä¿®æ”¹å¯èƒ½å½±å“çš„å…³è”å­—æ®µæˆ–æµç¨‹
4. **é£é™©ç­‰çº§åˆ¤æ–­**: è¯„ä¼°ä¿®æ”¹çš„æ½œåœ¨é£é™©

## ç‰¹æ®Šå…³æ³¨å­—æ®µï¼ˆL2çº§åˆ«ï¼‰
- è´Ÿè´£äººã€ååŠ©äºº: æ¶‰åŠæƒé™å’Œè´£ä»»è½¬ç§»
- å…·ä½“è®¡åˆ’å†…å®¹: å½±å“é¡¹ç›®èŒƒå›´å’Œèµ„æº
- ç›‘ç£äºº: æ¶‰åŠå®¡æ‰¹é“¾å’Œç®¡æ§
- é‡è¦ç¨‹åº¦: å½±å“ä¼˜å…ˆçº§å’Œèµ„æºåˆ†é…

è¯·ä»¥JSONæ ¼å¼è¿”å›åˆ†æç»“æœï¼š
{{
    "semantic_decision": "APPROVE|REJECT|REVIEW",
    "confidence_score": 0.0-1.0,
    "semantic_reasoning": "è¯¦ç»†çš„è¯­ä¹‰åˆ†æç†ç”±",
    "risk_level": "L1|L2|L3",
    "business_impact": "HIGH|MEDIUM|LOW",
    "approval_required": true|false,
    "auto_processable": true|false,
    "l2_specific_factors": ["è¯†åˆ«çš„L2çº§åˆ«ç‰¹å®šå› ç´ "]
}}
        """
        
        logger.info("âœ… ä¼˜åŒ–Claude L2è¯­ä¹‰åˆ†æåŒ…è£…å™¨åˆå§‹åŒ–å®Œæˆ")
    
    async def __aenter__(self):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        self.session = aiohttp.ClientSession(
            timeout=aiohttp.ClientTimeout(total=30),
            connector=aiohttp.TCPConnector(limit=10, limit_per_host=5)
        )
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡ºå£"""
        if self.session:
            await self.session.close()
    
    async def analyze_l2_semantic_change(self, request: L2SemanticAnalysisRequest) -> L2SemanticAnalysisResult:
        """åˆ†æL2è¯­ä¹‰å˜åŒ–"""
        start_time = time.time()
        
        try:
            # ç”ŸæˆL2ä¸“ç”¨åˆ†ææç¤ºè¯
            prompt = self._generate_l2_prompt(request)
            
            # è°ƒç”¨Claude API
            api_response = await self._call_claude_with_retry(prompt)
            
            # è§£æå¹¶éªŒè¯ç»“æœ
            analysis_result = self._parse_l2_response(api_response, request)
            
            processing_time = time.time() - start_time
            analysis_result.processing_time = processing_time
            
            # è®°å½•æˆåŠŸ
            self.stability_monitor.record_request(True, processing_time)
            
            logger.info(f"âœ… L2è¯­ä¹‰åˆ†æå®Œæˆ: {request.change_id}, å†³ç­–: {analysis_result.semantic_decision}")
            
            return analysis_result
            
        except Exception as e:
            processing_time = time.time() - start_time
            error_type = type(e).__name__
            
            # è®°å½•å¤±è´¥
            self.stability_monitor.record_request(False, processing_time, error_type)
            
            logger.error(f"âŒ L2è¯­ä¹‰åˆ†æå¤±è´¥: {request.change_id}, é”™è¯¯: {e}")
            
            # è¿”å›å®‰å…¨é»˜è®¤ç»“æœ
            return self._create_fallback_result(request, str(e), processing_time)
    
    async def analyze_l2_batch(self, requests: List[L2SemanticAnalysisRequest]) -> List[L2SemanticAnalysisResult]:
        """æ‰¹é‡L2è¯­ä¹‰åˆ†æ"""
        logger.info(f"ğŸ” å¼€å§‹æ‰¹é‡L2è¯­ä¹‰åˆ†æ: {len(requests)}ä¸ªä¿®æ”¹")
        
        results = []
        
        # æŒ‰ä¼˜å…ˆçº§æ’åº
        sorted_requests = sorted(requests, key=lambda x: {'high': 0, 'normal': 1, 'low': 2}[x.priority])
        
        # æ‰¹é‡å¤„ç†
        batch_size = 3  # æ§åˆ¶å¹¶å‘æ•°é¿å…APIé™æµ
        for i in range(0, len(sorted_requests), batch_size):
            batch = sorted_requests[i:i + batch_size]
            
            # å¹¶å‘å¤„ç†å½“å‰æ‰¹æ¬¡
            batch_tasks = [self.analyze_l2_semantic_change(req) for req in batch]
            batch_results = await asyncio.gather(*batch_tasks, return_exceptions=True)
            
            # å¤„ç†ç»“æœ
            for req, result in zip(batch, batch_results):
                if isinstance(result, Exception):
                    fallback_result = self._create_fallback_result(req, str(result), 0.0)
                    results.append(fallback_result)
                else:
                    results.append(result)
            
            # æ‰¹æ¬¡é—´æš‚åœé¿å…APIé™æµ
            if i + batch_size < len(sorted_requests):
                await asyncio.sleep(0.5)
        
        logger.info(f"âœ… æ‰¹é‡L2è¯­ä¹‰åˆ†æå®Œæˆ: {len(results)}ä¸ªç»“æœ")
        return results
    
    def _generate_l2_prompt(self, request: L2SemanticAnalysisRequest) -> str:
        """ç”ŸæˆL2ä¸“ç”¨åˆ†ææç¤ºè¯"""
        
        # å¢å¼ºä¸Šä¸‹æ–‡ä¿¡æ¯
        enhanced_context = self._enhance_context(request)
        
        prompt = self.l2_prompt_template.format(
            table_context=enhanced_context['table_context'],
            column_name=request.column_name,
            original_value=request.original_value,
            new_value=request.new_value,
            business_context=enhanced_context['business_context']
        )
        
        return prompt.strip()
    
    def _enhance_context(self, request: L2SemanticAnalysisRequest) -> Dict[str, str]:
        """å¢å¼ºä¸Šä¸‹æ–‡ä¿¡æ¯"""
        
        # L2çº§åˆ«å­—æ®µçš„ç‰¹æ®Šè§„åˆ™
        l2_field_rules = {
            'è´Ÿè´£äºº': 'æ¶‰åŠé¡¹ç›®è´£ä»»è½¬ç§»ï¼Œéœ€ç¡®è®¤æ–°è´Ÿè´£äººå…·å¤‡ç›¸åº”èƒ½åŠ›å’Œæƒé™',
            'ååŠ©äºº': 'å½±å“å›¢é˜Ÿåä½œï¼Œéœ€è¯„ä¼°å·¥ä½œè´Ÿè·åˆ†é…å’ŒæŠ€èƒ½åŒ¹é…',
            'ç›‘ç£äºº': 'æ¶‰åŠç®¡æ§é“¾è·¯ï¼Œéœ€ç¡®è®¤æ–°ç›‘ç£äººçš„ç®¡ç†æƒé™',
            'å…·ä½“è®¡åˆ’å†…å®¹': 'å½±å“é¡¹ç›®èŒƒå›´å’Œèµ„æºåˆ†é…ï¼Œéœ€è¯„ä¼°å˜æ›´çš„åˆç†æ€§',
            'é‡è¦ç¨‹åº¦': 'å½±å“ä¼˜å…ˆçº§å’Œèµ„æºæŠ•å…¥ï¼Œéœ€è¯„ä¼°è°ƒæ•´çš„ä¸šåŠ¡ä¾æ®',
            'é¢„è®¡å®Œæˆæ—¶é—´': 'å½±å“é¡¹ç›®æ’æœŸå’Œé‡Œç¨‹ç¢‘ï¼Œéœ€è¯„ä¼°æ—¶é—´è°ƒæ•´çš„å¯è¡Œæ€§'
        }
        
        # è·å–å­—æ®µç‰¹å®šè§„åˆ™
        field_rule = l2_field_rules.get(request.column_name, 'é€šç”¨L2å­—æ®µï¼Œéœ€è¦è¯­ä¹‰å®¡æ ¸ç¡®è®¤')
        
        # å¢å¼ºä¸šåŠ¡ä¸Šä¸‹æ–‡
        enhanced_business_context = f"{request.business_context}\n\nå­—æ®µè§„åˆ™: {field_rule}"
        
        # è¯­ä¹‰ç›¸ä¼¼æ€§åˆ†æ
        similarity_info = self._analyze_semantic_similarity(request.original_value, request.new_value)
        
        return {
            'table_context': f"{request.table_context}\nè¯­ä¹‰ç›¸ä¼¼åº¦: {similarity_info}",
            'business_context': enhanced_business_context
        }
    
    def _analyze_semantic_similarity(self, original: str, new: str) -> str:
        """åˆ†æè¯­ä¹‰ç›¸ä¼¼æ€§"""
        try:
            from difflib import SequenceMatcher
            
            # å­—ç¬¦çº§ç›¸ä¼¼åº¦
            char_similarity = SequenceMatcher(None, str(original), str(new)).ratio()
            
            # è¯æ±‡çº§ç›¸ä¼¼åº¦ï¼ˆç®€åŒ–ç‰ˆï¼‰
            orig_words = set(str(original).split())
            new_words = set(str(new).split())
            
            if orig_words or new_words:
                word_similarity = len(orig_words & new_words) / max(len(orig_words | new_words), 1)
            else:
                word_similarity = 1.0
            
            # é•¿åº¦ç›¸ä¼¼åº¦
            if len(str(original)) == 0 and len(str(new)) == 0:
                length_similarity = 1.0
            else:
                max_len = max(len(str(original)), len(str(new)))
                min_len = min(len(str(original)), len(str(new)))
                length_similarity = min_len / max_len if max_len > 0 else 0.0
            
            # ç»¼åˆç›¸ä¼¼åº¦
            overall_similarity = (char_similarity + word_similarity + length_similarity) / 3
            
            if overall_similarity > 0.8:
                return f"é«˜ç›¸ä¼¼åº¦({overall_similarity:.2f}) - å¯èƒ½æ˜¯æ ¼å¼è°ƒæ•´æˆ–é”™è¯¯ä¿®æ­£"
            elif overall_similarity > 0.5:
                return f"ä¸­ç­‰ç›¸ä¼¼åº¦({overall_similarity:.2f}) - éƒ¨åˆ†å†…å®¹ä¿®æ”¹"
            else:
                return f"ä½ç›¸ä¼¼åº¦({overall_similarity:.2f}) - é‡è¦å†…å®¹å˜æ›´"
                
        except Exception:
            return "ç›¸ä¼¼åº¦åˆ†æä¸å¯ç”¨"
    
    async def _call_claude_with_retry(self, prompt: str) -> Dict[str, Any]:
        """å¸¦é‡è¯•çš„Claude APIè°ƒç”¨"""
        last_error = None
        
        for attempt in range(self.retry_config['max_retries'] + 1):
            try:
                # æ„å»ºè¯·æ±‚è½½è· - ä½¿ç”¨ClaudeæœåŠ¡æ”¯æŒçš„æ ¼å¼
                payload = {
                    "content": prompt,
                    "analysis_type": "risk_assessment"  # ä½¿ç”¨æ”¯æŒçš„åˆ†æç±»å‹
                }
                
                # è°ƒç”¨API
                async with self.session.post(
                    f"{self.base_url}/analyze",
                    json=payload,
                    timeout=aiohttp.ClientTimeout(total=15)
                ) as response:
                    
                    if response.status == 200:
                        return await response.json()
                    elif response.status == 429:  # é™æµ
                        if attempt < self.retry_config['max_retries']:
                            delay = min(
                                self.retry_config['base_delay'] * 
                                (self.retry_config['backoff_multiplier'] ** attempt),
                                self.retry_config['max_delay']
                            )
                            logger.warning(f"APIé™æµï¼Œç­‰å¾…{delay}ç§’åé‡è¯•...")
                            await asyncio.sleep(delay)
                            continue
                    else:
                        error_text = await response.text()
                        raise Exception(f"HTTP {response.status}: {error_text}")
                        
            except asyncio.TimeoutError:
                last_error = Exception("APIè°ƒç”¨è¶…æ—¶")
                if attempt < self.retry_config['max_retries']:
                    await asyncio.sleep(2 ** attempt)
                    continue
            except Exception as e:
                last_error = e
                if attempt < self.retry_config['max_retries']:
                    await asyncio.sleep(1.0)
                    continue
                    
        raise last_error or Exception("APIè°ƒç”¨å¤±è´¥")
    
    def _parse_l2_response(self, api_response: Dict[str, Any], request: L2SemanticAnalysisRequest) -> L2SemanticAnalysisResult:
        """è§£æL2åˆ†æå“åº”"""
        try:
            # æå–ç»“æœå†…å®¹
            result_content = api_response.get('result', '')
            confidence = api_response.get('confidence', 0.0)
            
            # å°è¯•è§£æJSONç»“æœ
            json_data = self._extract_json_from_response(result_content)
            
            if json_data:
                # éªŒè¯å’Œæ ‡å‡†åŒ–ç»“æœ
                validated = self._validate_l2_result(json_data)
                
                return L2SemanticAnalysisResult(
                    change_id=request.change_id,
                    semantic_decision=validated['semantic_decision'],
                    confidence_score=validated['confidence_score'],
                    semantic_reasoning=validated['semantic_reasoning'],
                    risk_level=validated['risk_level'],
                    business_impact=validated['business_impact'],
                    approval_required=validated['approval_required'],
                    auto_processable=validated['auto_processable'],
                    l2_specific_factors=validated['l2_specific_factors'],
                    processing_time=0.0,  # å°†åœ¨å¤–éƒ¨è®¾ç½®
                    api_model_used=api_response.get('model', 'claude-wrapper'),
                    analysis_timestamp=datetime.now().isoformat()
                )
            else:
                # JSONè§£æå¤±è´¥ï¼Œä½¿ç”¨å¯å‘å¼æ–¹æ³•
                return self._heuristic_parse_result(result_content, request, confidence)
                
        except Exception as e:
            logger.error(f"è§£æL2å“åº”å¤±è´¥: {e}")
            raise Exception(f"å“åº”è§£æå¤±è´¥: {e}")
    
    def _extract_json_from_response(self, content: str) -> Optional[Dict[str, Any]]:
        """ä»å“åº”ä¸­æå–JSON"""
        try:
            # æŸ¥æ‰¾JSONéƒ¨åˆ†
            json_start = content.find('{')
            json_end = content.rfind('}') + 1
            
            if json_start != -1 and json_end > json_start:
                json_str = content[json_start:json_end]
                return json.loads(json_str)
                
        except json.JSONDecodeError:
            pass
        
        return None
    
    def _heuristic_parse_result(self, content: str, request: L2SemanticAnalysisRequest, confidence: float) -> L2SemanticAnalysisResult:
        """å¯å‘å¼è§£æç»“æœ"""
        content_lower = content.lower()
        
        # å†³ç­–åˆ¤æ–­
        if 'approve' in content_lower or 'æ‰¹å‡†' in content_lower or 'é€šè¿‡' in content_lower:
            decision = 'APPROVE'
        elif 'reject' in content_lower or 'æ‹’ç»' in content_lower or 'ç¦æ­¢' in content_lower:
            decision = 'REJECT'
        else:
            decision = 'REVIEW'
        
        # ä¸šåŠ¡å½±å“
        if 'high' in content_lower or 'é«˜' in content_lower or 'ä¸¥é‡' in content_lower:
            impact = 'HIGH'
        elif 'low' in content_lower or 'ä½' in content_lower or 'è½»å¾®' in content_lower:
            impact = 'LOW'
        else:
            impact = 'MEDIUM'
        
        return L2SemanticAnalysisResult(
            change_id=request.change_id,
            semantic_decision=decision,
            confidence_score=confidence,
            semantic_reasoning=content[:500],  # æˆªå–å‰500å­—ç¬¦
            risk_level='L2',  # é»˜è®¤L2
            business_impact=impact,
            approval_required=decision != 'APPROVE',
            auto_processable=decision == 'APPROVE' and confidence > 0.8,
            l2_specific_factors=[request.column_name],
            processing_time=0.0,
            api_model_used='claude-wrapper-heuristic',
            analysis_timestamp=datetime.now().isoformat()
        )
    
    def _validate_l2_result(self, json_data: Dict[str, Any]) -> Dict[str, Any]:
        """éªŒè¯L2åˆ†æç»“æœ"""
        validated = {}
        
        # è¯­ä¹‰å†³ç­–
        decision = json_data.get('semantic_decision', 'REVIEW')
        validated['semantic_decision'] = decision if decision in ['APPROVE', 'REJECT', 'REVIEW'] else 'REVIEW'
        
        # ç½®ä¿¡åº¦
        confidence = json_data.get('confidence_score', 0.0)
        try:
            confidence = float(confidence)
            validated['confidence_score'] = max(0.0, min(1.0, confidence))
        except (ValueError, TypeError):
            validated['confidence_score'] = 0.0
        
        # å…¶ä»–å­—æ®µ
        validated['semantic_reasoning'] = str(json_data.get('semantic_reasoning', ''))[:1000]  # é™åˆ¶é•¿åº¦
        
        risk_level = json_data.get('risk_level', 'L2')
        validated['risk_level'] = risk_level if risk_level in ['L1', 'L2', 'L3'] else 'L2'
        
        business_impact = json_data.get('business_impact', 'MEDIUM')
        validated['business_impact'] = business_impact if business_impact in ['HIGH', 'MEDIUM', 'LOW'] else 'MEDIUM'
        
        validated['approval_required'] = bool(json_data.get('approval_required', True))
        validated['auto_processable'] = bool(json_data.get('auto_processable', False))
        
        # L2ç‰¹å®šå› ç´ 
        l2_factors = json_data.get('l2_specific_factors', [])
        if isinstance(l2_factors, list):
            validated['l2_specific_factors'] = [str(f)[:100] for f in l2_factors[:10]]  # æœ€å¤š10ä¸ªï¼Œæ¯ä¸ªæœ€å¤š100å­—ç¬¦
        else:
            validated['l2_specific_factors'] = []
        
        return validated
    
    def _create_fallback_result(self, request: L2SemanticAnalysisRequest, error_msg: str, processing_time: float) -> L2SemanticAnalysisResult:
        """åˆ›å»ºé™çº§ç»“æœ"""
        return L2SemanticAnalysisResult(
            change_id=request.change_id,
            semantic_decision='REVIEW',
            confidence_score=0.0,
            semantic_reasoning=f"AIåˆ†æå¤±è´¥ï¼Œéœ€è¦äººå·¥å®¡æ ¸ã€‚é”™è¯¯ä¿¡æ¯: {error_msg[:200]}",
            risk_level='L2',
            business_impact='MEDIUM',
            approval_required=True,
            auto_processable=False,
            l2_specific_factors=['åˆ†æå¤±è´¥', request.column_name],
            processing_time=processing_time,
            api_model_used='fallback',
            analysis_timestamp=datetime.now().isoformat()
        )
    
    def get_stability_status(self) -> Dict[str, Any]:
        """è·å–ç¨³å®šæ€§çŠ¶æ€"""
        return self.stability_monitor.get_health_status()


# å‘½ä»¤è¡Œæ¥å£
async def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='ä¼˜åŒ–Claude L2è¯­ä¹‰åˆ†æAPI')
    parser.add_argument('--url', default='http://localhost:8081', help='ClaudeæœåŠ¡URL')
    parser.add_argument('--test', action='store_true', help='è¿è¡Œæµ‹è¯•')
    parser.add_argument('--status', action='store_true', help='æ˜¾ç¤ºç¨³å®šæ€§çŠ¶æ€')
    
    args = parser.parse_args()
    
    async with OptimizedClaudeL2Wrapper(args.url) as wrapper:
        if args.status:
            print("ğŸ” Claude L2è¯­ä¹‰åˆ†æAPIç¨³å®šæ€§çŠ¶æ€:")
            status = wrapper.get_stability_status()
            print(f"   çŠ¶æ€: {status['status']}")
            print(f"   ç¨³å®šæ€§è¯„åˆ†: {status['stability_score']}/100")
            print(f"   æˆåŠŸç‡: {status['success_rate']:.1f}%")
            print(f"   å¹³å‡å“åº”æ—¶é—´: {status['avg_response_time']:.2f}ç§’")
            print(f"   æ€»è¯·æ±‚æ•°: {status['total_requests']}")
            if status['recommendations']:
                print("   æ”¹è¿›å»ºè®®:")
                for rec in status['recommendations']:
                    print(f"     - {rec}")
        
        if args.test:
            print("ğŸ§ª å¼€å§‹L2è¯­ä¹‰åˆ†ææµ‹è¯•...")
            
            # æµ‹è¯•è¯·æ±‚
            test_request = L2SemanticAnalysisRequest(
                change_id="test_001",
                column_name="è´Ÿè´£äºº",
                original_value="å¼ ä¸‰",
                new_value="æå››",
                table_context="å°çº¢ä¹¦éƒ¨é—¨é¡¹ç›®ç®¡ç†è¡¨",
                business_context="é¡¹ç›®è´Ÿè´£äººè°ƒæ•´ï¼Œéœ€è¦ç¡®è®¤æƒé™è½¬ç§»",
                priority="high"
            )
            
            # æ‰§è¡Œæµ‹è¯•
            result = await wrapper.analyze_l2_semantic_change(test_request)
            
            print(f"\nâœ… æµ‹è¯•å®Œæˆ:")
            print(f"   å˜æ›´ID: {result.change_id}")
            print(f"   è¯­ä¹‰å†³ç­–: {result.semantic_decision}")
            print(f"   ç½®ä¿¡åº¦: {result.confidence_score:.2f}")
            print(f"   é£é™©ç­‰çº§: {result.risk_level}")
            print(f"   ä¸šåŠ¡å½±å“: {result.business_impact}")
            print(f"   éœ€è¦å®¡æ‰¹: {result.approval_required}")
            print(f"   å¯è‡ªåŠ¨å¤„ç†: {result.auto_processable}")
            print(f"   å¤„ç†æ—¶é—´: {result.processing_time:.3f}ç§’")
            print(f"   åˆ†æç†ç”±: {result.semantic_reasoning[:100]}...")
            
            # æ˜¾ç¤ºç¨³å®šæ€§çŠ¶æ€
            status = wrapper.get_stability_status()
            print(f"\nğŸ“Š APIç¨³å®šæ€§: {status['stability_score']}/100 ({status['status']})")


if __name__ == "__main__":
    asyncio.run(main())