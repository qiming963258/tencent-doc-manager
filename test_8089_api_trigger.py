#!/usr/bin/env python3
"""
é€šè¿‡8089 APIè§¦å‘çœŸå®å·¥ä½œæµæµ‹è¯•
ç›‘æ§å®Œæ•´çš„ä¸‹è½½-å¯¹æ¯”-æ‰“åˆ†-ä¸Šä¼ é“¾è·¯
"""

import asyncio
import aiohttp
import json
import logging
import time
from pathlib import Path
from datetime import datetime
import csv

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class Real8089WorkflowTest:
    """çœŸå®çš„8089å·¥ä½œæµæµ‹è¯•"""

    def __init__(self):
        self.base_dir = Path('/root/projects/tencent-doc-manager')
        self.api_base = 'http://localhost:8089'
        self.test_url = 'https://docs.qq.com/sheet/DWEFNU25TemFnZXJN'

    async def prepare_config(self):
        """å‡†å¤‡é…ç½®ï¼šç¡®ä¿æœ‰URLå’ŒCookie"""
        logger.info("ğŸ“‹ Step 1: å‡†å¤‡é…ç½®")

        # ç¡®ä¿Cookieå­˜åœ¨
        cookie_file = self.base_dir / 'config/cookies.json'
        if not cookie_file.exists():
            logger.error("âŒ Cookieæ–‡ä»¶ä¸å­˜åœ¨")
            return False

        with open(cookie_file) as f:
            cookie_data = json.load(f)
            if not cookie_data.get('current_cookies'):
                logger.error("âŒ Cookieä¸ºç©º")
                return False
            logger.info("âœ… Cookieé…ç½®æœ‰æ•ˆ")

        # é…ç½®ä¸‹è½½é“¾æ¥
        async with aiohttp.ClientSession() as session:
            config_data = {
                "links": [
                    {
                        "name": "æµ‹è¯•æ–‡æ¡£-å‡ºå›½é”€å”®è®¡åˆ’è¡¨",
                        "url": self.test_url,
                        "category": "æµ‹è¯•",
                        "description": "8089å·¥ä½œæµæµ‹è¯•æ–‡æ¡£"
                    }
                ]
            }

            # ä¿å­˜ä¸‹è½½é“¾æ¥é…ç½®
            async with session.post(
                f'{self.api_base}/api/save-download-links',
                json=config_data
            ) as response:
                if response.status == 200:
                    result = await response.json()
                    logger.info(f"âœ… é…ç½®ä¿å­˜æˆåŠŸ: {result}")
                    return True
                else:
                    logger.error(f"âŒ é…ç½®ä¿å­˜å¤±è´¥: {response.status}")
                    return False

    async def prepare_baseline(self):
        """å‡†å¤‡åŸºçº¿æ–‡ä»¶ï¼ˆå¦‚æœéœ€è¦ï¼‰"""
        logger.info("ğŸ“‹ Step 2: å‡†å¤‡åŸºçº¿æ–‡ä»¶")

        # æ£€æŸ¥æ˜¯å¦å·²æœ‰åŸºçº¿
        from production.core_modules.week_time_manager import WeekTimeManager
        week_mgr = WeekTimeManager()
        week_info = week_mgr.get_current_week_info()

        baseline_dir = self.base_dir / f"csv_versions/2025_W{week_info['week_number']}/baseline"
        baseline_files = list(baseline_dir.glob("*å‡ºå›½é”€å”®*.csv"))

        if baseline_files:
            logger.info(f"âœ… å‘ç°åŸºçº¿æ–‡ä»¶: {baseline_files[0].name}")
            return str(baseline_files[0])
        else:
            logger.info("âš ï¸ æœªå‘ç°åŸºçº¿æ–‡ä»¶ï¼Œå°†ä¸‹è½½ä½œä¸ºåŸºçº¿")
            # å…ˆä¸‹è½½ä¸€ä¸ªä½œä¸ºåŸºçº¿
            baseline_dir.mkdir(parents=True, exist_ok=True)

            # ä½¿ç”¨PlaywrightDownloaderä¸‹è½½åŸºçº¿
            from production.core_modules.playwright_downloader import PlaywrightDownloader

            cookie_file = self.base_dir / 'config/cookies.json'
            with open(cookie_file) as f:
                cookie_data = json.load(f)
            cookie_string = cookie_data.get('current_cookies', '')

            downloader = PlaywrightDownloader()
            result = await downloader.download(
                url=self.test_url,
                cookies=cookie_string,
                format='csv',
                download_dir=str(baseline_dir)
            )

            if result and result.get('success'):
                baseline_file = result.get('file_path')
                # é‡å‘½åä¸ºbaseline
                new_name = baseline_file.replace('midweek', 'baseline')
                Path(baseline_file).rename(new_name)
                logger.info(f"âœ… åˆ›å»ºåŸºçº¿æ–‡ä»¶: {Path(new_name).name}")
                return new_name
            else:
                logger.error("âŒ æ— æ³•åˆ›å»ºåŸºçº¿æ–‡ä»¶")
                return None

    async def trigger_workflow(self):
        """è§¦å‘8089å·¥ä½œæµ"""
        logger.info("ğŸš€ Step 3: è§¦å‘8089å·¥ä½œæµ")

        async with aiohttp.ClientSession() as session:
            # è°ƒç”¨start-download API
            logger.info("ğŸ“¡ è°ƒç”¨ /api/start-download")

            async with session.post(
                f'{self.api_base}/api/start-download',
                json={},
                timeout=aiohttp.ClientTimeout(total=300)
            ) as response:
                if response.status == 200:
                    result = await response.json()
                    if result.get('success'):
                        logger.info(f"âœ… å·¥ä½œæµå¯åŠ¨æˆåŠŸ: {result}")
                        return True
                    else:
                        logger.error(f"âŒ å·¥ä½œæµå¯åŠ¨å¤±è´¥: {result.get('error')}")
                        return False
                else:
                    logger.error(f"âŒ APIè°ƒç”¨å¤±è´¥: {response.status}")
                    text = await response.text()
                    logger.error(f"å“åº”å†…å®¹: {text[:500]}")
                    return False

    async def monitor_progress(self, timeout=120):
        """ç›‘æ§å·¥ä½œæµè¿›åº¦"""
        logger.info("ğŸ“Š Step 4: ç›‘æ§å·¥ä½œæµè¿›åº¦")

        start_time = time.time()
        last_log_count = 0

        async with aiohttp.ClientSession() as session:
            while time.time() - start_time < timeout:
                # è·å–å·¥ä½œæµçŠ¶æ€
                try:
                    async with session.get(
                        f'{self.api_base}/api/workflow-status'
                    ) as response:
                        if response.status == 200:
                            status = await response.json()
                            if status.get('running'):
                                logger.info(f"ğŸ”„ å·¥ä½œæµè¿è¡Œä¸­: {status.get('current_step', 'æœªçŸ¥æ­¥éª¤')}")

                                # è·å–æ—¥å¿—
                                logs = status.get('logs', [])
                                if len(logs) > last_log_count:
                                    for log in logs[last_log_count:]:
                                        logger.info(f"  ğŸ“ {log}")
                                    last_log_count = len(logs)
                            else:
                                logger.info("âœ… å·¥ä½œæµå®Œæˆ")
                                return status
                except:
                    pass

                await asyncio.sleep(2)

        logger.warning("â±ï¸ ç›‘æ§è¶…æ—¶")
        return None

    async def check_results(self):
        """æ£€æŸ¥å·¥ä½œæµç»“æœ"""
        logger.info("ğŸ” Step 5: æ£€æŸ¥å·¥ä½œæµç»“æœ")

        results = {}

        # 1. æ£€æŸ¥æ˜¯å¦æœ‰æ–°çš„CSVæ–‡ä»¶
        from production.core_modules.week_time_manager import WeekTimeManager
        week_mgr = WeekTimeManager()
        week_info = week_mgr.get_current_week_info()

        midweek_dir = self.base_dir / f"csv_versions/2025_W{week_info['week_number']}/midweek"
        recent_files = sorted(
            midweek_dir.glob("*.csv"),
            key=lambda x: x.stat().st_mtime,
            reverse=True
        )

        if recent_files:
            latest_csv = recent_files[0]
            # æ£€æŸ¥æ˜¯å¦æ˜¯æœ€è¿‘5åˆ†é’Ÿå†…çš„
            if time.time() - latest_csv.stat().st_mtime < 300:
                logger.info(f"âœ… å‘ç°æ–°ä¸‹è½½çš„CSV: {latest_csv.name}")
                results['csv_file'] = str(latest_csv)

        # 2. æ£€æŸ¥æ˜¯å¦æœ‰æ–°çš„ç»¼åˆæ‰“åˆ†æ–‡ä»¶
        scoring_dir = self.base_dir / 'scoring_results/comprehensive'
        scoring_files = sorted(
            scoring_dir.glob('comprehensive_*.json'),
            key=lambda x: x.stat().st_mtime,
            reverse=True
        )

        if scoring_files:
            latest_scoring = scoring_files[0]
            if time.time() - latest_scoring.stat().st_mtime < 300:
                logger.info(f"âœ… å‘ç°æ–°çš„ç»¼åˆæ‰“åˆ†: {latest_scoring.name}")

                with open(latest_scoring) as f:
                    scoring_data = json.load(f)
                    logger.info(f"   - å‚æ•°æ€»æ•°: {scoring_data.get('metadata', {}).get('total_params', 0)}")
                    logger.info(f"   - è¡¨æ ¼æ•°: {len(scoring_data.get('table_details', []))}")

                    # æ£€æŸ¥æ˜¯å¦åŒ…å«9ç±»å¿…éœ€å‚æ•°
                    required_keys = [
                        'table_names', 'column_names', 'heatmap_data',
                        'table_details', 'hover_data', 'statistics'
                    ]
                    missing_keys = [k for k in required_keys if k not in scoring_data]
                    if missing_keys:
                        logger.warning(f"   âš ï¸ ç¼ºå°‘å‚æ•°: {missing_keys}")
                    else:
                        logger.info("   âœ… åŒ…å«æ‰€æœ‰9ç±»å¿…éœ€å‚æ•°")

                results['scoring_file'] = str(latest_scoring)

        # 3. æ£€æŸ¥æ˜¯å¦æœ‰æ–°çš„Excelæ–‡ä»¶
        excel_dirs = [
            self.base_dir / "excel_outputs/marked",
            self.base_dir / "excel_outputs/complete_with_upload",
            self.base_dir / "excel_outputs"
        ]

        for excel_dir in excel_dirs:
            if excel_dir.exists():
                excel_files = sorted(
                    excel_dir.glob("*.xlsx"),
                    key=lambda x: x.stat().st_mtime,
                    reverse=True
                )

                if excel_files:
                    latest_excel = excel_files[0]
                    if time.time() - latest_excel.stat().st_mtime < 300:
                        logger.info(f"âœ… å‘ç°æ–°çš„Excel: {latest_excel.name}")
                        results['excel_file'] = str(latest_excel)
                        break

        # 4. æ£€æŸ¥æ˜¯å¦æœ‰URLè¿”å›
        url_file = self.base_dir / 'scoring_results/comprehensive/table_urls.json'
        if url_file.exists():
            with open(url_file) as f:
                url_data = json.load(f)
                if url_data.get('table_urls'):
                    for name, url in url_data['table_urls'].items():
                        logger.info(f"âœ… å‘ç°ä¸Šä¼ URL: {url}")
                        results['upload_url'] = url
                        break

        return results

    async def run_test(self):
        """è¿è¡Œå®Œæ•´æµ‹è¯•"""
        logger.info("=" * 70)
        logger.info("ğŸš€ å¼€å§‹8089çœŸå®å·¥ä½œæµæµ‹è¯•")
        logger.info("=" * 70)

        # å‡†å¤‡é…ç½®
        if not await self.prepare_config():
            logger.error("âŒ é…ç½®å‡†å¤‡å¤±è´¥")
            return None

        # å‡†å¤‡åŸºçº¿
        baseline = await self.prepare_baseline()
        if baseline:
            logger.info(f"ğŸ“Š åŸºçº¿æ–‡ä»¶: {Path(baseline).name}")

        # è§¦å‘å·¥ä½œæµ
        if not await self.trigger_workflow():
            logger.error("âŒ å·¥ä½œæµè§¦å‘å¤±è´¥")
            return None

        # ç›‘æ§è¿›åº¦
        await self.monitor_progress()

        # ç­‰å¾…å¤„ç†å®Œæˆ
        logger.info("â³ ç­‰å¾…å¤„ç†å®Œæˆ...")
        await asyncio.sleep(10)

        # æ£€æŸ¥ç»“æœ
        results = await self.check_results()

        logger.info("=" * 70)
        if results.get('upload_url'):
            logger.info("âœ… æµ‹è¯•æˆåŠŸï¼")
            logger.info(f"ğŸ”— æœ€ç»ˆURL: {results['upload_url']}")
        else:
            logger.warning("âš ï¸ æœªè·å–åˆ°æœ€ç»ˆURL")
            logger.info("ç”Ÿæˆçš„æ–‡ä»¶:")
            for key, value in results.items():
                logger.info(f"  - {key}: {Path(value).name if value else 'æ— '}")

        logger.info("=" * 70)

        return results.get('upload_url')

async def main():
    """ä¸»å‡½æ•°"""
    tester = Real8089WorkflowTest()
    final_url = await tester.run_test()

    if final_url:
        print(f"\n" + "=" * 70)
        print(f"ğŸ¯ æœ€ç»ˆè¿”å›çš„URL: {final_url}")
        print(f"=" * 70)
        print(f"\nè¯·éªŒè¯ä»¥ä¸‹å†…å®¹:")
        print(f"1. è®¿é—®URLæŸ¥çœ‹æ˜¯å¦æœ‰æ¶‚è‰²: {final_url}")
        print(f"2. æ£€æŸ¥8089 UIæ˜¯å¦æ›´æ–°: http://202.140.143.88:8089/")
        print(f"3. ç‚¹å‡»è¡¨æ ¼åç§°æ˜¯å¦è·³è½¬åˆ°æ­£ç¡®URL")
    else:
        print(f"\nâŒ æœªè·å–åˆ°æœ€ç»ˆURL")

    return final_url

if __name__ == "__main__":
    result = asyncio.run(main())