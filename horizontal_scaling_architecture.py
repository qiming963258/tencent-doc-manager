#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ°´å¹³æ‰©å±•æ¶æ„è®¾è®¡
å®ç°å¤šå®ä¾‹åè°ƒã€è´Ÿè½½å‡è¡¡ã€Cookieæ± ç®¡ç†
"""

import asyncio
import json
import logging
import time
import uuid
import redis
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass, asdict
from enum import Enum
import aiohttp
import hashlib

logger = logging.getLogger(__name__)

class TaskStatus(Enum):
    """ä»»åŠ¡çŠ¶æ€"""
    PENDING = "pending"
    ASSIGNED = "assigned"
    IN_PROGRESS = "in_progress"
    COMPLETED = "completed"
    FAILED = "failed"
    RETRY = "retry"

class InstanceStatus(Enum):
    """å®ä¾‹çŠ¶æ€"""
    HEALTHY = "healthy"
    BUSY = "busy"
    DEGRADED = "degraded"
    OFFLINE = "offline"

@dataclass
class DownloadTask:
    """ä¸‹è½½ä»»åŠ¡"""
    task_id: str
    doc_url: str
    doc_id: str
    format_type: str
    priority: int = 0
    created_at: datetime = None
    assigned_at: Optional[datetime] = None
    completed_at: Optional[datetime] = None
    assigned_instance: Optional[str] = None
    status: TaskStatus = TaskStatus.PENDING
    retry_count: int = 0
    max_retries: int = 3
    error_message: Optional[str] = None
    result_path: Optional[str] = None
    
    def __post_init__(self):
        if self.created_at is None:
            self.created_at = datetime.now()

@dataclass
class InstanceInfo:
    """å®ä¾‹ä¿¡æ¯"""
    instance_id: str
    host: str
    port: int
    status: InstanceStatus
    current_tasks: int
    max_concurrent_tasks: int
    last_heartbeat: datetime
    health_score: float
    total_processed: int
    success_rate: float
    avg_response_time: float
    
    def to_dict(self):
        return asdict(self)

class CookiePool:
    """Cookieæ± ç®¡ç†å™¨"""
    
    def __init__(self, redis_client):
        self.redis_client = redis_client
        self.pool_key = "cookie_pool"
        self.usage_key = "cookie_usage"
        self.blacklist_key = "cookie_blacklist"
        
    async def add_cookie(self, cookie_data: Dict[str, Any]) -> bool:
        """æ·»åŠ Cookieåˆ°æ± ä¸­"""
        try:
            cookie_id = hashlib.md5(cookie_data["cookie_string"].encode()).hexdigest()
            
            cookie_info = {
                "cookie_id": cookie_id,
                "cookie_string": cookie_data["cookie_string"],
                "account_info": cookie_data.get("account_info", {}),
                "added_at": datetime.now().isoformat(),
                "last_used": None,
                "usage_count": 0,
                "success_count": 0,
                "failure_count": 0,
                "is_active": True,
                "daily_usage_limit": cookie_data.get("daily_limit", 100),
                "concurrent_usage_limit": cookie_data.get("concurrent_limit", 2)
            }
            
            await self.redis_client.hset(self.pool_key, cookie_id, json.dumps(cookie_info))
            logger.info(f"âœ… Cookie {cookie_id[:8]} å·²æ·»åŠ åˆ°æ± ä¸­")
            return True
        
        except Exception as e:
            logger.error(f"âŒ æ·»åŠ Cookieå¤±è´¥: {e}")
            return False
    
    async def get_available_cookie(self, exclude_ids: List[str] = None) -> Optional[Dict[str, Any]]:
        """è·å–å¯ç”¨çš„Cookie"""
        try:
            exclude_ids = exclude_ids or []
            
            # è·å–æ‰€æœ‰Cookie
            all_cookies = await self.redis_client.hgetall(self.pool_key)
            
            available_cookies = []
            
            for cookie_id, cookie_data in all_cookies.items():
                if isinstance(cookie_id, bytes):
                    cookie_id = cookie_id.decode()
                
                if cookie_id in exclude_ids:
                    continue
                
                cookie_info = json.loads(cookie_data)
                
                # æ£€æŸ¥Cookieæ˜¯å¦å¯ç”¨
                if not cookie_info.get("is_active", True):
                    continue
                
                # æ£€æŸ¥æ˜¯å¦åœ¨é»‘åå•ä¸­
                is_blacklisted = await self.redis_client.sismember(self.blacklist_key, cookie_id)
                if is_blacklisted:
                    continue
                
                # æ£€æŸ¥å½“å‰ä½¿ç”¨æ•°é‡
                current_usage = await self._get_current_usage(cookie_id)
                if current_usage >= cookie_info.get("concurrent_usage_limit", 2):
                    continue
                
                # æ£€æŸ¥æ¯æ—¥ä½¿ç”¨é™åˆ¶
                daily_usage = await self._get_daily_usage(cookie_id)
                if daily_usage >= cookie_info.get("daily_usage_limit", 100):
                    continue
                
                # è®¡ç®—ä¼˜å…ˆçº§åˆ†æ•°ï¼ˆåŸºäºæˆåŠŸç‡å’Œä½¿ç”¨é¢‘ç‡ï¼‰
                success_rate = cookie_info.get("success_count", 0) / max(cookie_info.get("usage_count", 1), 1)
                priority_score = success_rate - (current_usage * 0.1)
                
                available_cookies.append((cookie_id, cookie_info, priority_score))
            
            if not available_cookies:
                logger.warning("âš ï¸ æ²¡æœ‰å¯ç”¨çš„Cookie")
                return None
            
            # æŒ‰ä¼˜å…ˆçº§æ’åºï¼Œé€‰æ‹©æœ€ä½³Cookie
            available_cookies.sort(key=lambda x: x[2], reverse=True)
            best_cookie_id, best_cookie_info, _ = available_cookies[0]
            
            # æ ‡è®°ä¸ºä½¿ç”¨ä¸­
            await self._mark_cookie_in_use(best_cookie_id)
            
            logger.info(f"ğŸª åˆ†é…Cookie {best_cookie_id[:8]}")
            return {
                "cookie_id": best_cookie_id,
                "cookie_string": best_cookie_info["cookie_string"],
                "account_info": best_cookie_info.get("account_info", {})
            }
        
        except Exception as e:
            logger.error(f"âŒ è·å–å¯ç”¨Cookieå¤±è´¥: {e}")
            return None
    
    async def release_cookie(self, cookie_id: str, success: bool = True):
        """é‡Šæ”¾Cookieä½¿ç”¨"""
        try:
            # æ›´æ–°ä½¿ç”¨ç»Ÿè®¡
            cookie_data = await self.redis_client.hget(self.pool_key, cookie_id)
            if cookie_data:
                cookie_info = json.loads(cookie_data)
                cookie_info["usage_count"] = cookie_info.get("usage_count", 0) + 1
                cookie_info["last_used"] = datetime.now().isoformat()
                
                if success:
                    cookie_info["success_count"] = cookie_info.get("success_count", 0) + 1
                else:
                    cookie_info["failure_count"] = cookie_info.get("failure_count", 0) + 1
                    
                    # å¦‚æœå¤±è´¥ç‡è¿‡é«˜ï¼Œæ ‡è®°ä¸ºä¸æ´»è·ƒ
                    total_usage = cookie_info["usage_count"]
                    failure_rate = cookie_info["failure_count"] / total_usage
                    if total_usage >= 10 and failure_rate >= 0.5:
                        cookie_info["is_active"] = False
                        logger.warning(f"âš ï¸ Cookie {cookie_id[:8]} å› å¤±è´¥ç‡è¿‡é«˜è¢«ç¦ç”¨")
                
                await self.redis_client.hset(self.pool_key, cookie_id, json.dumps(cookie_info))
            
            # ç§»é™¤ä½¿ç”¨æ ‡è®°
            await self._unmark_cookie_in_use(cookie_id)
            
        except Exception as e:
            logger.error(f"âŒ é‡Šæ”¾Cookieå¤±è´¥: {e}")
    
    async def _get_current_usage(self, cookie_id: str) -> int:
        """è·å–Cookieå½“å‰ä½¿ç”¨æ•°é‡"""
        try:
            usage_count = await self.redis_client.scard(f"{self.usage_key}:{cookie_id}")
            return usage_count
        except Exception:
            return 0
    
    async def _get_daily_usage(self, cookie_id: str) -> int:
        """è·å–Cookieæ¯æ—¥ä½¿ç”¨æ¬¡æ•°"""
        try:
            today = datetime.now().strftime('%Y-%m-%d')
            daily_key = f"daily_usage:{cookie_id}:{today}"
            usage = await self.redis_client.get(daily_key)
            return int(usage) if usage else 0
        except Exception:
            return 0
    
    async def _mark_cookie_in_use(self, cookie_id: str):
        """æ ‡è®°Cookieæ­£åœ¨ä½¿ç”¨"""
        try:
            usage_id = str(uuid.uuid4())
            await self.redis_client.sadd(f"{self.usage_key}:{cookie_id}", usage_id)
            await self.redis_client.expire(f"{self.usage_key}:{cookie_id}", 300)  # 5åˆ†é’Ÿè¿‡æœŸ
            
            # æ›´æ–°æ¯æ—¥ä½¿ç”¨ç»Ÿè®¡
            today = datetime.now().strftime('%Y-%m-%d')
            daily_key = f"daily_usage:{cookie_id}:{today}"
            await self.redis_client.incr(daily_key)
            await self.redis_client.expire(daily_key, 86400)  # 24å°æ—¶è¿‡æœŸ
            
            return usage_id
        except Exception as e:
            logger.error(f"æ ‡è®°Cookieä½¿ç”¨å¤±è´¥: {e}")
            return None
    
    async def _unmark_cookie_in_use(self, cookie_id: str, usage_id: str = None):
        """å–æ¶ˆCookieä½¿ç”¨æ ‡è®°"""
        try:
            if usage_id:
                await self.redis_client.srem(f"{self.usage_key}:{cookie_id}", usage_id)
            else:
                # æ¸…ç©ºæ‰€æœ‰ä½¿ç”¨æ ‡è®°
                await self.redis_client.delete(f"{self.usage_key}:{cookie_id}")
        except Exception as e:
            logger.error(f"å–æ¶ˆCookieä½¿ç”¨æ ‡è®°å¤±è´¥: {e}")

class TaskDistributor:
    """ä»»åŠ¡åˆ†å‘å™¨"""
    
    def __init__(self, redis_client):
        self.redis_client = redis_client
        self.task_queue_key = "download_tasks"
        self.processing_queue_key = "processing_tasks"
        self.completed_queue_key = "completed_tasks"
        self.instances_key = "instances"
        
    async def submit_task(self, task: DownloadTask) -> str:
        """æäº¤ä»»åŠ¡åˆ°é˜Ÿåˆ—"""
        try:
            task_data = asdict(task)
            # åºåˆ—åŒ–datetimeå¯¹è±¡
            for key, value in task_data.items():
                if isinstance(value, datetime):
                    task_data[key] = value.isoformat()
            
            # æ ¹æ®ä¼˜å…ˆçº§æ·»åŠ åˆ°ä¸åŒé˜Ÿåˆ—
            if task.priority > 5:
                queue_key = f"{self.task_queue_key}:high"
            elif task.priority > 2:
                queue_key = f"{self.task_queue_key}:normal"
            else:
                queue_key = f"{self.task_queue_key}:low"
            
            await self.redis_client.lpush(queue_key, json.dumps(task_data))
            
            logger.info(f"ğŸ“‹ ä»»åŠ¡ {task.task_id} å·²æäº¤åˆ°é˜Ÿåˆ—")
            return task.task_id
        
        except Exception as e:
            logger.error(f"âŒ ä»»åŠ¡æäº¤å¤±è´¥: {e}")
            return None
    
    async def get_next_task(self, instance_id: str) -> Optional[DownloadTask]:
        """ä¸ºå®ä¾‹è·å–ä¸‹ä¸€ä¸ªä»»åŠ¡"""
        try:
            # æŒ‰ä¼˜å…ˆçº§ä»é˜Ÿåˆ—ä¸­è·å–ä»»åŠ¡
            queues = [
                f"{self.task_queue_key}:high",
                f"{self.task_queue_key}:normal",
                f"{self.task_queue_key}:low"
            ]
            
            for queue_key in queues:
                task_data = await self.redis_client.brpop(queue_key, timeout=1)
                if task_data:
                    _, task_json = task_data
                    task_dict = json.loads(task_json)
                    
                    # é‡å»ºdatetimeå¯¹è±¡
                    for key, value in task_dict.items():
                        if key.endswith('_at') and value:
                            task_dict[key] = datetime.fromisoformat(value)
                    
                    task = DownloadTask(**task_dict)
                    task.status = TaskStatus.ASSIGNED
                    task.assigned_instance = instance_id
                    task.assigned_at = datetime.now()
                    
                    # ç§»åˆ°å¤„ç†é˜Ÿåˆ—
                    await self._move_to_processing(task)
                    
                    logger.info(f"ğŸ“¤ ä»»åŠ¡ {task.task_id} åˆ†é…ç»™å®ä¾‹ {instance_id}")
                    return task
            
            return None
        
        except Exception as e:
            logger.error(f"âŒ è·å–ä»»åŠ¡å¤±è´¥: {e}")
            return None
    
    async def _move_to_processing(self, task: DownloadTask):
        """å°†ä»»åŠ¡ç§»åˆ°å¤„ç†é˜Ÿåˆ—"""
        try:
            task_data = asdict(task)
            # åºåˆ—åŒ–datetimeå¯¹è±¡
            for key, value in task_data.items():
                if isinstance(value, datetime):
                    task_data[key] = value.isoformat()
            
            await self.redis_client.hset(
                self.processing_queue_key, 
                task.task_id, 
                json.dumps(task_data)
            )
        except Exception as e:
            logger.error(f"ç§»åŠ¨ä»»åŠ¡åˆ°å¤„ç†é˜Ÿåˆ—å¤±è´¥: {e}")
    
    async def complete_task(self, task: DownloadTask, result: Dict[str, Any]):
        """å®Œæˆä»»åŠ¡"""
        try:
            task.status = TaskStatus.COMPLETED if result.get("success") else TaskStatus.FAILED
            task.completed_at = datetime.now()
            task.result_path = result.get("file_path")
            task.error_message = result.get("error_message")
            
            # ç§»åˆ°å®Œæˆé˜Ÿåˆ—
            task_data = asdict(task)
            for key, value in task_data.items():
                if isinstance(value, datetime):
                    task_data[key] = value.isoformat()
            
            await self.redis_client.hset(
                self.completed_queue_key,
                task.task_id,
                json.dumps(task_data)
            )
            
            # ä»å¤„ç†é˜Ÿåˆ—ä¸­ç§»é™¤
            await self.redis_client.hdel(self.processing_queue_key, task.task_id)
            
            logger.info(f"âœ… ä»»åŠ¡ {task.task_id} å·²å®Œæˆ: {task.status.value}")
            
        except Exception as e:
            logger.error(f"âŒ å®Œæˆä»»åŠ¡å¤±è´¥: {e}")
    
    async def retry_task(self, task: DownloadTask):
        """é‡è¯•ä»»åŠ¡"""
        try:
            if task.retry_count < task.max_retries:
                task.retry_count += 1
                task.status = TaskStatus.RETRY
                task.assigned_instance = None
                task.assigned_at = None
                
                # é‡æ–°æäº¤åˆ°é˜Ÿåˆ—
                await self.submit_task(task)
                
                # ä»å¤„ç†é˜Ÿåˆ—ç§»é™¤
                await self.redis_client.hdel(self.processing_queue_key, task.task_id)
                
                logger.info(f"ğŸ”„ ä»»åŠ¡ {task.task_id} é‡è¯• {task.retry_count}/{task.max_retries}")
            else:
                # è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œæ ‡è®°ä¸ºå¤±è´¥
                await self.complete_task(task, {"success": False, "error_message": "Max retries exceeded"})
                logger.error(f"âŒ ä»»åŠ¡ {task.task_id} è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°")
                
        except Exception as e:
            logger.error(f"âŒ ä»»åŠ¡é‡è¯•å¤±è´¥: {e}")

class LoadBalancer:
    """è´Ÿè½½å‡è¡¡å™¨"""
    
    def __init__(self, redis_client):
        self.redis_client = redis_client
        self.instances_key = "instances"
        
    async def register_instance(self, instance: InstanceInfo):
        """æ³¨å†Œå®ä¾‹"""
        try:
            instance_data = instance.to_dict()
            instance_data["last_heartbeat"] = instance.last_heartbeat.isoformat()
            
            await self.redis_client.hset(
                self.instances_key,
                instance.instance_id,
                json.dumps(instance_data)
            )
            
            logger.info(f"ğŸ“¡ å®ä¾‹ {instance.instance_id} å·²æ³¨å†Œ")
            
        except Exception as e:
            logger.error(f"âŒ å®ä¾‹æ³¨å†Œå¤±è´¥: {e}")
    
    async def update_instance_status(self, instance_id: str, status_update: Dict[str, Any]):
        """æ›´æ–°å®ä¾‹çŠ¶æ€"""
        try:
            instance_data = await self.redis_client.hget(self.instances_key, instance_id)
            if instance_data:
                instance_info = json.loads(instance_data)
                instance_info.update(status_update)
                instance_info["last_heartbeat"] = datetime.now().isoformat()
                
                await self.redis_client.hset(
                    self.instances_key,
                    instance_id,
                    json.dumps(instance_info)
                )
                
        except Exception as e:
            logger.error(f"âŒ æ›´æ–°å®ä¾‹çŠ¶æ€å¤±è´¥: {e}")
    
    async def get_best_instance(self) -> Optional[str]:
        """è·å–æœ€ä½³å®ä¾‹ID"""
        try:
            all_instances = await self.redis_client.hgetall(self.instances_key)
            
            if not all_instances:
                return None
            
            healthy_instances = []
            
            for instance_id, instance_data in all_instances.items():
                if isinstance(instance_id, bytes):
                    instance_id = instance_id.decode()
                
                instance_info = json.loads(instance_data)
                
                # æ£€æŸ¥å®ä¾‹å¥åº·çŠ¶æ€
                last_heartbeat = datetime.fromisoformat(instance_info["last_heartbeat"])
                if datetime.now() - last_heartbeat > timedelta(minutes=2):
                    continue  # å®ä¾‹å¯èƒ½ç¦»çº¿
                
                if instance_info["status"] not in ["healthy", "busy"]:
                    continue  # å®ä¾‹ä¸å¥åº·
                
                if instance_info["current_tasks"] >= instance_info["max_concurrent_tasks"]:
                    continue  # å®ä¾‹å·²æ»¡è½½
                
                # è®¡ç®—å®ä¾‹è´Ÿè½½åˆ†æ•°ï¼ˆè¶Šä½è¶Šå¥½ï¼‰
                load_score = (
                    (instance_info["current_tasks"] / instance_info["max_concurrent_tasks"]) * 0.4 +
                    (1 - instance_info["health_score"]) * 0.3 +
                    (instance_info["avg_response_time"] / 100.0) * 0.2 +
                    (1 - instance_info["success_rate"]) * 0.1
                )
                
                healthy_instances.append((instance_id, load_score))
            
            if not healthy_instances:
                return None
            
            # é€‰æ‹©è´Ÿè½½åˆ†æ•°æœ€ä½çš„å®ä¾‹
            healthy_instances.sort(key=lambda x: x[1])
            return healthy_instances[0][0]
            
        except Exception as e:
            logger.error(f"âŒ è·å–æœ€ä½³å®ä¾‹å¤±è´¥: {e}")
            return None
    
    async def get_all_instances(self) -> List[InstanceInfo]:
        """è·å–æ‰€æœ‰å®ä¾‹ä¿¡æ¯"""
        try:
            all_instances = await self.redis_client.hgetall(self.instances_key)
            instances = []
            
            for instance_id, instance_data in all_instances.items():
                if isinstance(instance_id, bytes):
                    instance_id = instance_id.decode()
                
                instance_info = json.loads(instance_data)
                instance_info["last_heartbeat"] = datetime.fromisoformat(instance_info["last_heartbeat"])
                instance_info["status"] = InstanceStatus(instance_info["status"])
                
                instances.append(InstanceInfo(**instance_info))
            
            return instances
            
        except Exception as e:
            logger.error(f"âŒ è·å–å®ä¾‹åˆ—è¡¨å¤±è´¥: {e}")
            return []

class HorizontalScalingManager:
    """æ°´å¹³æ‰©å±•ç®¡ç†å™¨"""
    
    def __init__(self, redis_url: str = "redis://localhost:6379"):
        """åˆå§‹åŒ–æ°´å¹³æ‰©å±•ç®¡ç†å™¨"""
        self.redis_client = redis.from_url(redis_url, decode_responses=True)
        self.cookie_pool = CookiePool(self.redis_client)
        self.task_distributor = TaskDistributor(self.redis_client)
        self.load_balancer = LoadBalancer(self.redis_client)
        
        self.scaling_config = {
            "min_instances": 1,
            "max_instances": 10,
            "target_cpu_utilization": 70,
            "scale_up_threshold": 80,
            "scale_down_threshold": 30,
            "cooldown_period": 300  # 5åˆ†é’Ÿå†·å´æœŸ
        }
        
        self.last_scaling_action = None
        
    async def submit_download_request(self, doc_url: str, format_type: str = "csv", 
                                    priority: int = 0) -> str:
        """æäº¤ä¸‹è½½è¯·æ±‚"""
        try:
            # ç”Ÿæˆä»»åŠ¡ID
            task_id = str(uuid.uuid4())
            
            # ä»URLæå–æ–‡æ¡£ID
            doc_id = doc_url.split('/')[-1] if '/' in doc_url else doc_url
            
            # åˆ›å»ºä»»åŠ¡
            task = DownloadTask(
                task_id=task_id,
                doc_url=doc_url,
                doc_id=doc_id,
                format_type=format_type,
                priority=priority
            )
            
            # æäº¤åˆ°ä»»åŠ¡é˜Ÿåˆ—
            submitted_id = await self.task_distributor.submit_task(task)
            
            if submitted_id:
                logger.info(f"ğŸ“¥ ä¸‹è½½è¯·æ±‚å·²æäº¤: {task_id}")
                return task_id
            else:
                logger.error(f"âŒ ä¸‹è½½è¯·æ±‚æäº¤å¤±è´¥")
                return None
        
        except Exception as e:
            logger.error(f"âŒ æäº¤ä¸‹è½½è¯·æ±‚å¤±è´¥: {e}")
            return None
    
    async def get_task_status(self, task_id: str) -> Dict[str, Any]:
        """è·å–ä»»åŠ¡çŠ¶æ€"""
        try:
            # æ£€æŸ¥å¤„ç†é˜Ÿåˆ—
            processing_task = await self.redis_client.hget(
                self.task_distributor.processing_queue_key, 
                task_id
            )
            if processing_task:
                task_data = json.loads(processing_task)
                return {
                    "status": "processing",
                    "details": task_data
                }
            
            # æ£€æŸ¥å®Œæˆé˜Ÿåˆ—
            completed_task = await self.redis_client.hget(
                self.task_distributor.completed_queue_key,
                task_id
            )
            if completed_task:
                task_data = json.loads(completed_task)
                return {
                    "status": "completed",
                    "details": task_data
                }
            
            # æ£€æŸ¥å¾…å¤„ç†é˜Ÿåˆ—
            queues = [
                f"{self.task_distributor.task_queue_key}:high",
                f"{self.task_distributor.task_queue_key}:normal", 
                f"{self.task_distributor.task_queue_key}:low"
            ]
            
            for queue_key in queues:
                queue_length = await self.redis_client.llen(queue_key)
                if queue_length > 0:
                    # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…å¯ä»¥éå†é˜Ÿåˆ—æŸ¥æ‰¾ç‰¹å®šä»»åŠ¡
                    return {
                        "status": "pending",
                        "queue_position": "unknown",
                        "estimated_wait_time": queue_length * 45  # å‡è®¾æ¯ä»»åŠ¡45ç§’
                    }
            
            return {
                "status": "not_found",
                "message": "Task not found in any queue"
            }
        
        except Exception as e:
            logger.error(f"âŒ è·å–ä»»åŠ¡çŠ¶æ€å¤±è´¥: {e}")
            return {"status": "error", "message": str(e)}
    
    async def start_coordinator(self):
        """å¯åŠ¨åè°ƒå™¨"""
        try:
            logger.info("ğŸš€ å¯åŠ¨æ°´å¹³æ‰©å±•åè°ƒå™¨...")
            
            # åˆ›å»ºåè°ƒä»»åŠ¡
            tasks = [
                asyncio.create_task(self._monitor_system_load()),
                asyncio.create_task(self._manage_scaling()),
                asyncio.create_task(self._cleanup_expired_tasks()),
                asyncio.create_task(self._health_check_instances())
            ]
            
            await asyncio.gather(*tasks)
            
        except Exception as e:
            logger.error(f"âŒ åè°ƒå™¨å¯åŠ¨å¤±è´¥: {e}")
    
    async def _monitor_system_load(self):
        """ç›‘æ§ç³»ç»Ÿè´Ÿè½½"""
        while True:
            try:
                # è·å–æ‰€æœ‰å®ä¾‹ä¿¡æ¯
                instances = await self.load_balancer.get_all_instances()
                
                if instances:
                    # è®¡ç®—å¹³å‡è´Ÿè½½
                    avg_cpu = sum(inst.health_score for inst in instances) / len(instances)
                    avg_tasks = sum(inst.current_tasks for inst in instances) / len(instances)
                    
                    # è®¡ç®—é˜Ÿåˆ—é•¿åº¦
                    total_queue_length = 0
                    queues = [
                        f"{self.task_distributor.task_queue_key}:high",
                        f"{self.task_distributor.task_queue_key}:normal",
                        f"{self.task_distributor.task_queue_key}:low"
                    ]
                    
                    for queue in queues:
                        length = await self.redis_client.llen(queue)
                        total_queue_length += length
                    
                    # å­˜å‚¨ç›‘æ§æ•°æ®
                    monitoring_data = {
                        "timestamp": datetime.now().isoformat(),
                        "active_instances": len(instances),
                        "avg_cpu_utilization": avg_cpu * 100,
                        "avg_concurrent_tasks": avg_tasks,
                        "total_queue_length": total_queue_length,
                        "healthy_instances": len([i for i in instances if i.status == InstanceStatus.HEALTHY])
                    }
                    
                    await self.redis_client.lpush("system_monitoring", json.dumps(monitoring_data))
                    await self.redis_client.ltrim("system_monitoring", 0, 999)  # ä¿ç•™æœ€æ–°1000æ¡è®°å½•
                    
                    logger.debug(f"ğŸ“Š ç³»ç»Ÿè´Ÿè½½: CPU {avg_cpu*100:.1f}%, é˜Ÿåˆ— {total_queue_length}, å®ä¾‹ {len(instances)}")
                
                await asyncio.sleep(30)  # 30ç§’ç›‘æ§ä¸€æ¬¡
                
            except Exception as e:
                logger.error(f"âŒ ç³»ç»Ÿè´Ÿè½½ç›‘æ§å¤±è´¥: {e}")
                await asyncio.sleep(30)
    
    async def _manage_scaling(self):
        """ç®¡ç†è‡ªåŠ¨æ‰©ç¼©å®¹"""
        while True:
            try:
                # æ£€æŸ¥æ˜¯å¦åœ¨å†·å´æœŸ
                if (self.last_scaling_action and 
                    datetime.now() - self.last_scaling_action < timedelta(seconds=self.scaling_config["cooldown_period"])):
                    await asyncio.sleep(60)
                    continue
                
                instances = await self.load_balancer.get_all_instances()
                
                if not instances:
                    await asyncio.sleep(60)
                    continue
                
                # è®¡ç®—æ‰©ç¼©å®¹æŒ‡æ ‡
                avg_cpu = sum(inst.health_score for inst in instances) / len(instances) * 100
                active_instances = len([i for i in instances if i.status in [InstanceStatus.HEALTHY, InstanceStatus.BUSY]])
                
                # è®¡ç®—æ€»é˜Ÿåˆ—é•¿åº¦
                total_queue = 0
                for queue_suffix in ["high", "normal", "low"]:
                    queue_key = f"{self.task_distributor.task_queue_key}:{queue_suffix}"
                    total_queue += await self.redis_client.llen(queue_key)
                
                # æ‰©å®¹å†³ç­–
                should_scale_up = (
                    avg_cpu > self.scaling_config["scale_up_threshold"] or
                    total_queue > active_instances * 3 or
                    active_instances < self.scaling_config["min_instances"]
                )
                
                # ç¼©å®¹å†³ç­–
                should_scale_down = (
                    avg_cpu < self.scaling_config["scale_down_threshold"] and
                    total_queue == 0 and
                    active_instances > self.scaling_config["min_instances"]
                )
                
                if should_scale_up and active_instances < self.scaling_config["max_instances"]:
                    await self._trigger_scale_up()
                elif should_scale_down:
                    await self._trigger_scale_down()
                
                await asyncio.sleep(60)  # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
                
            except Exception as e:
                logger.error(f"âŒ è‡ªåŠ¨æ‰©ç¼©å®¹ç®¡ç†å¤±è´¥: {e}")
                await asyncio.sleep(60)
    
    async def _trigger_scale_up(self):
        """è§¦å‘æ‰©å®¹"""
        try:
            logger.info("ğŸ“ˆ è§¦å‘æ‰©å®¹æ“ä½œ")
            
            # è¿™é‡Œåº”è¯¥è°ƒç”¨äº‘æœåŠ¡APIåˆ›å»ºæ–°å®ä¾‹
            # æš‚æ—¶è®°å½•æ‰©å®¹äº‹ä»¶
            scale_event = {
                "action": "scale_up",
                "timestamp": datetime.now().isoformat(),
                "reason": "High load detected"
            }
            
            await self.redis_client.lpush("scaling_events", json.dumps(scale_event))
            self.last_scaling_action = datetime.now()
            
            # å®é™…ç¯å¢ƒä¸­ï¼Œè¿™é‡Œåº”è¯¥ï¼š
            # 1. è°ƒç”¨äº‘æœåŠ¡APIåˆ›å»ºæ–°çš„Dockerå®¹å™¨/è™šæ‹Ÿæœº
            # 2. é…ç½®æ–°å®ä¾‹çš„ç¯å¢ƒå˜é‡å’Œé…ç½®
            # 3. å¯åŠ¨æ–°å®ä¾‹çš„æœåŠ¡è¿›ç¨‹
            
        except Exception as e:
            logger.error(f"âŒ æ‰©å®¹æ“ä½œå¤±è´¥: {e}")
    
    async def _trigger_scale_down(self):
        """è§¦å‘ç¼©å®¹"""
        try:
            logger.info("ğŸ“‰ è§¦å‘ç¼©å®¹æ“ä½œ")
            
            # é€‰æ‹©æœ€ç©ºé—²çš„å®ä¾‹è¿›è¡Œä¸‹çº¿
            instances = await self.load_balancer.get_all_instances()
            idle_instances = [
                inst for inst in instances 
                if inst.current_tasks == 0 and inst.status == InstanceStatus.HEALTHY
            ]
            
            if idle_instances:
                # é€‰æ‹©å¤„ç†ä»»åŠ¡æœ€å°‘çš„å®ä¾‹
                target_instance = min(idle_instances, key=lambda x: x.total_processed)
                
                # æ ‡è®°å®ä¾‹ä¸ºä¸‹çº¿çŠ¶æ€
                await self.load_balancer.update_instance_status(
                    target_instance.instance_id,
                    {"status": "offline"}
                )
                
                scale_event = {
                    "action": "scale_down",
                    "timestamp": datetime.now().isoformat(),
                    "target_instance": target_instance.instance_id,
                    "reason": "Low load detected"
                }
                
                await self.redis_client.lpush("scaling_events", json.dumps(scale_event))
                self.last_scaling_action = datetime.now()
                
                logger.info(f"ğŸ“‰ å®ä¾‹ {target_instance.instance_id} å·²æ ‡è®°ä¸‹çº¿")
        
        except Exception as e:
            logger.error(f"âŒ ç¼©å®¹æ“ä½œå¤±è´¥: {e}")
    
    async def _cleanup_expired_tasks(self):
        """æ¸…ç†è¿‡æœŸä»»åŠ¡"""
        while True:
            try:
                # æ¸…ç†è¶…æ—¶çš„å¤„ç†ä¸­ä»»åŠ¡
                processing_tasks = await self.redis_client.hgetall(
                    self.task_distributor.processing_queue_key
                )
                
                current_time = datetime.now()
                
                for task_id, task_data in processing_tasks.items():
                    task_info = json.loads(task_data)
                    assigned_at = datetime.fromisoformat(task_info["assigned_at"])
                    
                    # å¦‚æœä»»åŠ¡è¶…è¿‡10åˆ†é’Ÿè¿˜æœªå®Œæˆï¼Œè®¤ä¸ºè¶…æ—¶
                    if current_time - assigned_at > timedelta(minutes=10):
                        logger.warning(f"â° ä»»åŠ¡ {task_id} è¶…æ—¶ï¼Œé‡æ–°è°ƒåº¦")
                        
                        # é‡å»ºä»»åŠ¡å¯¹è±¡
                        task_dict = task_info.copy()
                        for key, value in task_dict.items():
                            if key.endswith('_at') and value:
                                task_dict[key] = datetime.fromisoformat(value)
                        
                        task = DownloadTask(**task_dict)
                        await self.task_distributor.retry_task(task)
                
                # æ¸…ç†è¿‡æœŸçš„Cookieä½¿ç”¨æ ‡è®°
                # æ¸…ç†è¿‡æœŸçš„å®ä¾‹å¿ƒè·³è®°å½•
                
                await asyncio.sleep(300)  # 5åˆ†é’Ÿæ¸…ç†ä¸€æ¬¡
                
            except Exception as e:
                logger.error(f"âŒ æ¸…ç†è¿‡æœŸä»»åŠ¡å¤±è´¥: {e}")
                await asyncio.sleep(300)
    
    async def _health_check_instances(self):
        """å¥åº·æ£€æŸ¥å®ä¾‹"""
        while True:
            try:
                instances = await self.load_balancer.get_all_instances()
                current_time = datetime.now()
                
                for instance in instances:
                    time_since_heartbeat = current_time - instance.last_heartbeat
                    
                    if time_since_heartbeat > timedelta(minutes=5):
                        # å®ä¾‹å¯èƒ½ç¦»çº¿
                        await self.load_balancer.update_instance_status(
                            instance.instance_id,
                            {"status": "offline"}
                        )
                        logger.warning(f"âš ï¸ å®ä¾‹ {instance.instance_id} å¯èƒ½ç¦»çº¿")
                    elif time_since_heartbeat > timedelta(minutes=2):
                        # å®ä¾‹å“åº”ç¼“æ…¢
                        await self.load_balancer.update_instance_status(
                            instance.instance_id,
                            {"status": "degraded"}
                        )
                
                await asyncio.sleep(60)  # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
                
            except Exception as e:
                logger.error(f"âŒ å®ä¾‹å¥åº·æ£€æŸ¥å¤±è´¥: {e}")
                await asyncio.sleep(60)
    
    def get_system_status(self) -> Dict[str, Any]:
        """è·å–ç³»ç»ŸçŠ¶æ€"""
        try:
            # è¿™é‡Œåº”è¯¥è¿”å›ç³»ç»Ÿçš„å®æ—¶çŠ¶æ€
            # ç®€åŒ–å®ç°ï¼Œè¿”å›åŸºæœ¬ä¿¡æ¯
            return {
                "status": "running",
                "coordinator_active": True,
                "scaling_config": self.scaling_config,
                "last_scaling_action": self.last_scaling_action.isoformat() if self.last_scaling_action else None
            }
        except Exception as e:
            logger.error(f"âŒ è·å–ç³»ç»ŸçŠ¶æ€å¤±è´¥: {e}")
            return {"status": "error", "message": str(e)}


# å·¥å…·å‡½æ•°
async def setup_test_environment():
    """è®¾ç½®æµ‹è¯•ç¯å¢ƒ"""
    try:
        manager = HorizontalScalingManager()
        
        # æ·»åŠ æµ‹è¯•Cookie
        test_cookies = [
            {
                "cookie_string": "test_cookie_1=value1; test_cookie_2=value2",
                "account_info": {"account_id": "test1", "username": "testuser1"},
                "daily_limit": 100,
                "concurrent_limit": 2
            },
            {
                "cookie_string": "test_cookie_3=value3; test_cookie_4=value4", 
                "account_info": {"account_id": "test2", "username": "testuser2"},
                "daily_limit": 150,
                "concurrent_limit": 3
            }
        ]
        
        for cookie_data in test_cookies:
            await manager.cookie_pool.add_cookie(cookie_data)
        
        # æ³¨å†Œæµ‹è¯•å®ä¾‹
        test_instance = InstanceInfo(
            instance_id="test-instance-1",
            host="localhost",
            port=8080,
            status=InstanceStatus.HEALTHY,
            current_tasks=0,
            max_concurrent_tasks=3,
            last_heartbeat=datetime.now(),
            health_score=0.85,
            total_processed=0,
            success_rate=0.95,
            avg_response_time=42.0
        )
        
        await manager.load_balancer.register_instance(test_instance)
        
        logger.info("âœ… æµ‹è¯•ç¯å¢ƒè®¾ç½®å®Œæˆ")
        return manager
        
    except Exception as e:
        logger.error(f"âŒ æµ‹è¯•ç¯å¢ƒè®¾ç½®å¤±è´¥: {e}")
        return None


async def main():
    """ä¸»å‡½æ•° - æ¼”ç¤ºæ°´å¹³æ‰©å±•ç³»ç»Ÿ"""
    try:
        print("=== è…¾è®¯æ–‡æ¡£æ°´å¹³æ‰©å±•æ¶æ„æ¼”ç¤º ===")
        
        # è®¾ç½®æµ‹è¯•ç¯å¢ƒ
        manager = await setup_test_environment()
        if not manager:
            print("âŒ æµ‹è¯•ç¯å¢ƒè®¾ç½®å¤±è´¥")
            return
        
        # æäº¤æµ‹è¯•ä»»åŠ¡
        test_urls = [
            "https://docs.qq.com/sheet/DWEVjZndkR2xVSWJN",
            "https://docs.qq.com/sheet/DRFppYm15RGZ2WExN", 
            "https://docs.qq.com/sheet/DRHZrS1hOS3pwRGZB"
        ]
        
        submitted_tasks = []
        for url in test_urls:
            task_id = await manager.submit_download_request(url, "csv", priority=1)
            if task_id:
                submitted_tasks.append(task_id)
                print(f"âœ… ä»»åŠ¡æäº¤æˆåŠŸ: {task_id}")
        
        # æŸ¥è¯¢ä»»åŠ¡çŠ¶æ€
        await asyncio.sleep(2)
        for task_id in submitted_tasks:
            status = await manager.get_task_status(task_id)
            print(f"ğŸ“‹ ä»»åŠ¡ {task_id[:8]} çŠ¶æ€: {status['status']}")
        
        # è·å–ç³»ç»ŸçŠ¶æ€
        system_status = manager.get_system_status()
        print(f"\nğŸ–¥ï¸ ç³»ç»ŸçŠ¶æ€: {system_status}")
        
        print("\nâœ… æ¼”ç¤ºå®Œæˆ")
        
    except Exception as e:
        print(f"âŒ æ¼”ç¤ºå¤±è´¥: {e}")


if __name__ == "__main__":
    # é…ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # è¿è¡Œæ¼”ç¤º
    asyncio.run(main())