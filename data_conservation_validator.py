#!/usr/bin/env python3
"""
æ•°æ®å®ˆæ’éªŒè¯å™¨
ç¡®ä¿æ•°æ®åœ¨å„å¤„ç†é˜¶æ®µä¸ä¼šä¸¢å¤±ã€é”™ä½æˆ–æ— æ³•ä¼ é€’
"""

import json
import logging
from typing import Dict, List, Any, Tuple
from datetime import datetime

class DataLossException(Exception):
    """æ•°æ®ä¸¢å¤±å¼‚å¸¸"""
    pass

class DataConservationValidator:
    """æ•°æ®å®ˆæ’éªŒè¯å™¨"""
    
    def __init__(self, strict_mode: bool = True):
        """
        åˆå§‹åŒ–éªŒè¯å™¨
        
        Args:
            strict_mode: ä¸¥æ ¼æ¨¡å¼ï¼Œä»»ä½•æ•°æ®ä¸¢å¤±éƒ½ä¼šæŠ›å‡ºå¼‚å¸¸
        """
        self.strict_mode = strict_mode
        self.audit_log = []
        self.validation_results = {
            "total_validations": 0,
            "failed_validations": 0,
            "data_loss_events": [],
            "stage_summary": {}
        }
        
    def validate_stage_consistency(self, 
                                 input_data: Any, 
                                 output_data: Any, 
                                 stage_name: str,
                                 expected_transformations: Dict[str, Any] = None) -> bool:
        """
        éªŒè¯å•ä¸ªå¤„ç†é˜¶æ®µçš„æ•°æ®ä¸€è‡´æ€§
        
        Args:
            input_data: è¾“å…¥æ•°æ®
            output_data: è¾“å‡ºæ•°æ®
            stage_name: é˜¶æ®µåç§°
            expected_transformations: é¢„æœŸçš„æ•°æ®è½¬æ¢è§„åˆ™
        
        Returns:
            bool: éªŒè¯æ˜¯å¦é€šè¿‡
        """
        
        self.validation_results["total_validations"] += 1
        
        # è®¡ç®—è¾“å…¥è¾“å‡ºæ•°æ®é‡
        input_count = self._calculate_data_count(input_data, stage_name)
        output_count = self._calculate_output_count(output_data, stage_name)
        
        # è®°å½•éªŒè¯ä¿¡æ¯
        validation_record = {
            "timestamp": datetime.now().isoformat(),
            "stage_name": stage_name,
            "input_count": input_count,
            "output_count": output_count,
            "data_preserved": True,
            "issues": []
        }
        
        # æ•°æ®é‡éªŒè¯
        data_loss_detected = False
        
        if stage_name == "csv_comparison":
            # CSVå¯¹æ¯”é˜¶æ®µï¼šè¾“å…¥è¾“å‡ºåº”è¯¥ç›¸ç­‰
            if input_count != output_count:
                data_loss_detected = True
                validation_record["issues"].append({
                    "type": "quantity_mismatch",
                    "description": f"CSVå¯¹æ¯”é˜¶æ®µæ•°æ®é‡ä¸åŒ¹é…: {input_count} â†’ {output_count}"
                })
        
        elif stage_name == "intelligent_mapping":
            # æ™ºèƒ½æ˜ å°„é˜¶æ®µï¼šéœ€è¦éªŒè¯æ˜ å°„è¦†ç›–ç‡
            mapping_info = self._analyze_mapping_coverage(input_data, output_data)
            validation_record["mapping_coverage"] = mapping_info["coverage_rate"]
            
            if mapping_info["unmapped_count"] > 0:
                if self.strict_mode and mapping_info["coverage_rate"] < 0.8:
                    data_loss_detected = True
                validation_record["issues"].append({
                    "type": "mapping_incomplete", 
                    "description": f"æ˜ å°„è¦†ç›–ç‡è¿‡ä½: {mapping_info['coverage_rate']:.1%}",
                    "unmapped_columns": mapping_info["unmapped_columns"],
                    "lost_data_count": mapping_info["lost_data_count"]
                })
        
        elif stage_name == "heatmap_generation":
            # çƒ­åŠ›å›¾ç”Ÿæˆé˜¶æ®µï¼šéªŒè¯çƒ­ç‚¹ç”Ÿæˆæ•ˆç‡
            hotspot_info = self._analyze_hotspot_generation(input_data, output_data)
            validation_record["hotspot_efficiency"] = hotspot_info["efficiency"]
            
            if hotspot_info["efficiency"] < 0.5:  # è‡³å°‘50%çš„è¾“å…¥æ•°æ®åº”äº§ç”Ÿå¯è§çƒ­ç‚¹
                validation_record["issues"].append({
                    "type": "hotspot_generation_low",
                    "description": f"çƒ­ç‚¹ç”Ÿæˆæ•ˆç‡è¿‡ä½: {hotspot_info['efficiency']:.1%}",
                    "expected_hotspots": hotspot_info["expected_hotspots"],
                    "actual_hotspots": hotspot_info["actual_hotspots"]
                })
        
        # æ ‡è®°éªŒè¯çŠ¶æ€
        validation_record["data_preserved"] = not data_loss_detected
        
        if data_loss_detected:
            self.validation_results["failed_validations"] += 1
            self.validation_results["data_loss_events"].append(validation_record)
            
            if self.strict_mode:
                raise DataLossException(
                    f"{stage_name}é˜¶æ®µæ£€æµ‹åˆ°æ•°æ®ä¸¢å¤±: {validation_record['issues']}"
                )
        
        # è®°å½•åˆ°å®¡è®¡æ—¥å¿—
        self.audit_log.append(validation_record)
        
        # æ›´æ–°é˜¶æ®µæ‘˜è¦
        if stage_name not in self.validation_results["stage_summary"]:
            self.validation_results["stage_summary"][stage_name] = {
                "validations": 0,
                "failures": 0,
                "avg_data_preservation": 0.0
            }
        
        stage_summary = self.validation_results["stage_summary"][stage_name]
        stage_summary["validations"] += 1
        if data_loss_detected:
            stage_summary["failures"] += 1
        
        return not data_loss_detected
    
    def _calculate_data_count(self, data: Any, stage_name: str) -> int:
        """è®¡ç®—æ•°æ®é‡"""
        
        if isinstance(data, list):
            return len(data)
        elif isinstance(data, dict):
            if stage_name == "csv_comparison" and "differences" in data:
                return len(data["differences"])
            elif stage_name == "intelligent_mapping" and "mappings" in data:
                return len(data["mappings"])
            else:
                return len(data)
        else:
            return 1 if data else 0
    
    def _calculate_output_count(self, output_data: Any, stage_name: str) -> int:
        """è®¡ç®—è¾“å‡ºæ•°æ®é‡"""
        
        if stage_name == "heatmap_generation":
            # å¯¹çƒ­åŠ›å›¾ï¼Œè®¡ç®—æ˜¾è‘—çƒ­ç‚¹æ•°é‡
            if isinstance(output_data, dict) and "heatmap_data" in output_data:
                matrix = output_data["heatmap_data"]
                significant_hotspots = 0
                for row in matrix:
                    for value in row:
                        if value > 0.1:  # æ˜¾è‘—çƒ­ç‚¹é˜ˆå€¼
                            significant_hotspots += 1
                return significant_hotspots
        
        return self._calculate_data_count(output_data, stage_name)
    
    def _analyze_mapping_coverage(self, input_data: Any, output_data: Any) -> Dict[str, Any]:
        """åˆ†ææ˜ å°„è¦†ç›–ç‡"""
        
        coverage_info = {
            "coverage_rate": 1.0,
            "unmapped_count": 0,
            "unmapped_columns": [],
            "lost_data_count": 0
        }
        
        if isinstance(output_data, dict) and "column_mapping" in output_data:
            mapping_result = output_data["column_mapping"]
            coverage_info["coverage_rate"] = mapping_result.get("coverage_rate", 1.0)
            coverage_info["unmapped_columns"] = mapping_result.get("unmapped_columns", [])
            coverage_info["unmapped_count"] = len(coverage_info["unmapped_columns"])
            
            # è®¡ç®—å› æ˜ å°„å¤±è´¥è€Œä¸¢å¤±çš„æ•°æ®æ•°é‡
            if isinstance(input_data, list):
                for diff in input_data:
                    if diff.get("åˆ—å") in coverage_info["unmapped_columns"]:
                        coverage_info["lost_data_count"] += 1
        
        return coverage_info
    
    def _analyze_hotspot_generation(self, input_data: Any, output_data: Any) -> Dict[str, Any]:
        """åˆ†æçƒ­ç‚¹ç”Ÿæˆæ•ˆç‡"""
        
        hotspot_info = {
            "efficiency": 1.0,
            "expected_hotspots": 0,
            "actual_hotspots": 0
        }
        
        # è®¡ç®—é¢„æœŸçƒ­ç‚¹æ•°ï¼ˆåŸºäºæ˜ å°„æˆåŠŸçš„å·®å¼‚æ•°é‡ï¼‰
        if isinstance(input_data, dict) and "differences" in input_data:
            # å‡è®¾è¿™æ˜¯ä»CSVå¯¹æ¯”ä¼ å…¥çš„æ•°æ®
            hotspot_info["expected_hotspots"] = len(input_data["differences"])
        elif isinstance(input_data, list):
            hotspot_info["expected_hotspots"] = len(input_data)
        
        # è®¡ç®—å®é™…ç”Ÿæˆçš„æ˜¾è‘—çƒ­ç‚¹æ•°é‡
        if isinstance(output_data, dict) and "heatmap_data" in output_data:
            matrix = output_data["heatmap_data"]
            for row in matrix:
                for value in row:
                    if value > 0.1:  # æ˜¾è‘—çƒ­ç‚¹é˜ˆå€¼
                        hotspot_info["actual_hotspots"] += 1
        
        # è®¡ç®—æ•ˆç‡
        if hotspot_info["expected_hotspots"] > 0:
            hotspot_info["efficiency"] = min(1.0, hotspot_info["actual_hotspots"] / hotspot_info["expected_hotspots"])
        
        return hotspot_info
    
    def validate_complete_pipeline(self, 
                                 csv_differences: List[Dict],
                                 mapping_result: Dict[str, Any],
                                 heatmap_result: Dict[str, Any]) -> Dict[str, Any]:
        """éªŒè¯å®Œæ•´çš„æ•°æ®å¤„ç†ç®¡é“"""
        
        print("ğŸ” å¼€å§‹å®Œæ•´æ•°æ®ç®¡é“éªŒè¯...")
        
        # é˜¶æ®µ1: CSVå¯¹æ¯”æ•°æ®éªŒè¯
        csv_valid = self.validate_stage_consistency(
            csv_differences, csv_differences, "csv_comparison"
        )
        
        # é˜¶æ®µ2: æ™ºèƒ½æ˜ å°„éªŒè¯
        mapping_valid = self.validate_stage_consistency(
            csv_differences, mapping_result, "intelligent_mapping"
        )
        
        # é˜¶æ®µ3: çƒ­åŠ›å›¾ç”ŸæˆéªŒè¯
        heatmap_valid = self.validate_stage_consistency(
            mapping_result, heatmap_result, "heatmap_generation"
        )
        
        # è®¡ç®—æ€»ä½“æ•°æ®å®Œæ•´æ€§
        overall_integrity = self._calculate_overall_integrity(
            csv_differences, mapping_result, heatmap_result
        )
        
        validation_summary = {
            "pipeline_valid": csv_valid and mapping_valid and heatmap_valid,
            "overall_data_integrity": overall_integrity,
            "stage_results": {
                "csv_comparison": csv_valid,
                "intelligent_mapping": mapping_valid, 
                "heatmap_generation": heatmap_valid
            },
            "detailed_report": self.validation_results,
            "audit_log": self.audit_log
        }
        
        print(f"   âœ… éªŒè¯å®Œæˆ - æ•°æ®å®Œæ•´æ€§: {overall_integrity:.1%}")
        
        return validation_summary
    
    def _calculate_overall_integrity(self, 
                                   csv_data: List[Dict],
                                   mapping_result: Dict[str, Any],
                                   heatmap_result: Dict[str, Any]) -> float:
        """è®¡ç®—æ€»ä½“æ•°æ®å®Œæ•´æ€§"""
        
        if not csv_data:
            return 1.0
        
        original_count = len(csv_data)
        
        # è®¡ç®—æœ€ç»ˆå¯è§çš„çƒ­ç‚¹æ•°é‡
        final_visible_count = 0
        if isinstance(heatmap_result, dict) and "heatmap_data" in heatmap_result:
            matrix = heatmap_result["heatmap_data"]
            for row in matrix:
                for value in row:
                    if value > 0.1:  # å¯è§çƒ­ç‚¹é˜ˆå€¼
                        final_visible_count += 1
        
        # æ•°æ®å®Œæ•´æ€§ = æœ€ç»ˆå¯è§æ•°æ® / åŸå§‹æ•°æ®
        return min(1.0, final_visible_count / original_count) if original_count > 0 else 1.0
    
    def generate_data_integrity_report(self, output_file: str = None) -> Dict[str, Any]:
        """ç”Ÿæˆæ•°æ®å®Œæ•´æ€§æŠ¥å‘Š"""
        
        report = {
            "timestamp": datetime.now().isoformat(),
            "validation_summary": self.validation_results,
            "audit_trail": self.audit_log,
            "recommendations": []
        }
        
        # ç”Ÿæˆä¿®å¤å»ºè®®
        if self.validation_results["failed_validations"] > 0:
            report["recommendations"].append({
                "priority": "HIGH",
                "action": "ä¿®å¤æ•°æ®æ˜ å°„è¦†ç›–ç‡",
                "description": "æ‰©å±•æ™ºèƒ½æ˜ å°„ç®—æ³•çš„è¯­ä¹‰è¯å…¸"
            })
        
        if len([e for e in self.validation_results["data_loss_events"] if "hotspot_generation_low" in str(e)]) > 0:
            report["recommendations"].append({
                "priority": "MEDIUM", 
                "action": "ä¼˜åŒ–çƒ­ç‚¹ç”Ÿæˆç®—æ³•",
                "description": "è°ƒæ•´çƒ­åŠ›å¼ºåº¦è®¡ç®—ï¼Œç¡®ä¿æ›´å¤šæ•°æ®å¯è§"
            })
        
        # ä¿å­˜æŠ¥å‘Š
        if output_file:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=2, ensure_ascii=False)
            print(f"ğŸ“„ æ•°æ®å®Œæ•´æ€§æŠ¥å‘Šå·²ä¿å­˜: {output_file}")
        
        return report

def test_data_conservation():
    """æµ‹è¯•æ•°æ®å®ˆæ’éªŒè¯"""
    
    # æ¨¡æ‹Ÿæµ‹è¯•æ•°æ®
    test_csv_data = [
        {"è¡Œå·": 2, "åˆ—å": "è´Ÿè´£äºº", "åŸå€¼": "æå››", "æ–°å€¼": "æå°æ˜"},
        {"è¡Œå·": 2, "åˆ—å": "å·¥èµ„", "åŸå€¼": "7500", "æ–°å€¼": "8500"},
        {"è¡Œå·": 3, "åˆ—å": "çŠ¶æ€", "åŸå€¼": "æ­£å¸¸", "æ–°å€¼": "ç¦»èŒ"}
    ]
    
    test_mapping_result = {
        "column_mapping": {
            "coverage_rate": 0.6,
            "unmapped_columns": ["å·¥èµ„"]
        }
    }
    
    test_heatmap_result = {
        "heatmap_data": [[0.05, 0.25], [0.05, 0.05]]  # åªæœ‰1ä¸ªæ˜¾è‘—çƒ­ç‚¹
    }
    
    # åˆ›å»ºéªŒè¯å™¨
    validator = DataConservationValidator(strict_mode=False)
    
    # æ‰§è¡ŒéªŒè¯
    result = validator.validate_complete_pipeline(
        test_csv_data, test_mapping_result, test_heatmap_result
    )
    
    # ç”ŸæˆæŠ¥å‘Š
    report = validator.generate_data_integrity_report(
        "/root/projects/tencent-doc-manager/data_conservation_report.json"
    )
    
    print("ğŸ¯ æ•°æ®å®ˆæ’éªŒè¯æµ‹è¯•å®Œæˆ")
    return result

if __name__ == "__main__":
    test_data_conservation()