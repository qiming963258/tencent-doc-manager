#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è…¾è®¯æ–‡æ¡£è‡ªåŠ¨åŒ–ç³»ç»Ÿ - å®¹é‡è§„åˆ’åˆ†æ
DevOps å®¹é‡è¯„ä¼°å’Œæ‰©å±•ç­–ç•¥
"""

import asyncio
import json
import logging
import time
import psutil
import os
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass
import statistics
import numpy as np

logger = logging.getLogger(__name__)

@dataclass
class PerformanceMetrics:
    """æ€§èƒ½æŒ‡æ ‡"""
    timestamp: datetime
    cpu_usage: float
    memory_usage: float
    disk_io: Dict[str, float]
    network_io: Dict[str, float]
    concurrent_downloads: int
    avg_download_time: float
    success_rate: float
    queue_length: int

@dataclass
class CapacityLimits:
    """å®¹é‡é™åˆ¶"""
    max_concurrent_downloads: int
    max_cpu_usage: float
    max_memory_usage: float
    max_disk_io_rate: float
    max_response_time: float
    min_success_rate: float

class CapacityPlanner:
    """å®¹é‡è§„åˆ’å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–å®¹é‡è§„åˆ’å™¨"""
        self.performance_history = []
        self.capacity_limits = CapacityLimits(
            max_concurrent_downloads=5,  # åŸºäºå½“å‰æµ‹è¯•ç»“æœ
            max_cpu_usage=0.8,  # 80% CPUä½¿ç”¨ç‡
            max_memory_usage=0.75,  # 75% å†…å­˜ä½¿ç”¨ç‡
            max_disk_io_rate=100.0,  # MB/s
            max_response_time=60.0,  # 60ç§’æœ€å¤§å“åº”æ—¶é—´
            min_success_rate=0.85  # 85% æœ€ä½æˆåŠŸç‡
        )
        
        # åŸºäº8æœˆ28æ—¥æµ‹è¯•æ•°æ®çš„åŸºçº¿
        self.baseline_performance = {
            "single_download_time": 42.0,  # å¹³å‡42ç§’
            "single_download_cpu": 0.15,   # 15% CPUä½¿ç”¨ç‡
            "single_download_memory": 0.05, # 5% å†…å­˜å¢é•¿
            "file_size_csv": 71859,        # CSVæ–‡ä»¶å¤§å°
            "file_size_excel": 46335,      # Excelæ–‡ä»¶å¤§å°
            "success_rate": 1.0             # 100% æˆåŠŸç‡
        }
    
    def analyze_single_instance_capacity(self) -> Dict[str, any]:
        """åˆ†æå•å®ä¾‹å¤„ç†èƒ½åŠ›"""
        try:
            logger.info("ğŸ“Š å¼€å§‹å•å®ä¾‹å®¹é‡åˆ†æ...")
            
            # è·å–ç³»ç»Ÿä¿¡æ¯
            system_info = self._get_system_info()
            
            # è®¡ç®—ç†è®ºæœ€å¤§å¹¶å‘æ•°
            theoretical_max = self._calculate_theoretical_max_concurrency()
            
            # è®¡ç®—å®é™…å®‰å…¨å¹¶å‘æ•°
            safe_max = self._calculate_safe_max_concurrency()
            
            # è®¡ç®—èµ„æºåˆ©ç”¨ç‡
            resource_utilization = self._calculate_resource_utilization()
            
            # è®¡ç®—ååé‡é¢„æµ‹
            throughput_analysis = self._analyze_throughput_capacity()
            
            capacity_analysis = {
                "system_info": system_info,
                "capacity_limits": {
                    "theoretical_max_concurrent": theoretical_max,
                    "safe_max_concurrent": safe_max,
                    "recommended_concurrent": min(safe_max, self.capacity_limits.max_concurrent_downloads)
                },
                "resource_analysis": resource_utilization,
                "throughput_analysis": throughput_analysis,
                "bottleneck_analysis": self._identify_bottlenecks(),
                "scaling_recommendations": self._generate_scaling_recommendations(),
                "performance_projections": self._project_performance_under_load()
            }
            
            logger.info("âœ… å•å®ä¾‹å®¹é‡åˆ†æå®Œæˆ")
            return capacity_analysis
        
        except Exception as e:
            logger.error(f"âŒ å•å®ä¾‹å®¹é‡åˆ†æå¤±è´¥: {e}")
            return {"error": str(e)}
    
    def _get_system_info(self) -> Dict[str, any]:
        """è·å–ç³»ç»Ÿä¿¡æ¯"""
        try:
            return {
                "cpu_count": psutil.cpu_count(),
                "cpu_freq": psutil.cpu_freq()._asdict() if psutil.cpu_freq() else {},
                "memory_total": psutil.virtual_memory().total,
                "memory_available": psutil.virtual_memory().available,
                "disk_total": psutil.disk_usage('/').total,
                "disk_free": psutil.disk_usage('/').free,
                "network_interfaces": list(psutil.net_io_counters(pernic=True).keys()),
                "platform": {
                    "system": os.uname().sysname,
                    "release": os.uname().release,
                    "machine": os.uname().machine
                }
            }
        except Exception as e:
            logger.error(f"è·å–ç³»ç»Ÿä¿¡æ¯å¤±è´¥: {e}")
            return {}
    
    def _calculate_theoretical_max_concurrency(self) -> int:
        """è®¡ç®—ç†è®ºæœ€å¤§å¹¶å‘æ•°"""
        try:
            # åŸºäºä¸åŒèµ„æºç»´åº¦è®¡ç®—æœ€å¤§å¹¶å‘æ•°
            
            # CPUç»´åº¦: å‡è®¾æ¯ä¸ªä¸‹è½½ä»»åŠ¡å ç”¨15% CPU
            cpu_cores = psutil.cpu_count()
            cpu_max_concurrent = int((cpu_cores * self.capacity_limits.max_cpu_usage) / 
                                   self.baseline_performance["single_download_cpu"])
            
            # å†…å­˜ç»´åº¦: å‡è®¾æ¯ä¸ªä¸‹è½½ä»»åŠ¡å ç”¨5%çš„å¯ç”¨å†…å­˜
            available_memory = psutil.virtual_memory().available
            total_memory = psutil.virtual_memory().total
            memory_max_concurrent = int((total_memory * self.capacity_limits.max_memory_usage) / 
                                      (total_memory * self.baseline_performance["single_download_memory"]))
            
            # ç½‘ç»œI/Oç»´åº¦: åŸºäºå¸¦å®½å’Œæ–‡ä»¶å¤§å°
            avg_file_size = (self.baseline_performance["file_size_csv"] + 
                           self.baseline_performance["file_size_excel"]) / 2
            # å‡è®¾100Mbpså¸¦å®½
            assumed_bandwidth_mbps = 100
            bandwidth_bytes_per_sec = (assumed_bandwidth_mbps * 1024 * 1024) / 8
            download_time = self.baseline_performance["single_download_time"]
            network_max_concurrent = int((bandwidth_bytes_per_sec * download_time) / avg_file_size)
            
            # å–æœ€å°å€¼ä½œä¸ºç†è®ºæœ€å¤§å€¼
            theoretical_max = min(cpu_max_concurrent, memory_max_concurrent, network_max_concurrent)
            
            logger.info(f"ç†è®ºæœ€å¤§å¹¶å‘è®¡ç®—: CPU={cpu_max_concurrent}, å†…å­˜={memory_max_concurrent}, ç½‘ç»œ={network_max_concurrent}")
            
            return max(1, theoretical_max)  # è‡³å°‘1ä¸ªå¹¶å‘
        
        except Exception as e:
            logger.error(f"ç†è®ºæœ€å¤§å¹¶å‘è®¡ç®—å¤±è´¥: {e}")
            return 1
    
    def _calculate_safe_max_concurrency(self) -> int:
        """è®¡ç®—å®‰å…¨æœ€å¤§å¹¶å‘æ•°"""
        try:
            # åœ¨ç†è®ºæœ€å¤§å€¼åŸºç¡€ä¸Šåº”ç”¨å®‰å…¨ç³»æ•°
            theoretical_max = self._calculate_theoretical_max_concurrency()
            
            # åº”ç”¨70%å®‰å…¨ç³»æ•°
            safety_factor = 0.7
            safe_max = int(theoretical_max * safety_factor)
            
            # è€ƒè™‘Cookieç®¡ç†å™¨çš„é™åˆ¶ï¼ˆåŒæ—¶åªèƒ½å¤„ç†æœ‰é™æ•°é‡çš„ä¼šè¯ï¼‰
            cookie_session_limit = 3  # åŸºäºå½“å‰Cookieç®¡ç†æœºåˆ¶
            
            # è€ƒè™‘è…¾è®¯æ–‡æ¡£çš„åçˆ¬è™«é™åˆ¶
            anti_scraping_limit = 2  # ä¿å®ˆä¼°è®¡ï¼Œé¿å…è§¦å‘åçˆ¬è™«
            
            # å–æœ€å°å€¼
            final_safe_max = min(safe_max, cookie_session_limit, anti_scraping_limit)
            
            return max(1, final_safe_max)
        
        except Exception as e:
            logger.error(f"å®‰å…¨æœ€å¤§å¹¶å‘è®¡ç®—å¤±è´¥: {e}")
            return 1
    
    def _calculate_resource_utilization(self) -> Dict[str, any]:
        """è®¡ç®—èµ„æºåˆ©ç”¨ç‡"""
        try:
            current_usage = {
                "cpu_current": psutil.cpu_percent(interval=1),
                "memory_current": psutil.virtual_memory().percent,
                "disk_current": psutil.disk_usage('/').percent
            }
            
            # é¢„æµ‹åœ¨ä¸åŒå¹¶å‘çº§åˆ«ä¸‹çš„èµ„æºä½¿ç”¨
            concurrency_levels = [1, 2, 3, 5, 8, 10]
            resource_projections = {}
            
            for level in concurrency_levels:
                projected_cpu = current_usage["cpu_current"] + (
                    level * self.baseline_performance["single_download_cpu"] * 100
                )
                projected_memory = current_usage["memory_current"] + (
                    level * self.baseline_performance["single_download_memory"] * 100
                )
                
                resource_projections[f"concurrent_{level}"] = {
                    "cpu_usage": min(100, projected_cpu),
                    "memory_usage": min(100, projected_memory),
                    "is_safe": (projected_cpu < self.capacity_limits.max_cpu_usage * 100 and
                              projected_memory < self.capacity_limits.max_memory_usage * 100)
                }
            
            return {
                "current_usage": current_usage,
                "projections": resource_projections,
                "safe_operating_range": {
                    "max_cpu_threshold": self.capacity_limits.max_cpu_usage * 100,
                    "max_memory_threshold": self.capacity_limits.max_memory_usage * 100
                }
            }
        
        except Exception as e:
            logger.error(f"èµ„æºåˆ©ç”¨ç‡è®¡ç®—å¤±è´¥: {e}")
            return {}
    
    def _analyze_throughput_capacity(self) -> Dict[str, any]:
        """åˆ†æååé‡å®¹é‡"""
        try:
            # åŸºäºå½“å‰æ€§èƒ½åŸºçº¿è®¡ç®—ä¸åŒåœºæ™¯ä¸‹çš„ååé‡
            
            single_download_time = self.baseline_performance["single_download_time"]
            success_rate = self.baseline_performance["success_rate"]
            
            throughput_scenarios = {}
            
            # ä¸²è¡Œå¤„ç†ï¼ˆ1ä¸ªå¹¶å‘ï¼‰
            throughput_scenarios["serial"] = {
                "concurrent_downloads": 1,
                "downloads_per_hour": int((3600 / single_download_time) * success_rate),
                "downloads_per_day": int((86400 / single_download_time) * success_rate),
                "avg_response_time": single_download_time
            }
            
            # å¹¶è¡Œå¤„ç†ï¼ˆä¸åŒå¹¶å‘çº§åˆ«ï¼‰
            for concurrent in [2, 3, 5]:
                # è€ƒè™‘å¹¶å‘å¼€é”€ï¼ˆæ¯å¢åŠ 1ä¸ªå¹¶å‘ï¼Œå“åº”æ—¶é—´å¢åŠ 5%ï¼‰
                overhead_factor = 1 + (concurrent - 1) * 0.05
                avg_response_time = single_download_time * overhead_factor
                
                # è€ƒè™‘æˆåŠŸç‡å¯èƒ½çš„ä¸‹é™
                concurrent_success_rate = success_rate * (0.98 ** (concurrent - 1))
                
                effective_throughput = (concurrent / avg_response_time) * concurrent_success_rate
                
                throughput_scenarios[f"concurrent_{concurrent}"] = {
                    "concurrent_downloads": concurrent,
                    "downloads_per_hour": int(effective_throughput * 3600),
                    "downloads_per_day": int(effective_throughput * 86400),
                    "avg_response_time": avg_response_time,
                    "success_rate": concurrent_success_rate,
                    "efficiency": effective_throughput / concurrent  # å•çº¿ç¨‹æ•ˆç‡
                }
            
            # æ¨èé…ç½®
            best_efficiency = max(
                throughput_scenarios.items(),
                key=lambda x: x[1].get("efficiency", 0) if x[0] != "serial" else 0
            )
            
            return {
                "scenarios": throughput_scenarios,
                "recommended_config": {
                    "scenario": best_efficiency[0],
                    "details": best_efficiency[1]
                },
                "peak_throughput": {
                    "max_downloads_per_hour": max(s["downloads_per_hour"] for s in throughput_scenarios.values()),
                    "max_downloads_per_day": max(s["downloads_per_day"] for s in throughput_scenarios.values())
                }
            }
        
        except Exception as e:
            logger.error(f"ååé‡åˆ†æå¤±è´¥: {e}")
            return {}
    
    def _identify_bottlenecks(self) -> List[Dict[str, any]]:
        """è¯†åˆ«ç³»ç»Ÿç“¶é¢ˆ"""
        try:
            bottlenecks = []
            
            # CPUç“¶é¢ˆåˆ†æ
            cpu_cores = psutil.cpu_count()
            if cpu_cores < 4:
                bottlenecks.append({
                    "type": "cpu",
                    "severity": "medium",
                    "description": f"CPUæ ¸å¿ƒæ•°è¾ƒå°‘ ({cpu_cores}æ ¸)ï¼Œå¯èƒ½å½±å“å¹¶å‘å¤„ç†èƒ½åŠ›",
                    "impact": "é™åˆ¶æœ€å¤§å¹¶å‘æ•°åˆ°2-3ä¸ª",
                    "recommendation": "è€ƒè™‘å‡çº§åˆ°è‡³å°‘4æ ¸CPU"
                })
            
            # å†…å­˜ç“¶é¢ˆåˆ†æ
            total_memory_gb = psutil.virtual_memory().total / (1024**3)
            if total_memory_gb < 8:
                bottlenecks.append({
                    "type": "memory",
                    "severity": "high",
                    "description": f"å†…å­˜å®¹é‡è¾ƒå° ({total_memory_gb:.1f}GB)",
                    "impact": "é™åˆ¶å¹¶å‘ä¸‹è½½å’Œæµè§ˆå™¨å®ä¾‹æ•°é‡",
                    "recommendation": "å»ºè®®å‡çº§åˆ°è‡³å°‘8GBå†…å­˜"
                })
            
            # ç½‘ç»œå¸¦å®½ç“¶é¢ˆï¼ˆåŸºäºæ–‡ä»¶å¤§å°å’Œä¸‹è½½æ—¶é—´æ¨æµ‹ï¼‰
            avg_file_size_mb = (self.baseline_performance["file_size_csv"] + 
                               self.baseline_performance["file_size_excel"]) / 2 / (1024**2)
            download_time = self.baseline_performance["single_download_time"]
            estimated_bandwidth_mbps = (avg_file_size_mb * 8) / download_time
            
            if estimated_bandwidth_mbps < 10:  # ä½äº10Mbps
                bottlenecks.append({
                    "type": "network",
                    "severity": "medium",
                    "description": f"ç½‘ç»œå¸¦å®½å¯èƒ½è¾ƒä½ (ä¼°è®¡ {estimated_bandwidth_mbps:.1f}Mbps)",
                    "impact": "å½±å“å¹¶å‘ä¸‹è½½æ€§èƒ½",
                    "recommendation": "æ£€æŸ¥ç½‘ç»œè¿æ¥è´¨é‡ï¼Œè€ƒè™‘å‡çº§å¸¦å®½"
                })
            
            # Cookieç®¡ç†ç“¶é¢ˆ
            bottlenecks.append({
                "type": "authentication",
                "severity": "high",
                "description": "Cookieç®¡ç†æœºåˆ¶é™åˆ¶äº†ä¼šè¯å¹¶å‘æ•°",
                "impact": "å•Cookieè´¦æˆ·é™åˆ¶å¹¶å‘ä¸‹è½½æ•°é‡",
                "recommendation": "å®æ–½å¤šCookieè½®è½¬æœºåˆ¶æˆ–Cookieæ± "
            })
            
            # åçˆ¬è™«é™åˆ¶
            bottlenecks.append({
                "type": "anti_scraping",
                "severity": "high",
                "description": "è…¾è®¯æ–‡æ¡£åçˆ¬è™«æœºåˆ¶é™åˆ¶è¯·æ±‚é¢‘ç‡",
                "impact": "è¿‡é«˜çš„å¹¶å‘å¯èƒ½å¯¼è‡´IPå°ç¦æˆ–è´¦æˆ·é™åˆ¶",
                "recommendation": "å®æ–½è¯·æ±‚é™æµå’Œå»¶è¿Ÿç­–ç•¥"
            })
            
            return bottlenecks
        
        except Exception as e:
            logger.error(f"ç“¶é¢ˆåˆ†æå¤±è´¥: {e}")
            return []
    
    def _generate_scaling_recommendations(self) -> Dict[str, any]:
        """ç”Ÿæˆæ‰©å±•å»ºè®®"""
        try:
            recommendations = {
                "horizontal_scaling": {
                    "description": "å¢åŠ æ›´å¤šå®ä¾‹æ¥æé«˜æ•´ä½“ååé‡",
                    "strategies": [
                        {
                            "strategy": "å¤šå®ä¾‹éƒ¨ç½²",
                            "implementation": "ä½¿ç”¨Dockerå®¹å™¨éƒ¨ç½²å¤šä¸ªå®ä¾‹",
                            "pros": ["çº¿æ€§æ‰©å±•èƒ½åŠ›", "æ•…éšœéš”ç¦»", "è´Ÿè½½åˆ†æ•£"],
                            "cons": ["éœ€è¦è´Ÿè½½å‡è¡¡å™¨", "Cookieç®¡ç†å¤æ‚åŒ–"],
                            "estimated_cost": "medium"
                        },
                        {
                            "strategy": "åˆ†å¸ƒå¼é˜Ÿåˆ—",
                            "implementation": "ä½¿ç”¨Redis/RabbitMQå®ç°ä»»åŠ¡åˆ†å‘",
                            "pros": ["ä»»åŠ¡å‡è¡¡åˆ†é…", "å¤±è´¥é‡è¯•æœºåˆ¶", "ç›‘æ§ä¾¿åˆ©"],
                            "cons": ["æ¶æ„å¤æ‚åŒ–", "æ–°å¢ä¾èµ–ç»„ä»¶"],
                            "estimated_cost": "high"
                        }
                    ]
                },
                "vertical_scaling": {
                    "description": "æå‡å•å®ä¾‹ç¡¬ä»¶é…ç½®",
                    "strategies": [
                        {
                            "strategy": "CPUå‡çº§",
                            "implementation": "å‡çº§åˆ°8æ ¸æˆ–æ›´å¤šCPU",
                            "expected_improvement": "å¹¶å‘å¤„ç†èƒ½åŠ›æå‡2-3å€",
                            "estimated_cost": "medium"
                        },
                        {
                            "strategy": "å†…å­˜å‡çº§",
                            "implementation": "å‡çº§åˆ°16GBæˆ–æ›´å¤šå†…å­˜",
                            "expected_improvement": "æ”¯æŒæ›´å¤šæµè§ˆå™¨å®ä¾‹",
                            "estimated_cost": "low"
                        },
                        {
                            "strategy": "SSDå­˜å‚¨",
                            "implementation": "ä½¿ç”¨é«˜é€ŸSSDå­˜å‚¨",
                            "expected_improvement": "I/Oæ€§èƒ½æå‡5-10å€",
                            "estimated_cost": "medium"
                        }
                    ]
                },
                "optimization_strategies": {
                    "description": "è½¯ä»¶å±‚é¢ä¼˜åŒ–ç­–ç•¥",
                    "strategies": [
                        {
                            "strategy": "æµè§ˆå™¨æ± åŒ–",
                            "implementation": "å¤ç”¨æµè§ˆå™¨å®ä¾‹ï¼Œå‡å°‘å¯åŠ¨å¼€é”€",
                            "expected_improvement": "å“åº”æ—¶é—´å‡å°‘30-50%",
                            "difficulty": "medium"
                        },
                        {
                            "strategy": "Cookieæ± ç®¡ç†",
                            "implementation": "ç»´æŠ¤å¤šä¸ªæœ‰æ•ˆCookieï¼Œè½®è½¬ä½¿ç”¨",
                            "expected_improvement": "å¹¶å‘èƒ½åŠ›æå‡3-5å€",
                            "difficulty": "high"
                        },
                        {
                            "strategy": "æ™ºèƒ½é‡è¯•æœºåˆ¶",
                            "implementation": "åŸºäºé”™è¯¯ç±»å‹çš„å·®å¼‚åŒ–é‡è¯•ç­–ç•¥",
                            "expected_improvement": "æˆåŠŸç‡æå‡5-10%",
                            "difficulty": "low"
                        },
                        {
                            "strategy": "ç¼“å­˜æœºåˆ¶",
                            "implementation": "ç¼“å­˜æœ€è¿‘ä¸‹è½½çš„æ–‡æ¡£ï¼Œé¿å…é‡å¤ä¸‹è½½",
                            "expected_improvement": "å‡å°‘50-80%é‡å¤è¯·æ±‚",
                            "difficulty": "medium"
                        }
                    ]
                }
            }
            
            # åŸºäºå½“å‰ç³»ç»ŸçŠ¶å†µçš„æ¨èä¼˜å…ˆçº§
            system_info = self._get_system_info()
            cpu_cores = system_info.get("cpu_count", 1)
            memory_gb = system_info.get("memory_total", 0) / (1024**3)
            
            priority_recommendations = []
            
            if memory_gb < 8:
                priority_recommendations.append("å†…å­˜å‡çº§ï¼ˆé«˜ä¼˜å…ˆçº§ï¼‰")
            if cpu_cores < 4:
                priority_recommendations.append("CPUå‡çº§ï¼ˆä¸­ä¼˜å…ˆçº§ï¼‰")
                
            priority_recommendations.extend([
                "Cookieæ± ç®¡ç†å®ç°ï¼ˆé«˜ä¼˜å…ˆçº§ï¼‰",
                "æµè§ˆå™¨æ± åŒ–ï¼ˆä¸­ä¼˜å…ˆçº§ï¼‰",
                "æ™ºèƒ½é‡è¯•æœºåˆ¶ï¼ˆä½ä¼˜å…ˆçº§ï¼‰"
            ])
            
            recommendations["priority_recommendations"] = priority_recommendations
            
            return recommendations
        
        except Exception as e:
            logger.error(f"æ‰©å±•å»ºè®®ç”Ÿæˆå¤±è´¥: {e}")
            return {}
    
    def _project_performance_under_load(self) -> Dict[str, any]:
        """é¢„æµ‹è´Ÿè½½ä¸‹çš„æ€§èƒ½è¡¨ç°"""
        try:
            load_scenarios = {
                "light_load": {"requests_per_hour": 10, "concurrent_avg": 1},
                "medium_load": {"requests_per_hour": 50, "concurrent_avg": 2},
                "heavy_load": {"requests_per_hour": 100, "concurrent_avg": 3},
                "peak_load": {"requests_per_hour": 200, "concurrent_avg": 5}
            }
            
            projections = {}
            
            for scenario_name, scenario in load_scenarios.items():
                requests_per_hour = scenario["requests_per_hour"]
                avg_concurrent = scenario["concurrent_avg"]
                
                # è®¡ç®—èµ„æºä½¿ç”¨é¢„æµ‹
                cpu_usage = min(100, avg_concurrent * self.baseline_performance["single_download_cpu"] * 100)
                memory_usage = min(100, avg_concurrent * self.baseline_performance["single_download_memory"] * 100)
                
                # è®¡ç®—å“åº”æ—¶é—´ï¼ˆè€ƒè™‘æ’é˜Ÿå»¶è¿Ÿï¼‰
                service_rate = 3600 / self.baseline_performance["single_download_time"]  # æ¯å°æ—¶å¤„ç†èƒ½åŠ›
                if requests_per_hour < service_rate:
                    # è½»è´Ÿè½½ï¼Œæ— æ’é˜Ÿ
                    avg_response_time = self.baseline_performance["single_download_time"]
                    queue_length = 0
                else:
                    # é‡è´Ÿè½½ï¼Œæœ‰æ’é˜Ÿ
                    utilization = requests_per_hour / service_rate
                    avg_response_time = self.baseline_performance["single_download_time"] / (1 - utilization)
                    queue_length = (utilization ** 2) / (1 - utilization)
                
                # è®¡ç®—æˆåŠŸç‡ï¼ˆè´Ÿè½½è¶Šé«˜ï¼ŒæˆåŠŸç‡å¯èƒ½ä¸‹é™ï¼‰
                success_rate = self.baseline_performance["success_rate"] * (0.95 ** (avg_concurrent - 1))
                
                projections[scenario_name] = {
                    "requests_per_hour": requests_per_hour,
                    "avg_concurrent": avg_concurrent,
                    "cpu_usage": cpu_usage,
                    "memory_usage": memory_usage,
                    "avg_response_time": avg_response_time,
                    "queue_length": queue_length,
                    "success_rate": success_rate,
                    "system_stable": (cpu_usage < 80 and memory_usage < 75 and avg_response_time < 120),
                    "recommendations": self._get_load_recommendations(cpu_usage, memory_usage, avg_response_time)
                }
            
            return {
                "scenarios": projections,
                "capacity_ceiling": self._calculate_capacity_ceiling(),
                "performance_degradation_points": self._identify_degradation_points(projections)
            }
        
        except Exception as e:
            logger.error(f"æ€§èƒ½é¢„æµ‹å¤±è´¥: {e}")
            return {}
    
    def _get_load_recommendations(self, cpu_usage: float, memory_usage: float, response_time: float) -> List[str]:
        """åŸºäºè´Ÿè½½æƒ…å†µç”Ÿæˆå»ºè®®"""
        recommendations = []
        
        if cpu_usage > 80:
            recommendations.append("CPUä½¿ç”¨ç‡è¿‡é«˜ï¼Œå»ºè®®å¢åŠ CPUæ ¸å¿ƒæˆ–å‡å°‘å¹¶å‘æ•°")
        if memory_usage > 75:
            recommendations.append("å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜ï¼Œå»ºè®®å¢åŠ å†…å­˜æˆ–ä¼˜åŒ–å†…å­˜ä½¿ç”¨")
        if response_time > 90:
            recommendations.append("å“åº”æ—¶é—´è¿‡é•¿ï¼Œå»ºè®®å¢åŠ å¹¶å‘å¤„ç†èƒ½åŠ›æˆ–ä¼˜åŒ–æµç¨‹")
        
        if not recommendations:
            recommendations.append("ç³»ç»Ÿè¿è¡Œæ­£å¸¸ï¼Œå¯ç»§ç»­å½“å‰é…ç½®")
        
        return recommendations
    
    def _calculate_capacity_ceiling(self) -> Dict[str, any]:
        """è®¡ç®—å®¹é‡ä¸Šé™"""
        try:
            # åŸºäºä¸åŒèµ„æºé™åˆ¶è®¡ç®—ç†è®ºä¸Šé™
            cpu_ceiling = int(80 / (self.baseline_performance["single_download_cpu"] * 100))
            memory_ceiling = int(75 / (self.baseline_performance["single_download_memory"] * 100))
            
            # è€ƒè™‘å“åº”æ—¶é—´é™åˆ¶
            max_acceptable_response_time = 120  # ç§’
            response_time_ceiling = int(max_acceptable_response_time / self.baseline_performance["single_download_time"])
            
            theoretical_ceiling = min(cpu_ceiling, memory_ceiling, response_time_ceiling)
            
            # åº”ç”¨å®‰å…¨ç³»æ•°
            practical_ceiling = int(theoretical_ceiling * 0.8)
            
            return {
                "theoretical_max_concurrent": theoretical_ceiling,
                "practical_max_concurrent": practical_ceiling,
                "limiting_factors": {
                    "cpu_limit": cpu_ceiling,
                    "memory_limit": memory_ceiling,
                    "response_time_limit": response_time_ceiling
                },
                "max_throughput_per_hour": int(practical_ceiling * (3600 / self.baseline_performance["single_download_time"])),
                "max_throughput_per_day": int(practical_ceiling * (86400 / self.baseline_performance["single_download_time"]))
            }
        
        except Exception as e:
            logger.error(f"å®¹é‡ä¸Šé™è®¡ç®—å¤±è´¥: {e}")
            return {}
    
    def _identify_degradation_points(self, projections: Dict) -> List[Dict[str, any]]:
        """è¯†åˆ«æ€§èƒ½é™çº§ç‚¹"""
        degradation_points = []
        
        for scenario_name, projection in projections.items():
            if not projection["system_stable"]:
                degradation_points.append({
                    "scenario": scenario_name,
                    "requests_per_hour": projection["requests_per_hour"],
                    "degradation_reason": self._get_degradation_reason(projection),
                    "impact": "ç³»ç»Ÿæ€§èƒ½æ˜¾è‘—ä¸‹é™",
                    "mitigation": "éœ€è¦æ‰©å®¹æˆ–ä¼˜åŒ–"
                })
        
        return degradation_points
    
    def _get_degradation_reason(self, projection: Dict) -> str:
        """è·å–æ€§èƒ½é™çº§åŸå› """
        reasons = []
        
        if projection["cpu_usage"] >= 80:
            reasons.append("CPUä½¿ç”¨ç‡è¿‡é«˜")
        if projection["memory_usage"] >= 75:
            reasons.append("å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜")
        if projection["avg_response_time"] >= 120:
            reasons.append("å“åº”æ—¶é—´è¿‡é•¿")
        
        return "; ".join(reasons) if reasons else "æœªçŸ¥åŸå› "
    
    def generate_capacity_report(self) -> Dict[str, any]:
        """ç”Ÿæˆå®Œæ•´çš„å®¹é‡è§„åˆ’æŠ¥å‘Š"""
        try:
            logger.info("ğŸ“‹ ç”Ÿæˆå®¹é‡è§„åˆ’æŠ¥å‘Š...")
            
            # åˆ†æå•å®ä¾‹å®¹é‡
            single_instance_analysis = self.analyze_single_instance_capacity()
            
            # ç”Ÿæˆå¤šå®ä¾‹æ‰©å±•æ–¹æ¡ˆ
            multi_instance_scenarios = self._analyze_multi_instance_scaling()
            
            # æˆæœ¬æ•ˆç›Šåˆ†æ
            cost_benefit_analysis = self._perform_cost_benefit_analysis()
            
            # é£é™©è¯„ä¼°
            risk_assessment = self._assess_scaling_risks()
            
            report = {
                "report_metadata": {
                    "generated_at": datetime.now().isoformat(),
                    "report_version": "1.0",
                    "baseline_date": "2025-08-28",
                    "baseline_success_rate": self.baseline_performance["success_rate"]
                },
                "executive_summary": {
                    "current_capacity": single_instance_analysis.get("capacity_limits", {}),
                    "key_bottlenecks": [b["type"] for b in single_instance_analysis.get("bottleneck_analysis", [])],
                    "recommended_actions": single_instance_analysis.get("scaling_recommendations", {}).get("priority_recommendations", [])
                },
                "detailed_analysis": {
                    "single_instance": single_instance_analysis,
                    "multi_instance_scaling": multi_instance_scenarios,
                    "cost_benefit": cost_benefit_analysis,
                    "risk_assessment": risk_assessment
                },
                "implementation_roadmap": self._create_implementation_roadmap(),
                "monitoring_recommendations": self._generate_monitoring_recommendations()
            }
            
            # ä¿å­˜æŠ¥å‘Š
            report_file = f"/root/projects/tencent-doc-manager/reports/capacity_planning_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            os.makedirs(os.path.dirname(report_file), exist_ok=True)
            
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2, default=str)
            
            logger.info(f"âœ… å®¹é‡è§„åˆ’æŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")
            
            return report
        
        except Exception as e:
            logger.error(f"âŒ å®¹é‡è§„åˆ’æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
            return {"error": str(e)}
    
    def _analyze_multi_instance_scaling(self) -> Dict[str, any]:
        """åˆ†æå¤šå®ä¾‹æ‰©å±•æ–¹æ¡ˆ"""
        try:
            scaling_scenarios = {}
            
            # 2-10å®ä¾‹çš„æ‰©å±•æ–¹æ¡ˆ
            for instance_count in range(2, 11):
                # çº¿æ€§æ‰©å±•å‡è®¾ï¼ˆå®é™…å¯èƒ½æœ‰é¢å¤–å¼€é”€ï¼‰
                theoretical_throughput = instance_count * (3600 / self.baseline_performance["single_download_time"])
                
                # è€ƒè™‘åè°ƒå¼€é”€ï¼ˆæ¯å¢åŠ ä¸€ä¸ªå®ä¾‹ï¼Œæ•ˆç‡ä¸‹é™2%ï¼‰
                efficiency_factor = 0.98 ** (instance_count - 1)
                actual_throughput = theoretical_throughput * efficiency_factor
                
                # æˆæœ¬ä¼°ç®—ï¼ˆåŸºäºäº‘æœåŠ¡å™¨ä»·æ ¼ï¼‰
                estimated_hourly_cost = instance_count * 0.5  # $0.5/å°æ—¶æ¯å®ä¾‹
                cost_per_download = estimated_hourly_cost / actual_throughput
                
                scaling_scenarios[f"{instance_count}_instances"] = {
                    "instance_count": instance_count,
                    "theoretical_throughput_per_hour": int(theoretical_throughput),
                    "actual_throughput_per_hour": int(actual_throughput),
                    "efficiency_factor": efficiency_factor,
                    "estimated_hourly_cost_usd": estimated_hourly_cost,
                    "cost_per_download_usd": cost_per_download,
                    "scalability_bottlenecks": self._identify_scaling_bottlenecks(instance_count)
                }
            
            # æ‰¾å‡ºæœ€ä½³æ€§ä»·æ¯”é…ç½®
            best_value = min(
                scaling_scenarios.items(),
                key=lambda x: x[1]["cost_per_download_usd"]
            )
            
            return {
                "scenarios": scaling_scenarios,
                "best_value_config": {
                    "scenario": best_value[0],
                    "details": best_value[1]
                },
                "scaling_considerations": [
                    "Cookieç®¡ç†éœ€è¦åœ¨å¤šå®ä¾‹é—´åè°ƒ",
                    "è´Ÿè½½å‡è¡¡å™¨å¢åŠ é¢å¤–å»¶è¿Ÿ",
                    "ç›‘æ§å’Œæ—¥å¿—æ”¶é›†å¤æ‚åŒ–",
                    "æ•…éšœæ¢å¤éœ€è¦è·¨å®ä¾‹åè°ƒ"
                ]
            }
        
        except Exception as e:
            logger.error(f"å¤šå®ä¾‹æ‰©å±•åˆ†æå¤±è´¥: {e}")
            return {}
    
    def _identify_scaling_bottlenecks(self, instance_count: int) -> List[str]:
        """è¯†åˆ«æ‰©å±•ç“¶é¢ˆ"""
        bottlenecks = []
        
        if instance_count > 3:
            bottlenecks.append("Cookieç®¡ç†å¤æ‚åŒ–")
        if instance_count > 5:
            bottlenecks.append("å¯èƒ½è§¦å‘è…¾è®¯æ–‡æ¡£çš„IPçº§åˆ«é™åˆ¶")
        if instance_count > 8:
            bottlenecks.append("åè°ƒå¼€é”€æ˜¾è‘—å½±å“æ•ˆç‡")
        
        return bottlenecks
    
    def _perform_cost_benefit_analysis(self) -> Dict[str, any]:
        """æ‰§è¡Œæˆæœ¬æ•ˆç›Šåˆ†æ"""
        try:
            # åŸºç¡€æˆæœ¬ï¼ˆå•å®ä¾‹ï¼‰
            single_instance_cost = {
                "hardware_monthly": 100,  # $100/æœˆ åŸºç¡€æœåŠ¡å™¨
                "bandwidth_monthly": 50,   # $50/æœˆ å¸¦å®½è´¹ç”¨
                "maintenance_monthly": 30,  # $30/æœˆ ç»´æŠ¤è´¹ç”¨
                "total_monthly": 180
            }
            
            # æ‰©å±•æ–¹æ¡ˆæˆæœ¬åˆ†æ
            scaling_options = [
                {
                    "name": "å‚ç›´æ‰©å±•ï¼ˆCPU+å†…å­˜å‡çº§ï¼‰",
                    "additional_cost_monthly": 80,
                    "performance_improvement": 2.5,
                    "implementation_complexity": "low",
                    "roi_months": 3
                },
                {
                    "name": "æ°´å¹³æ‰©å±•ï¼ˆ3å®ä¾‹ï¼‰",
                    "additional_cost_monthly": 360,
                    "performance_improvement": 2.8,
                    "implementation_complexity": "medium",
                    "roi_months": 6
                },
                {
                    "name": "æ··åˆæ‰©å±•ï¼ˆ2å®ä¾‹+ç¡¬ä»¶å‡çº§ï¼‰",
                    "additional_cost_monthly": 260,
                    "performance_improvement": 4.0,
                    "implementation_complexity": "medium",
                    "roi_months": 4
                }
            ]
            
            # è®¡ç®—æ¯ç§æ–¹æ¡ˆçš„æ€§ä»·æ¯”
            for option in scaling_options:
                total_cost = single_instance_cost["total_monthly"] + option["additional_cost_monthly"]
                cost_per_performance_unit = total_cost / option["performance_improvement"]
                option["cost_per_performance_unit"] = cost_per_performance_unit
                option["total_monthly_cost"] = total_cost
            
            # æ¨èæ–¹æ¡ˆ
            best_roi = min(scaling_options, key=lambda x: x["cost_per_performance_unit"])
            
            return {
                "baseline_cost": single_instance_cost,
                "scaling_options": scaling_options,
                "recommended_option": best_roi,
                "break_even_analysis": {
                    "monthly_processing_volume_threshold": 1000,  # æ¯æœˆ1000æ¬¡ä¸‹è½½ä¸ºç›ˆäºå¹³è¡¡ç‚¹
                    "cost_savings_percentage": 25  # è‡ªåŠ¨åŒ–ç›¸æ¯”äººå·¥èŠ‚çœ25%æˆæœ¬
                }
            }
        
        except Exception as e:
            logger.error(f"æˆæœ¬æ•ˆç›Šåˆ†æå¤±è´¥: {e}")
            return {}
    
    def _assess_scaling_risks(self) -> Dict[str, any]:
        """è¯„ä¼°æ‰©å±•é£é™©"""
        risks = {
            "technical_risks": [
                {
                    "risk": "Cookieç®¡ç†å¤æ‚åŒ–",
                    "probability": "high",
                    "impact": "medium",
                    "mitigation": "å®æ–½Cookieæ± å’Œè½®è½¬æœºåˆ¶",
                    "monitoring": "Cookieæœ‰æ•ˆæ€§å’Œä½¿ç”¨ç‡ç›‘æ§"
                },
                {
                    "risk": "åçˆ¬è™«æ£€æµ‹å¢å¼º",
                    "probability": "medium",
                    "impact": "high",
                    "mitigation": "é™ä½è¯·æ±‚é¢‘ç‡ï¼Œå¢åŠ éšæœºå»¶è¿Ÿ",
                    "monitoring": "æˆåŠŸç‡å’Œé”™è¯¯ç±»å‹ç›‘æ§"
                },
                {
                    "risk": "ç³»ç»Ÿå¤æ‚æ€§å¢åŠ ",
                    "probability": "high",
                    "impact": "medium",
                    "mitigation": "å®Œå–„ç›‘æ§å’Œè‡ªåŠ¨åŒ–è¿ç»´",
                    "monitoring": "ç³»ç»Ÿå¥åº·åº¦å’Œæ€§èƒ½æŒ‡æ ‡ç›‘æ§"
                }
            ],
            "business_risks": [
                {
                    "risk": "è¿è¥æˆæœ¬ä¸Šå‡",
                    "probability": "medium",
                    "impact": "medium",
                    "mitigation": "åŸºäºå®é™…ä½¿ç”¨é‡åŠ¨æ€è°ƒæ•´èµ„æº",
                    "monitoring": "æˆæœ¬æ•ˆç›Šæ¯”ç›‘æ§"
                },
                {
                    "risk": "æœåŠ¡ä¾èµ–æ€§å¢å¼º",
                    "probability": "high",
                    "impact": "medium",
                    "mitigation": "å®æ–½é™çº§æœºåˆ¶å’Œæ‰‹åŠ¨å¤‡ä»½æ–¹æ¡ˆ",
                    "monitoring": "æœåŠ¡å¯ç”¨æ€§ç›‘æ§"
                }
            ],
            "operational_risks": [
                {
                    "risk": "ç»´æŠ¤å¤æ‚æ€§å¢åŠ ",
                    "probability": "high",
                    "impact": "low",
                    "mitigation": "æ ‡å‡†åŒ–éƒ¨ç½²å’Œè¿ç»´æµç¨‹",
                    "monitoring": "è¿ç»´æ•ˆç‡å’Œå“åº”æ—¶é—´ç›‘æ§"
                },
                {
                    "risk": "æ•…éšœå½±å“èŒƒå›´æ‰©å¤§",
                    "probability": "medium",
                    "impact": "high",
                    "mitigation": "å®æ–½æ•…éšœéš”ç¦»å’Œå¿«é€Ÿæ¢å¤æœºåˆ¶",
                    "monitoring": "æ•…éšœæ£€æµ‹å’Œæ¢å¤æ—¶é—´ç›‘æ§"
                }
            ]
        }
        
        return risks
    
    def _create_implementation_roadmap(self) -> Dict[str, any]:
        """åˆ›å»ºå®æ–½è·¯çº¿å›¾"""
        roadmap = {
            "phase_1": {
                "name": "åŸºç¡€ä¼˜åŒ–é˜¶æ®µ",
                "duration": "2-4å‘¨",
                "objectives": [
                    "å®æ–½æµè§ˆå™¨æ± åŒ–",
                    "ä¼˜åŒ–Cookieç®¡ç†æœºåˆ¶",
                    "æ·»åŠ åŸºç¡€ç›‘æ§"
                ],
                "expected_improvement": "30-50%æ€§èƒ½æå‡",
                "investment": "ä½",
                "risk": "ä½"
            },
            "phase_2": {
                "name": "å‚ç›´æ‰©å±•é˜¶æ®µ",
                "duration": "1-2å‘¨",
                "objectives": [
                    "å‡çº§ç¡¬ä»¶é…ç½®",
                    "ä¼˜åŒ–èµ„æºåˆ©ç”¨",
                    "å®æ–½æ™ºèƒ½é‡è¯•æœºåˆ¶"
                ],
                "expected_improvement": "100-150%æ€§èƒ½æå‡",
                "investment": "ä¸­",
                "risk": "ä½"
            },
            "phase_3": {
                "name": "æ°´å¹³æ‰©å±•é˜¶æ®µ",
                "duration": "4-6å‘¨",
                "objectives": [
                    "å®æ–½å¤šå®ä¾‹éƒ¨ç½²",
                    "æ„å»ºè´Ÿè½½å‡è¡¡",
                    "å®æ–½åˆ†å¸ƒå¼Cookieç®¡ç†"
                ],
                "expected_improvement": "200-300%æ€§èƒ½æå‡",
                "investment": "é«˜",
                "risk": "ä¸­"
            },
            "phase_4": {
                "name": "æ™ºèƒ½ä¼˜åŒ–é˜¶æ®µ",
                "duration": "2-3å‘¨",
                "objectives": [
                    "å®æ–½AIé©±åŠ¨çš„è´Ÿè½½é¢„æµ‹",
                    "è‡ªåŠ¨åŒ–æ‰©ç¼©å®¹",
                    "é«˜çº§ç›‘æ§å’Œå‘Šè­¦"
                ],
                "expected_improvement": "ä¼˜åŒ–è¿è¥æ•ˆç‡",
                "investment": "ä¸­",
                "risk": "ä¸­"
            }
        }
        
        return roadmap
    
    def _generate_monitoring_recommendations(self) -> Dict[str, any]:
        """ç”Ÿæˆç›‘æ§å»ºè®®"""
        return {
            "key_metrics": [
                {
                    "metric": "å¹¶å‘ä¸‹è½½æ•°",
                    "threshold": "< 5ä¸ª",
                    "alert_level": "warning"
                },
                {
                    "metric": "å¹³å‡å“åº”æ—¶é—´",
                    "threshold": "> 60ç§’",
                    "alert_level": "warning"
                },
                {
                    "metric": "CPUä½¿ç”¨ç‡",
                    "threshold": "> 80%",
                    "alert_level": "critical"
                },
                {
                    "metric": "å†…å­˜ä½¿ç”¨ç‡",
                    "threshold": "> 75%",
                    "alert_level": "warning"
                },
                {
                    "metric": "ä¸‹è½½æˆåŠŸç‡",
                    "threshold": "< 85%",
                    "alert_level": "critical"
                }
            ],
            "dashboard_requirements": [
                "å®æ—¶æ€§èƒ½ç›‘æ§é¢æ¿",
                "å®¹é‡åˆ©ç”¨ç‡è¶‹åŠ¿å›¾",
                "é”™è¯¯ç‡å’Œå¤±è´¥åŸå› åˆ†æ",
                "æˆæœ¬æ•ˆç›Šè¿½è¸ªå›¾è¡¨"
            ],
            "automated_responses": [
                "é«˜è´Ÿè½½æ—¶è‡ªåŠ¨é™æµ",
                "èµ„æºè€—å°½æ—¶è‡ªåŠ¨æ¸…ç†",
                "è¿ç»­å¤±è´¥æ—¶è‡ªåŠ¨é™çº§"
            ]
        }


async def main():
    """ä¸»å‡½æ•°"""
    try:
        print("=== è…¾è®¯æ–‡æ¡£ç³»ç»Ÿå®¹é‡è§„åˆ’åˆ†æ ===")
        
        planner = CapacityPlanner()
        
        # ç”Ÿæˆå®Œæ•´çš„å®¹é‡è§„åˆ’æŠ¥å‘Š
        report = planner.generate_capacity_report()
        
        if "error" not in report:
            print("\nâœ… å®¹é‡è§„åˆ’åˆ†æå®Œæˆ")
            print(f"ğŸ“Š å•å®ä¾‹ç†è®ºæœ€å¤§å¹¶å‘: {report['detailed_analysis']['single_instance']['capacity_limits']['theoretical_max_concurrent']}")
            print(f"ğŸ”’ å•å®ä¾‹å®‰å…¨æœ€å¤§å¹¶å‘: {report['detailed_analysis']['single_instance']['capacity_limits']['safe_max_concurrent']}")
            print(f"ğŸ’¡ æ¨èå¹¶å‘æ•°: {report['detailed_analysis']['single_instance']['capacity_limits']['recommended_concurrent']}")
            
            print(f"\nğŸ“ˆ é¢„è®¡ååé‡:")
            throughput = report['detailed_analysis']['single_instance']['throughput_analysis']['recommended_config']['details']
            print(f"   - æ¯å°æ—¶: {throughput['downloads_per_hour']} æ¬¡ä¸‹è½½")
            print(f"   - æ¯å¤©: {throughput['downloads_per_day']} æ¬¡ä¸‹è½½")
            
            print(f"\nâš ï¸ ä¸»è¦ç“¶é¢ˆ:")
            for bottleneck in report['detailed_analysis']['single_instance']['bottleneck_analysis']:
                print(f"   - {bottleneck['type']}: {bottleneck['description']}")
        else:
            print(f"âŒ åˆ†æå¤±è´¥: {report['error']}")
            
    except Exception as e:
        print(f"âŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")


if __name__ == "__main__":
    # é…ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # è¿è¡Œåˆ†æ
    asyncio.run(main())