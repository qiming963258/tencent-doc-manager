#!/usr/bin/env python3
"""
è½»é‡çº§è…¾è®¯æ–‡æ¡£ä¸‹è½½å™¨
ä¸ä½¿ç”¨æµè§ˆå™¨ï¼Œç›´æ¥é€šè¿‡APIä¸‹è½½
"""

import requests
import json
import re
import os
import time
from datetime import datetime
from pathlib import Path
import urllib.parse

class LightweightTencentDownloader:
    """è½»é‡çº§è…¾è®¯æ–‡æ¡£ä¸‹è½½å™¨"""
    
    def __init__(self):
        self.session = requests.Session()
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
            'Accept': 'application/json, text/plain, */*',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Referer': 'https://docs.qq.com/',
            'Origin': 'https://docs.qq.com',
        }
        self.session.headers.update(self.headers)
        
    def set_cookies(self, cookie_string):
        """è®¾ç½®Cookie"""
        if not cookie_string:
            return
            
        # è§£æCookieå­—ç¬¦ä¸²
        cookies_dict = {}
        for item in cookie_string.split(';'):
            if '=' in item:
                key, value = item.strip().split('=', 1)
                cookies_dict[key] = value
        
        # è®¾ç½®åˆ°sessionä¸­
        for key, value in cookies_dict.items():
            self.session.cookies.set(key, value, domain='.qq.com')
            
        print(f"âœ… è®¾ç½®äº† {len(cookies_dict)} ä¸ªCookie")
    
    def extract_doc_info(self, url):
        """ä»URLæå–æ–‡æ¡£ä¿¡æ¯"""
        # æå–æ–‡æ¡£ID
        doc_id_match = re.search(r'/sheet/([A-Za-z0-9]+)', url)
        if doc_id_match:
            doc_id = doc_id_match.group(1)
            doc_type = 'sheet'
        else:
            doc_id_match = re.search(r'/doc/([A-Za-z0-9]+)', url)
            if doc_id_match:
                doc_id = doc_id_match.group(1)
                doc_type = 'doc'
            else:
                raise ValueError(f"æ— æ³•ä»URLæå–æ–‡æ¡£ID: {url}")
        
        # æå–tabå‚æ•°ï¼ˆå¦‚æœæœ‰ï¼‰
        tab_match = re.search(r'[?&]tab=([^&]+)', url)
        tab_id = tab_match.group(1) if tab_match else None
        
        return {
            'doc_id': doc_id,
            'doc_type': doc_type,
            'tab_id': tab_id,
            'url': url
        }
    
    def get_export_url(self, doc_info, format='csv'):
        """æ„é€ å¯¼å‡ºURL"""
        doc_id = doc_info['doc_id']
        doc_type = doc_info['doc_type']
        
        if doc_type == 'sheet':
            # è…¾è®¯è¡¨æ ¼å¯¼å‡ºæ¥å£
            if format == 'csv':
                # CSVå¯¼å‡ºï¼ˆå•ä¸ªå·¥ä½œè¡¨ï¼‰
                export_url = f"https://docs.qq.com/dop-api/get/sheet/export"
                params = {
                    'id': doc_id,
                    'format': 'csv',
                    'type': 'download'
                }
                if doc_info.get('tab_id'):
                    params['tab'] = doc_info['tab_id']
            else:
                # Excelå¯¼å‡ºï¼ˆæ‰€æœ‰å·¥ä½œè¡¨ï¼‰
                export_url = f"https://docs.qq.com/dop-api/get/sheet/export"
                params = {
                    'id': doc_id,
                    'format': 'xlsx',
                    'type': 'download'
                }
        else:
            # è…¾è®¯æ–‡æ¡£å¯¼å‡ºæ¥å£
            export_url = f"https://docs.qq.com/dop-api/get/doc/export"
            params = {
                'id': doc_id,
                'format': format,
                'type': 'download'
            }
        
        return export_url, params
    
    def download_via_api(self, url, cookie_string, format='csv', save_path=None):
        """é€šè¿‡APIä¸‹è½½æ–‡æ¡£"""
        try:
            print(f"ğŸš€ è½»é‡çº§ä¸‹è½½å™¨å¯åŠ¨")
            print(f"ğŸ“„ ç›®æ ‡URL: {url}")
            print(f"ğŸ“¦ æ ¼å¼: {format}")
            
            # è®¾ç½®Cookie
            self.set_cookies(cookie_string)
            
            # æå–æ–‡æ¡£ä¿¡æ¯
            doc_info = self.extract_doc_info(url)
            print(f"ğŸ“‹ æ–‡æ¡£ID: {doc_info['doc_id']}")
            print(f"ğŸ“‹ æ–‡æ¡£ç±»å‹: {doc_info['doc_type']}")
            
            # æ–¹æ³•1ï¼šå°è¯•ç›´æ¥å¯¼å‡ºAPI
            export_url, params = self.get_export_url(doc_info, format)
            print(f"ğŸ”— å¯¼å‡ºURL: {export_url}")
            
            response = self.session.get(export_url, params=params, stream=True)
            
            if response.status_code == 200:
                # æ£€æŸ¥å†…å®¹ç±»å‹
                content_type = response.headers.get('Content-Type', '')
                if 'json' in content_type:
                    # å¦‚æœè¿”å›JSONï¼Œå¯èƒ½éœ€è¦è¿›ä¸€æ­¥å¤„ç†
                    data = response.json()
                    if 'download_url' in data:
                        # è·å–å®é™…ä¸‹è½½é“¾æ¥
                        download_url = data['download_url']
                        print(f"ğŸ“¥ è·å–åˆ°ä¸‹è½½é“¾æ¥: {download_url}")
                        response = self.session.get(download_url, stream=True)
                    else:
                        print(f"âš ï¸ APIè¿”å›: {data}")
                        return None
                
                # ä¿å­˜æ–‡ä»¶
                if not save_path:
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    filename = f"tencent_doc_{doc_info['doc_id']}_{timestamp}.{format}"
                    save_path = Path('/root/projects/tencent-doc-manager/downloads') / filename
                    save_path.parent.mkdir(parents=True, exist_ok=True)
                
                with open(save_path, 'wb') as f:
                    for chunk in response.iter_content(chunk_size=8192):
                        if chunk:
                            f.write(chunk)
                
                print(f"âœ… ä¸‹è½½æˆåŠŸ: {save_path}")
                return str(save_path)
            else:
                print(f"âŒ APIè¿”å›é”™è¯¯: {response.status_code}")
                print(f"å“åº”å†…å®¹: {response.text[:500]}")
                
                # æ–¹æ³•2ï¼šå°è¯•è·å–é¡µé¢å¹¶è§£æå¯¼å‡ºé“¾æ¥
                return self.download_via_page_parse(url, cookie_string, format)
                
        except Exception as e:
            print(f"âŒ ä¸‹è½½å¤±è´¥: {e}")
            return None
    
    def download_via_page_parse(self, url, cookie_string, format='csv'):
        """é€šè¿‡è§£æé¡µé¢è·å–ä¸‹è½½é“¾æ¥"""
        try:
            print(f"ğŸ” å°è¯•è§£æé¡µé¢è·å–ä¸‹è½½é“¾æ¥...")
            
            # è®¿é—®æ–‡æ¡£é¡µé¢
            response = self.session.get(url)
            if response.status_code != 200:
                print(f"âŒ æ— æ³•è®¿é—®æ–‡æ¡£é¡µé¢: {response.status_code}")
                return None
            
            # æŸ¥æ‰¾å¯èƒ½çš„å¯¼å‡ºæ¥å£
            # è…¾è®¯æ–‡æ¡£é€šå¸¸åœ¨é¡µé¢ä¸­åŒ…å«é…ç½®æ•°æ®
            config_match = re.search(r'window\.__INITIAL_STATE__\s*=\s*({.*?});', response.text)
            if config_match:
                try:
                    config_data = json.loads(config_match.group(1))
                    # å°è¯•ä»é…ç½®ä¸­æå–å¯¼å‡ºç›¸å…³ä¿¡æ¯
                    if 'doc' in config_data and 'export_url' in config_data['doc']:
                        export_url = config_data['doc']['export_url']
                        print(f"ğŸ“¥ æ‰¾åˆ°å¯¼å‡ºURL: {export_url}")
                        # ä¸‹è½½æ–‡ä»¶
                        download_response = self.session.get(export_url, stream=True)
                        if download_response.status_code == 200:
                            save_path = self.save_response(download_response, format)
                            return save_path
                except json.JSONDecodeError:
                    pass
            
            # æŸ¥æ‰¾å¯¼å‡ºæŒ‰é’®æˆ–é“¾æ¥
            export_patterns = [
                r'href="([^"]*export[^"]*)"',
                r'data-export-url="([^"]*)"',
                r'exportUrl["\']:\s*["\']([^"\']+)["\']'
            ]
            
            for pattern in export_patterns:
                matches = re.findall(pattern, response.text)
                if matches:
                    for match in matches:
                        if format in match.lower() or 'export' in match.lower():
                            print(f"ğŸ“¥ æ‰¾åˆ°å¯èƒ½çš„å¯¼å‡ºé“¾æ¥: {match}")
                            # å°è¯•ä¸‹è½½
                            if not match.startswith('http'):
                                match = f"https://docs.qq.com{match}"
                            download_response = self.session.get(match, stream=True)
                            if download_response.status_code == 200:
                                save_path = self.save_response(download_response, format)
                                if save_path:
                                    return save_path
            
            print("âŒ æ— æ³•ä»é¡µé¢ä¸­æ‰¾åˆ°å¯¼å‡ºé“¾æ¥")
            return None
            
        except Exception as e:
            print(f"âŒ é¡µé¢è§£æå¤±è´¥: {e}")
            return None
    
    def save_response(self, response, format):
        """ä¿å­˜å“åº”å†…å®¹åˆ°æ–‡ä»¶"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"tencent_doc_{timestamp}.{format}"
            save_path = Path('/root/projects/tencent-doc-manager/downloads') / filename
            save_path.parent.mkdir(parents=True, exist_ok=True)
            
            with open(save_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
            
            print(f"âœ… æ–‡ä»¶ä¿å­˜: {save_path}")
            return str(save_path)
        except Exception as e:
            print(f"âŒ ä¿å­˜æ–‡ä»¶å¤±è´¥: {e}")
            return None
    
    def test_lightweight_download(self):
        """æµ‹è¯•è½»é‡çº§ä¸‹è½½"""
        print("=" * 60)
        print("è½»é‡çº§ä¸‹è½½æµ‹è¯•")
        print("=" * 60)
        
        # æµ‹è¯•URL
        test_url = "https://docs.qq.com/sheet/DWEFNU25TemFnZXJN"
        test_cookie = "test_cookie"  # éœ€è¦çœŸå®Cookie
        
        # å†…å­˜ä½¿ç”¨å¯¹æ¯”
        import psutil
        process = psutil.Process()
        
        mem_before = process.memory_info().rss / 1024 / 1024
        print(f"ğŸ“Š ä¸‹è½½å‰å†…å­˜ä½¿ç”¨: {mem_before:.2f}MB")
        
        # æ‰§è¡Œä¸‹è½½
        result = self.download_via_api(test_url, test_cookie, 'csv')
        
        mem_after = process.memory_info().rss / 1024 / 1024
        print(f"ğŸ“Š ä¸‹è½½åå†…å­˜ä½¿ç”¨: {mem_after:.2f}MB")
        print(f"ğŸ“Š å†…å­˜å¢é•¿: {mem_after - mem_before:.2f}MB")
        
        if result:
            print(f"âœ… æµ‹è¯•æˆåŠŸï¼æ–‡ä»¶: {result}")
        else:
            print("âŒ æµ‹è¯•å¤±è´¥")
        
        print("=" * 60)
        print("å¯¹æ¯”Chromiumæ–¹æ¡ˆï¼š")
        print("- ChromiumåŸºç¡€å¼€é”€: 500MB+")
        print("- è½»é‡çº§æ–¹æ¡ˆå¼€é”€: <50MB")
        print("- å†…å­˜èŠ‚çœ: 90%+")
        print("=" * 60)

if __name__ == "__main__":
    downloader = LightweightTencentDownloader()
    downloader.test_lightweight_download()