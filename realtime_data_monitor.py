#!/usr/bin/env python3
"""
å®æ—¶æ•°æ®æµç›‘æ§ç³»ç»Ÿ
ç›‘æ§æ•´ä¸ªæ•°æ®å¤„ç†ç®¡é“ï¼Œé˜²æ­¢æ•°æ®ä¸¢å¤±ã€é”™ä½å’Œä¼ é€’å¼‚å¸¸
"""

import json
import time
import threading
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Any, Callable
from dataclasses import dataclass, asdict
from collections import deque

@dataclass
class MonitoringAlert:
    """ç›‘æ§å‘Šè­¦æ•°æ®ç»“æ„"""
    timestamp: str
    alert_type: str
    severity: str  # LOW/MEDIUM/HIGH/CRITICAL
    stage: str
    message: str
    details: Dict[str, Any]
    resolved: bool = False

@dataclass
class PipelineMetrics:
    """ç®¡é“æŒ‡æ ‡æ•°æ®ç»“æ„"""
    timestamp: str
    stage: str
    input_count: int
    output_count: int
    processing_time: float
    success_rate: float
    data_integrity: float
    errors: List[str]

class DataFlowMonitor:
    """å®æ—¶æ•°æ®æµç›‘æ§å™¨"""
    
    def __init__(self, config: Dict[str, Any] = None):
        """
        åˆå§‹åŒ–ç›‘æ§å™¨
        
        Args:
            config: ç›‘æ§é…ç½®
        """
        self.config = config or self._get_default_config()
        self.alerts = deque(maxlen=1000)  # ä¿å­˜æœ€è¿‘1000ä¸ªå‘Šè­¦
        self.metrics_history = deque(maxlen=5000)  # ä¿å­˜æœ€è¿‘5000ä¸ªæŒ‡æ ‡
        self.is_monitoring = False
        self.monitor_thread = None
        self.alert_callbacks = []
        
        # è®¾ç½®æ—¥å¿—
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
        
        # ç›‘æ§é˜ˆå€¼
        self.thresholds = self.config.get("thresholds", {
            "min_column_coverage": 0.8,      # æœ€ä½80%åˆ—è¦†ç›–ç‡
            "max_data_loss_rate": 0.1,       # æœ€é«˜10%æ•°æ®ä¸¢å¤±ç‡
            "min_visible_hotspots": 1,       # è‡³å°‘1ä¸ªå¯è§çƒ­ç‚¹
            "max_processing_time": 30.0,     # æœ€å¤§å¤„ç†æ—¶é—´30ç§’
            "min_success_rate": 0.95         # æœ€å°æˆåŠŸç‡95%
        })
        
    def _get_default_config(self) -> Dict[str, Any]:
        """è·å–é»˜è®¤ç›‘æ§é…ç½®"""
        return {
            "monitoring_interval": 5.0,  # ç›‘æ§é—´éš”5ç§’
            "alert_retention_hours": 24,  # å‘Šè­¦ä¿ç•™24å°æ—¶
            "metrics_retention_hours": 48,  # æŒ‡æ ‡ä¿ç•™48å°æ—¶
            "enable_realtime_alerts": True,
            "alert_channels": ["console", "file"],
            "thresholds": {
                "min_column_coverage": 0.8,
                "max_data_loss_rate": 0.1,
                "min_visible_hotspots": 1,
                "max_processing_time": 30.0,
                "min_success_rate": 0.95
            }
        }
    
    def start_monitoring(self):
        """å¯åŠ¨å®æ—¶ç›‘æ§"""
        if self.is_monitoring:
            self.logger.warning("ç›‘æ§å·²ç»åœ¨è¿è¡Œä¸­")
            return
        
        self.is_monitoring = True
        self.monitor_thread = threading.Thread(target=self._monitoring_loop, daemon=True)
        self.monitor_thread.start()
        
        self.logger.info("ğŸŸ¢ å®æ—¶æ•°æ®æµç›‘æ§å·²å¯åŠ¨")
    
    def stop_monitoring(self):
        """åœæ­¢ç›‘æ§"""
        if not self.is_monitoring:
            return
        
        self.is_monitoring = False
        if self.monitor_thread and self.monitor_thread.is_alive():
            self.monitor_thread.join(timeout=5)
        
        self.logger.info("ğŸ”´ å®æ—¶æ•°æ®æµç›‘æ§å·²åœæ­¢")
    
    def _monitoring_loop(self):
        """ç›‘æ§ä¸»å¾ªç¯"""
        while self.is_monitoring:
            try:
                # æ¸…ç†è¿‡æœŸæ•°æ®
                self._cleanup_expired_data()
                
                # æ£€æŸ¥ç³»ç»Ÿå¥åº·çŠ¶æ€
                self._check_system_health()
                
                # ç­‰å¾…ä¸‹ä¸€æ¬¡ç›‘æ§å‘¨æœŸ
                time.sleep(self.config["monitoring_interval"])
                
            except Exception as e:
                self.logger.error(f"ç›‘æ§å¾ªç¯é”™è¯¯: {str(e)}")
                time.sleep(1)  # é”™è¯¯æ—¶çŸ­æš‚ç­‰å¾…
    
    def record_stage_metrics(self, 
                           stage: str,
                           input_count: int,
                           output_count: int, 
                           processing_time: float,
                           success: bool = True,
                           errors: List[str] = None,
                           additional_data: Dict[str, Any] = None) -> bool:
        """
        è®°å½•é˜¶æ®µå¤„ç†æŒ‡æ ‡
        
        Args:
            stage: å¤„ç†é˜¶æ®µåç§°
            input_count: è¾“å…¥æ•°æ®é‡
            output_count: è¾“å‡ºæ•°æ®é‡  
            processing_time: å¤„ç†æ—¶é—´
            success: æ˜¯å¦æˆåŠŸ
            errors: é”™è¯¯åˆ—è¡¨
            additional_data: é™„åŠ æ•°æ®
        
        Returns:
            bool: æ˜¯å¦è§¦å‘å‘Šè­¦
        """
        
        timestamp = datetime.now().isoformat()
        
        # è®¡ç®—æ•°æ®å®Œæ•´æ€§
        data_integrity = min(1.0, output_count / input_count) if input_count > 0 else 1.0
        
        # åˆ›å»ºæŒ‡æ ‡è®°å½•
        metrics = PipelineMetrics(
            timestamp=timestamp,
            stage=stage,
            input_count=input_count,
            output_count=output_count,
            processing_time=processing_time,
            success_rate=1.0 if success else 0.0,
            data_integrity=data_integrity,
            errors=errors or []
        )
        
        # ä¿å­˜æŒ‡æ ‡
        self.metrics_history.append(metrics)
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦å‘Šè­¦
        alerts_triggered = self._check_stage_thresholds(metrics, additional_data or {})
        
        return len(alerts_triggered) > 0
    
    def _check_stage_thresholds(self, metrics: PipelineMetrics, additional_data: Dict[str, Any]) -> List[MonitoringAlert]:
        """æ£€æŸ¥é˜¶æ®µæŒ‡æ ‡æ˜¯å¦è¶…å‡ºé˜ˆå€¼"""
        
        alerts = []
        timestamp = datetime.now().isoformat()
        
        # æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
        if metrics.data_integrity < (1.0 - self.thresholds["max_data_loss_rate"]):
            alert = MonitoringAlert(
                timestamp=timestamp,
                alert_type="data_integrity_low",
                severity="HIGH",
                stage=metrics.stage,
                message=f"æ•°æ®å®Œæ•´æ€§è¿‡ä½: {metrics.data_integrity:.1%}",
                details={
                    "threshold": 1.0 - self.thresholds["max_data_loss_rate"],
                    "actual": metrics.data_integrity,
                    "input_count": metrics.input_count,
                    "output_count": metrics.output_count
                }
            )
            alerts.append(alert)
        
        # æ£€æŸ¥å¤„ç†æ—¶é—´
        if metrics.processing_time > self.thresholds["max_processing_time"]:
            alert = MonitoringAlert(
                timestamp=timestamp,
                alert_type="processing_time_high",
                severity="MEDIUM",
                stage=metrics.stage,
                message=f"å¤„ç†æ—¶é—´è¿‡é•¿: {metrics.processing_time:.2f}ç§’",
                details={
                    "threshold": self.thresholds["max_processing_time"],
                    "actual": metrics.processing_time
                }
            )
            alerts.append(alert)
        
        # æ£€æŸ¥åˆ—è¦†ç›–ç‡ï¼ˆä»…å¯¹æ™ºèƒ½æ˜ å°„é˜¶æ®µï¼‰
        if metrics.stage == "intelligent_mapping" and "column_coverage" in additional_data:
            coverage = additional_data["column_coverage"]
            if coverage < self.thresholds["min_column_coverage"]:
                alert = MonitoringAlert(
                    timestamp=timestamp,
                    alert_type="column_coverage_low",
                    severity="HIGH",
                    stage=metrics.stage,
                    message=f"åˆ—æ˜ å°„è¦†ç›–ç‡è¿‡ä½: {coverage:.1%}",
                    details={
                        "threshold": self.thresholds["min_column_coverage"],
                        "actual": coverage,
                        "unmapped_columns": additional_data.get("unmapped_columns", [])
                    }
                )
                alerts.append(alert)
        
        # æ£€æŸ¥å¯è§çƒ­ç‚¹æ•°é‡ï¼ˆä»…å¯¹çƒ­åŠ›å›¾ç”Ÿæˆé˜¶æ®µï¼‰
        if metrics.stage == "heatmap_generation" and "visible_hotspots" in additional_data:
            hotspots = additional_data["visible_hotspots"]
            if hotspots < self.thresholds["min_visible_hotspots"]:
                alert = MonitoringAlert(
                    timestamp=timestamp,
                    alert_type="hotspots_insufficient",
                    severity="MEDIUM",
                    stage=metrics.stage,
                    message=f"å¯è§çƒ­ç‚¹æ•°é‡ä¸è¶³: {hotspots}ä¸ª",
                    details={
                        "threshold": self.thresholds["min_visible_hotspots"],
                        "actual": hotspots,
                        "expected": metrics.input_count
                    }
                )
                alerts.append(alert)
        
        # æ£€æŸ¥é”™è¯¯
        if metrics.errors:
            alert = MonitoringAlert(
                timestamp=timestamp,
                alert_type="stage_errors",
                severity="HIGH" if len(metrics.errors) > 1 else "MEDIUM",
                stage=metrics.stage,
                message=f"é˜¶æ®µå¤„ç†å‡ºç°{len(metrics.errors)}ä¸ªé”™è¯¯",
                details={
                    "error_count": len(metrics.errors),
                    "errors": metrics.errors
                }
            )
            alerts.append(alert)
        
        # è§¦å‘å‘Šè­¦
        for alert in alerts:
            self._trigger_alert(alert)
        
        return alerts
    
    def _trigger_alert(self, alert: MonitoringAlert):
        """è§¦å‘å‘Šè­¦"""
        
        # ä¿å­˜å‘Šè­¦
        self.alerts.append(alert)
        
        # è¾“å‡ºåˆ°æ§åˆ¶å°
        if "console" in self.config.get("alert_channels", []):
            severity_icon = {
                "LOW": "ğŸŸ¡",
                "MEDIUM": "ğŸŸ ", 
                "HIGH": "ğŸ”´",
                "CRITICAL": "ğŸ’€"
            }.get(alert.severity, "âšª")
            
            print(f"{severity_icon} [{alert.severity}] {alert.stage}: {alert.message}")
        
        # å†™å…¥æ–‡ä»¶
        if "file" in self.config.get("alert_channels", []):
            self._write_alert_to_file(alert)
        
        # è°ƒç”¨æ³¨å†Œçš„å›è°ƒå‡½æ•°
        for callback in self.alert_callbacks:
            try:
                callback(alert)
            except Exception as e:
                self.logger.error(f"å‘Šè­¦å›è°ƒæ‰§è¡Œå¤±è´¥: {str(e)}")
    
    def _write_alert_to_file(self, alert: MonitoringAlert):
        """å†™å…¥å‘Šè­¦åˆ°æ–‡ä»¶"""
        
        alert_file = "/root/projects/tencent-doc-manager/monitoring_alerts.jsonl"
        
        try:
            with open(alert_file, 'a', encoding='utf-8') as f:
                f.write(json.dumps(asdict(alert), ensure_ascii=False) + "\n")
        except Exception as e:
            self.logger.error(f"å†™å…¥å‘Šè­¦æ–‡ä»¶å¤±è´¥: {str(e)}")
    
    def _check_system_health(self):
        """æ£€æŸ¥ç³»ç»Ÿæ•´ä½“å¥åº·çŠ¶æ€"""
        
        if not self.metrics_history:
            return
        
        # æ£€æŸ¥æœ€è¿‘çš„æˆåŠŸç‡
        recent_metrics = [m for m in self.metrics_history 
                         if datetime.fromisoformat(m.timestamp) > datetime.now() - timedelta(minutes=10)]
        
        if recent_metrics:
            avg_success_rate = sum(m.success_rate for m in recent_metrics) / len(recent_metrics)
            
            if avg_success_rate < self.thresholds["min_success_rate"]:
                alert = MonitoringAlert(
                    timestamp=datetime.now().isoformat(),
                    alert_type="system_health_degraded",
                    severity="CRITICAL",
                    stage="system",
                    message=f"ç³»ç»ŸæˆåŠŸç‡é™ä½: {avg_success_rate:.1%}",
                    details={
                        "threshold": self.thresholds["min_success_rate"],
                        "actual": avg_success_rate,
                        "sample_size": len(recent_metrics)
                    }
                )
                self._trigger_alert(alert)
    
    def _cleanup_expired_data(self):
        """æ¸…ç†è¿‡æœŸæ•°æ®"""
        
        now = datetime.now()
        
        # æ¸…ç†è¿‡æœŸå‘Šè­¦
        alert_cutoff = now - timedelta(hours=self.config["alert_retention_hours"])
        self.alerts = deque([a for a in self.alerts 
                           if datetime.fromisoformat(a.timestamp) > alert_cutoff], 
                          maxlen=1000)
        
        # æ¸…ç†è¿‡æœŸæŒ‡æ ‡
        metrics_cutoff = now - timedelta(hours=self.config["metrics_retention_hours"])
        self.metrics_history = deque([m for m in self.metrics_history 
                                    if datetime.fromisoformat(m.timestamp) > metrics_cutoff], 
                                   maxlen=5000)
    
    def add_alert_callback(self, callback: Callable[[MonitoringAlert], None]):
        """æ·»åŠ å‘Šè­¦å›è°ƒå‡½æ•°"""
        self.alert_callbacks.append(callback)
    
    def get_current_status(self) -> Dict[str, Any]:
        """è·å–å½“å‰ç›‘æ§çŠ¶æ€"""
        
        if not self.metrics_history:
            return {
                "status": "no_data",
                "monitoring": self.is_monitoring
            }
        
        recent_metrics = [m for m in self.metrics_history 
                         if datetime.fromisoformat(m.timestamp) > datetime.now() - timedelta(minutes=30)]
        
        if not recent_metrics:
            return {
                "status": "stale_data",
                "monitoring": self.is_monitoring,
                "last_update": self.metrics_history[-1].timestamp if self.metrics_history else None
            }
        
        # è®¡ç®—æ•´ä½“å¥åº·æŒ‡æ ‡
        avg_data_integrity = sum(m.data_integrity for m in recent_metrics) / len(recent_metrics)
        avg_success_rate = sum(m.success_rate for m in recent_metrics) / len(recent_metrics)
        avg_processing_time = sum(m.processing_time for m in recent_metrics) / len(recent_metrics)
        
        # ç»Ÿè®¡æœªè§£å†³å‘Šè­¦
        recent_alerts = [a for a in self.alerts 
                        if not a.resolved and 
                        datetime.fromisoformat(a.timestamp) > datetime.now() - timedelta(hours=1)]
        
        status = "healthy"
        if any(a.severity in ["CRITICAL", "HIGH"] for a in recent_alerts):
            status = "critical"
        elif any(a.severity == "MEDIUM" for a in recent_alerts):
            status = "warning"
        
        return {
            "status": status,
            "monitoring": self.is_monitoring,
            "last_update": recent_metrics[-1].timestamp,
            "metrics": {
                "data_integrity": avg_data_integrity,
                "success_rate": avg_success_rate,
                "avg_processing_time": avg_processing_time,
                "sample_size": len(recent_metrics)
            },
            "alerts": {
                "total": len(recent_alerts),
                "critical": len([a for a in recent_alerts if a.severity == "CRITICAL"]),
                "high": len([a for a in recent_alerts if a.severity == "HIGH"]),
                "medium": len([a for a in recent_alerts if a.severity == "MEDIUM"]),
                "low": len([a for a in recent_alerts if a.severity == "LOW"])
            }
        }
    
    def generate_monitoring_report(self, output_file: str = None) -> Dict[str, Any]:
        """ç”Ÿæˆç›‘æ§æŠ¥å‘Š"""
        
        report = {
            "timestamp": datetime.now().isoformat(),
            "monitoring_period": {
                "start": self.metrics_history[0].timestamp if self.metrics_history else None,
                "end": self.metrics_history[-1].timestamp if self.metrics_history else None,
                "duration_hours": 24 if self.metrics_history else 0
            },
            "system_status": self.get_current_status(),
            "metrics_summary": self._generate_metrics_summary(),
            "alert_summary": self._generate_alert_summary(),
            "recommendations": self._generate_recommendations()
        }
        
        if output_file:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=2, ensure_ascii=False)
            print(f"ğŸ“Š ç›‘æ§æŠ¥å‘Šå·²ä¿å­˜: {output_file}")
        
        return report
    
    def _generate_metrics_summary(self) -> Dict[str, Any]:
        """ç”ŸæˆæŒ‡æ ‡æ‘˜è¦"""
        
        if not self.metrics_history:
            return {"no_data": True}
        
        # æŒ‰é˜¶æ®µåˆ†ç»„
        stage_metrics = {}
        for metric in self.metrics_history:
            if metric.stage not in stage_metrics:
                stage_metrics[metric.stage] = []
            stage_metrics[metric.stage].append(metric)
        
        summary = {}
        for stage, metrics in stage_metrics.items():
            summary[stage] = {
                "total_operations": len(metrics),
                "avg_data_integrity": sum(m.data_integrity for m in metrics) / len(metrics),
                "avg_processing_time": sum(m.processing_time for m in metrics) / len(metrics),
                "success_rate": sum(m.success_rate for m in metrics) / len(metrics),
                "total_errors": sum(len(m.errors) for m in metrics)
            }
        
        return summary
    
    def _generate_alert_summary(self) -> Dict[str, Any]:
        """ç”Ÿæˆå‘Šè­¦æ‘˜è¦"""
        
        if not self.alerts:
            return {"total_alerts": 0}
        
        # æŒ‰ç±»å‹å’Œä¸¥é‡ç¨‹åº¦ç»Ÿè®¡
        alert_types = {}
        severity_counts = {"LOW": 0, "MEDIUM": 0, "HIGH": 0, "CRITICAL": 0}
        
        for alert in self.alerts:
            if alert.alert_type not in alert_types:
                alert_types[alert.alert_type] = 0
            alert_types[alert.alert_type] += 1
            severity_counts[alert.severity] += 1
        
        return {
            "total_alerts": len(self.alerts),
            "by_type": alert_types,
            "by_severity": severity_counts,
            "unresolved": len([a for a in self.alerts if not a.resolved])
        }
    
    def _generate_recommendations(self) -> List[Dict[str, str]]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        
        recommendations = []
        
        # åŸºäºå‘Šè­¦å†å²ç”Ÿæˆå»ºè®®
        recent_alerts = [a for a in self.alerts 
                        if datetime.fromisoformat(a.timestamp) > datetime.now() - timedelta(hours=24)]
        
        if any(a.alert_type == "data_integrity_low" for a in recent_alerts):
            recommendations.append({
                "priority": "HIGH",
                "category": "æ•°æ®å®Œæ•´æ€§",
                "action": "æ£€æŸ¥æ˜ å°„ç®—æ³•é…ç½®",
                "description": "æ•°æ®å®Œæ•´æ€§å‘Šè­¦é¢‘å‘ï¼Œå»ºè®®æ£€æŸ¥åˆ—æ˜ å°„è¯å…¸å’Œå¼ºåº¦è®¡ç®—é…ç½®"
            })
        
        if any(a.alert_type == "processing_time_high" for a in recent_alerts):
            recommendations.append({
                "priority": "MEDIUM",
                "category": "æ€§èƒ½ä¼˜åŒ–",
                "action": "ä¼˜åŒ–å¤„ç†ç®—æ³•",
                "description": "å¤„ç†æ—¶é—´è¿‡é•¿ï¼Œå»ºè®®ä¼˜åŒ–é«˜æ–¯å¹³æ»‘æˆ–çŸ©é˜µè®¡ç®—ç®—æ³•"
            })
        
        if any(a.alert_type == "column_coverage_low" for a in recent_alerts):
            recommendations.append({
                "priority": "HIGH",
                "category": "æ˜ å°„ç®—æ³•",
                "action": "æ‰©å±•è¯­ä¹‰è¯å…¸",
                "description": "åˆ—æ˜ å°„è¦†ç›–ç‡ä½ï¼Œå»ºè®®æ‰©å±•æ™ºèƒ½æ˜ å°„ç®—æ³•çš„å…³é”®è¯åº“"
            })
        
        return recommendations

def test_monitoring_system():
    """æµ‹è¯•ç›‘æ§ç³»ç»Ÿ"""
    
    print("ğŸ§ª æµ‹è¯•å®æ—¶æ•°æ®æµç›‘æ§ç³»ç»Ÿ")
    print("=" * 50)
    
    # åˆ›å»ºç›‘æ§å™¨
    monitor = DataFlowMonitor()
    
    # æ·»åŠ å‘Šè­¦å›è°ƒ
    def alert_handler(alert: MonitoringAlert):
        print(f"ğŸ“¢ å‘Šè­¦å¤„ç†å™¨æ”¶åˆ°: {alert.alert_type} - {alert.message}")
    
    monitor.add_alert_callback(alert_handler)
    
    # å¯åŠ¨ç›‘æ§
    monitor.start_monitoring()
    
    # æ¨¡æ‹Ÿä¸€äº›æŒ‡æ ‡è®°å½•
    print("\nğŸ“Š æ¨¡æ‹Ÿæ•°æ®å¤„ç†æµç¨‹...")
    
    # æ­£å¸¸å¤„ç†
    monitor.record_stage_metrics(
        stage="csv_comparison",
        input_count=5,
        output_count=5,
        processing_time=2.5,
        success=True
    )
    
    # æ˜ å°„é˜¶æ®µ - è§¦å‘è¦†ç›–ç‡å‘Šè­¦
    monitor.record_stage_metrics(
        stage="intelligent_mapping", 
        input_count=5,
        output_count=3,
        processing_time=8.0,
        success=True,
        additional_data={
            "column_coverage": 0.6,  # ä½äº0.8é˜ˆå€¼
            "unmapped_columns": ["å·¥èµ„", "éƒ¨é—¨"]
        }
    )
    
    # çƒ­åŠ›å›¾ç”Ÿæˆ - è§¦å‘çƒ­ç‚¹ä¸è¶³å‘Šè­¦
    monitor.record_stage_metrics(
        stage="heatmap_generation",
        input_count=3,
        output_count=1,
        processing_time=15.0,
        success=True,
        additional_data={
            "visible_hotspots": 0  # ä½äº1çš„é˜ˆå€¼
        }
    )
    
    # ç­‰å¾…ç›‘æ§å¤„ç†
    time.sleep(2)
    
    # è·å–çŠ¶æ€
    status = monitor.get_current_status()
    print(f"\nğŸ¯ ç³»ç»ŸçŠ¶æ€: {status['status']}")
    print(f"æ•°æ®å®Œæ•´æ€§: {status['metrics']['data_integrity']:.1%}")
    print(f"æˆåŠŸç‡: {status['metrics']['success_rate']:.1%}")
    print(f"æœªè§£å†³å‘Šè­¦: {status['alerts']['total']}ä¸ª")
    
    # ç”ŸæˆæŠ¥å‘Š
    report = monitor.generate_monitoring_report(
        "/root/projects/tencent-doc-manager/monitoring_report.json"
    )
    
    # åœæ­¢ç›‘æ§
    monitor.stop_monitoring()
    
    print("\nğŸ‰ ç›‘æ§ç³»ç»Ÿæµ‹è¯•å®Œæˆ")
    return monitor

if __name__ == "__main__":
    test_monitoring_system()