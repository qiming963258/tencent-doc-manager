#!/usr/bin/env python3
"""
ä¸‹è½½å†…å®¹æ£€æŸ¥å™¨ï¼ˆè½»é‡ç‰ˆï¼‰
ä¸ä¾èµ–pandasï¼Œä½¿ç”¨åŸç”ŸPythonå®ç°
"""

import os
import json
import csv
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any
import hashlib
import re
from collections import Counter

class DownloadContentChecker:
    """ä¸‹è½½å†…å®¹æ£€æŸ¥å™¨ - éªŒè¯å’Œåˆ†æä¸‹è½½çš„æ–‡æ¡£"""
    
    # æ¼”ç¤ºæ•°æ®ç‰¹å¾ï¼ˆç”¨äºè¯†åˆ«å‡æ•°æ®ï¼‰
    DEMO_INDICATORS = [
        "å¼ ä¸‰", "æå››", "ç‹äº”", "èµµå…­",  # å¸¸è§æµ‹è¯•å§“å
        "test", "demo", "example", "sample",  # æµ‹è¯•æ ‡è¯†
        "æµ‹è¯•", "ç¤ºä¾‹", "æ¼”ç¤º",  # ä¸­æ–‡æµ‹è¯•æ ‡è¯†
        "test_cookie", "demo_purposes"  # æµ‹è¯•Cookie
    ]
    
    # çœŸå®æ–‡æ¡£ç‰¹å¾
    REAL_INDICATORS = [
        "å®é™…", "çœŸå®", "æ­£å¼", "ç”Ÿäº§",
        "è®¡åˆ’", "æŠ¥è¡¨", "ç»Ÿè®¡", "åˆ†æ",
        "é”€å”®", "è´¢åŠ¡", "é¡¹ç›®", "äº§å“"
    ]
    
    def __init__(self):
        """åˆå§‹åŒ–æ£€æŸ¥å™¨"""
        self.check_results = []
        self.stats = {}
        
    def check_file(self, file_path: str) -> Dict[str, Any]:
        """
        æ£€æŸ¥å•ä¸ªæ–‡ä»¶
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            
        Returns:
            æ£€æŸ¥ç»“æœå­—å…¸
        """
        file_path = Path(file_path)
        
        if not file_path.exists():
            return {
                'success': False,
                'error': f'æ–‡ä»¶ä¸å­˜åœ¨: {file_path}',
                'file_path': str(file_path)
            }
        
        # åŸºç¡€ä¿¡æ¯
        result = {
            'file_path': str(file_path),
            'file_name': file_path.name,
            'file_size': file_path.stat().st_size,
            'file_size_readable': self._format_size(file_path.stat().st_size),
            'modified_time': datetime.fromtimestamp(file_path.stat().st_mtime).isoformat(),
            'file_hash': self._calculate_hash(file_path),
            'file_type': file_path.suffix.lower()
        }
        
        # æ ¹æ®æ–‡ä»¶ç±»å‹è¿›è¡Œå†…å®¹æ£€æŸ¥
        if file_path.suffix.lower() == '.csv':
            result.update(self._check_csv_content_lite(file_path))
        elif file_path.suffix.lower() == '.json':
            result.update(self._check_json_content(file_path))
        else:
            result.update(self._check_text_content(file_path))
        
        # çœŸå®æ€§è¯„åˆ†
        result['authenticity_score'] = self._calculate_authenticity_score(result)
        result['is_demo_data'] = result['authenticity_score'] < 50
        
        # ç”Ÿæˆæ€»ç»“
        result['summary'] = self._generate_summary(result)
        
        return result
    
    def _check_csv_content_lite(self, file_path: Path) -> Dict[str, Any]:
        """æ£€æŸ¥CSVæ–‡ä»¶å†…å®¹ï¼ˆè½»é‡ç‰ˆï¼Œä¸ä¾èµ–pandasï¼‰"""
        try:
            with open(file_path, 'r', encoding='utf-8-sig') as f:
                reader = csv.reader(f)
                rows = []
                for i, row in enumerate(reader):
                    rows.append(row)
                    if i >= 1000:  # æœ€å¤šè¯»å–1000è¡Œ
                        break
            
            if not rows:
                return {
                    'content_type': 'csv',
                    'error': 'CSVæ–‡ä»¶ä¸ºç©º'
                }
            
            # å‡è®¾ç¬¬ä¸€è¡Œæ˜¯è¡¨å¤´
            headers = rows[0] if rows else []
            data_rows = rows[1:] if len(rows) > 1 else []
            
            content_info = {
                'content_type': 'csv',
                'row_count': len(data_rows),
                'column_count': len(headers),
                'columns': headers[:20],  # æœ€å¤šæ˜¾ç¤º20åˆ—
                'data_preview': data_rows[:5],  # å‰5è¡Œæ•°æ®
                'empty_cells': 0,
                'duplicate_rows': 0
            }
            
            # ç»Ÿè®¡ç©ºå•å…ƒæ ¼
            for row in data_rows:
                for cell in row:
                    if not cell or cell.strip() == '':
                        content_info['empty_cells'] += 1
            
            # æ£€æŸ¥é‡å¤è¡Œ
            seen_rows = set()
            for row in data_rows:
                row_str = '|'.join(row)
                if row_str in seen_rows:
                    content_info['duplicate_rows'] += 1
                seen_rows.add(row_str)
            
            # å†…å®¹åˆ†æ
            all_text = '\n'.join([','.join(row) for row in rows])
            content_info.update(self._analyze_content(all_text))
            
            # æ•°æ®è´¨é‡æ£€æŸ¥
            total_cells = len(data_rows) * len(headers) if headers else 1
            content_info['data_quality'] = {
                'completeness': (1 - content_info['empty_cells'] / max(total_cells, 1)) * 100,
                'uniqueness': (1 - content_info['duplicate_rows'] / max(len(data_rows), 1)) * 100,
                'has_headers': self._check_has_headers_lite(headers, data_rows)
            }
            
            return content_info
            
        except Exception as e:
            return {
                'content_type': 'csv',
                'error': f'CSVè¯»å–é”™è¯¯: {str(e)}'
            }
    
    def _check_json_content(self, file_path: Path) -> Dict[str, Any]:
        """æ£€æŸ¥JSONæ–‡ä»¶å†…å®¹"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            content_info = {
                'content_type': 'json',
                'structure': self._analyze_json_structure(data),
                'key_count': len(data) if isinstance(data, dict) else None,
                'item_count': len(data) if isinstance(data, list) else None
            }
            
            # å†…å®¹åˆ†æ
            content_text = json.dumps(data, ensure_ascii=False)
            content_info.update(self._analyze_content(content_text))
            
            return content_info
            
        except Exception as e:
            return {
                'content_type': 'json',
                'error': f'JSONè¯»å–é”™è¯¯: {str(e)}'
            }
    
    def _check_text_content(self, file_path: Path) -> Dict[str, Any]:
        """æ£€æŸ¥æ–‡æœ¬æ–‡ä»¶å†…å®¹"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read(10000)  # è¯»å–å‰10000å­—ç¬¦
            
            content_info = {
                'content_type': 'text',
                'character_count': len(content),
                'line_count': content.count('\n') + 1
            }
            
            # å†…å®¹åˆ†æ
            content_info.update(self._analyze_content(content))
            
            return content_info
            
        except Exception as e:
            return {
                'content_type': 'text',
                'error': f'æ–‡æœ¬è¯»å–é”™è¯¯: {str(e)}'
            }
    
    def _analyze_content(self, content: str) -> Dict[str, Any]:
        """åˆ†ææ–‡æœ¬å†…å®¹"""
        content_lower = content.lower()
        
        # æ£€æŸ¥æ¼”ç¤ºæ•°æ®æ ‡è¯†
        demo_count = sum(1 for indicator in self.DEMO_INDICATORS 
                        if indicator.lower() in content_lower)
        
        # æ£€æŸ¥çœŸå®æ•°æ®æ ‡è¯†
        real_count = sum(1 for indicator in self.REAL_INDICATORS 
                        if indicator in content)
        
        # æå–æ•°å­—
        numbers = re.findall(r'\d+\.?\d*', content)
        
        # æå–æ—¥æœŸ
        dates = re.findall(r'\d{4}[-/]\d{1,2}[-/]\d{1,2}', content)
        
        # è¯é¢‘ç»Ÿè®¡ï¼ˆä¸­æ–‡ï¼‰
        chinese_words = re.findall(r'[\u4e00-\u9fa5]+', content)
        word_freq = Counter(chinese_words).most_common(10)
        
        return {
            'demo_indicators_found': demo_count,
            'real_indicators_found': real_count,
            'contains_numbers': len(numbers) > 0,
            'number_count': len(numbers),
            'contains_dates': len(dates) > 0,
            'date_count': len(dates),
            'top_words': word_freq,
            'has_chinese': len(chinese_words) > 0,
            'chinese_ratio': len(''.join(chinese_words)) / max(len(content), 1)
        }
    
    def _analyze_json_structure(self, data: Any, max_depth: int = 3, current_depth: int = 0) -> str:
        """åˆ†æJSONç»“æ„"""
        if current_depth >= max_depth:
            return "..."
        
        if isinstance(data, dict):
            if not data:
                return "{}"
            keys = list(data.keys())[:5]  # æœ€å¤šæ˜¾ç¤º5ä¸ªé”®
            return "{" + ", ".join(f'"{k}": {self._analyze_json_structure(data[k], max_depth, current_depth+1)}' 
                                  for k in keys) + ("..." if len(data) > 5 else "") + "}"
        elif isinstance(data, list):
            if not data:
                return "[]"
            return f"[{self._analyze_json_structure(data[0], max_depth, current_depth+1)}...] (len={len(data)})"
        elif isinstance(data, str):
            return f'"{data[:20]}..."' if len(data) > 20 else f'"{data}"'
        else:
            return str(type(data).__name__)
    
    def _check_has_headers_lite(self, headers: List[str], data_rows: List[List[str]]) -> bool:
        """æ£€æŸ¥CSVæ˜¯å¦æœ‰åˆç†çš„è¡¨å¤´ï¼ˆè½»é‡ç‰ˆï¼‰"""
        if not headers:
            return False
        
        # å¦‚æœæ‰€æœ‰è¡¨å¤´éƒ½æ˜¯æ•°å­—ï¼Œå¯èƒ½æ²¡æœ‰çœŸæ­£çš„è¡¨å¤´
        if all(h.isdigit() for h in headers):
            return False
        
        # å¦‚æœè¡¨å¤´åŒ…å«å¸¸è§çš„åˆ—åï¼Œè®¤ä¸ºæœ‰è¡¨å¤´
        common_headers = ['å§“å', 'åç§°', 'name', 'id', 'æ—¥æœŸ', 'date', 'é‡‘é¢', 'amount', 'çŠ¶æ€', 'status']
        for header in headers:
            if any(common in header.lower() for common in common_headers):
                return True
        
        return True
    
    def _calculate_hash(self, file_path: Path) -> str:
        """è®¡ç®—æ–‡ä»¶å“ˆå¸Œå€¼"""
        hash_md5 = hashlib.md5()
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_md5.update(chunk)
        return hash_md5.hexdigest()
    
    def _format_size(self, size: int) -> str:
        """æ ¼å¼åŒ–æ–‡ä»¶å¤§å°"""
        for unit in ['B', 'KB', 'MB', 'GB']:
            if size < 1024.0:
                return f"{size:.2f} {unit}"
            size /= 1024.0
        return f"{size:.2f} TB"
    
    def _calculate_authenticity_score(self, result: Dict[str, Any]) -> float:
        """è®¡ç®—çœŸå®æ€§è¯„åˆ†ï¼ˆ0-100ï¼‰"""
        score = 50.0  # åŸºç¡€åˆ†
        
        # æ–‡ä»¶å¤§å°è¯„åˆ†
        if result['file_size'] > 1024:  # å¤§äº1KB
            score += 10
        if result['file_size'] > 10240:  # å¤§äº10KB
            score += 10
        
        # å†…å®¹è¯„åˆ†
        if 'demo_indicators_found' in result:
            score -= result['demo_indicators_found'] * 10
        if 'real_indicators_found' in result:
            score += result['real_indicators_found'] * 5
        
        # CSVç‰¹å®šè¯„åˆ†
        if 'row_count' in result:
            if result['row_count'] > 10:
                score += 10
            if result['row_count'] > 100:
                score += 10
        
        if 'column_count' in result:
            if result['column_count'] > 3:
                score += 5
        
        # æ•°æ®è´¨é‡è¯„åˆ†
        if 'data_quality' in result:
            score += result['data_quality']['completeness'] * 0.2
            score += result['data_quality']['uniqueness'] * 0.1
        
        # ç¡®ä¿åˆ†æ•°åœ¨0-100ä¹‹é—´
        return max(0, min(100, score))
    
    def _generate_summary(self, result: Dict[str, Any]) -> str:
        """ç”Ÿæˆæ£€æŸ¥æ€»ç»“"""
        summary_parts = []
        
        # æ–‡ä»¶åŸºæœ¬ä¿¡æ¯
        summary_parts.append(f"æ–‡ä»¶: {result['file_name']} ({result['file_size_readable']})")
        
        # çœŸå®æ€§åˆ¤æ–­
        if result['is_demo_data']:
            summary_parts.append("âš ï¸ ç–‘ä¼¼æ¼”ç¤ºæ•°æ®")
        else:
            summary_parts.append("âœ… å¯èƒ½æ˜¯çœŸå®æ•°æ®")
        
        summary_parts.append(f"çœŸå®æ€§è¯„åˆ†: {result['authenticity_score']:.1f}/100")
        
        # å†…å®¹ç‰¹å¾
        if 'row_count' in result:
            summary_parts.append(f"æ•°æ®è§„æ¨¡: {result['row_count']}è¡Œ Ã— {result['column_count']}åˆ—")
        
        if 'demo_indicators_found' in result and result['demo_indicators_found'] > 0:
            summary_parts.append(f"å‘ç°{result['demo_indicators_found']}ä¸ªæ¼”ç¤ºæ•°æ®æ ‡è¯†")
        
        if 'real_indicators_found' in result and result['real_indicators_found'] > 0:
            summary_parts.append(f"å‘ç°{result['real_indicators_found']}ä¸ªçœŸå®æ•°æ®æ ‡è¯†")
        
        return " | ".join(summary_parts)


# ä¾¿æ·å‡½æ•°
def quick_check(file_path: str) -> None:
    """å¿«é€Ÿæ£€æŸ¥å•ä¸ªæ–‡ä»¶å¹¶æ‰“å°ç»“æœ"""
    checker = DownloadContentChecker()
    result = checker.check_file(file_path)
    
    print("=" * 60)
    print("ğŸ“Š ä¸‹è½½å†…å®¹æ£€æŸ¥æŠ¥å‘Š")
    print("=" * 60)
    print(f"æ–‡ä»¶: {result['file_name']}")
    print(f"å¤§å°: {result['file_size_readable']}")
    print(f"ç±»å‹: {result.get('content_type', 'unknown')}")
    print("-" * 60)
    
    if result['is_demo_data']:
        print("âš ï¸ æ£€æµ‹ç»“æœ: ç–‘ä¼¼æ¼”ç¤ºæ•°æ®")
    else:
        print("âœ… æ£€æµ‹ç»“æœ: å¯èƒ½æ˜¯çœŸå®æ•°æ®")
    
    print(f"çœŸå®æ€§è¯„åˆ†: {result['authenticity_score']:.1f}/100")
    
    if 'row_count' in result:
        print(f"æ•°æ®è§„æ¨¡: {result['row_count']}è¡Œ Ã— {result['column_count']}åˆ—")
    
    if 'demo_indicators_found' in result:
        print(f"æ¼”ç¤ºæ ‡è¯†: {result['demo_indicators_found']}ä¸ª")
        
    if 'real_indicators_found' in result:
        print(f"çœŸå®æ ‡è¯†: {result['real_indicators_found']}ä¸ª")
    
    print("-" * 60)
    print("æ€»ç»“:", result['summary'])
    print("=" * 60)


if __name__ == "__main__":
    # æµ‹è¯•ç¤ºä¾‹
    import sys
    
    if len(sys.argv) > 1:
        file_path = sys.argv[1]
        quick_check(file_path)
    else:
        print("ä½¿ç”¨æ–¹æ³•: python download_content_checker_lite.py <æ–‡ä»¶è·¯å¾„>")
        print("ç¤ºä¾‹: python download_content_checker_lite.py /root/projects/tencent-doc-manager/downloads/document.csv")