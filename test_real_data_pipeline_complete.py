#!/usr/bin/env python3
"""
å®Œæ•´æ•°æ®é“¾è·¯çœŸå®æ€§éªŒè¯æµ‹è¯•
æµ‹è¯•ä»CSVå¯¹æ¯”åˆ°ç»¼åˆæ‰“åˆ†çš„æ•´ä¸ªæ•°æ®æµç¨‹æ˜¯å¦ä½¿ç”¨çœŸå®æ•°æ®
"""

import json
import os
import sys

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

def test_real_data_pipeline():
    """æµ‹è¯•å®Œæ•´æ•°æ®é“¾è·¯"""
    print("=" * 80)
    print("ğŸ” æ•°æ®é“¾è·¯çœŸå®æ€§éªŒè¯æµ‹è¯•")
    print("=" * 80)

    # æµ‹è¯•ç»“æœæ”¶é›†
    test_results = {
        "passed": [],
        "failed": [],
        "warnings": []
    }

    # 1. æ£€æŸ¥CSVå¯¹æ¯”ç»“æœæ–‡ä»¶
    print("\n1ï¸âƒ£ æ£€æŸ¥CSVå¯¹æ¯”ç»“æœæ–‡ä»¶...")
    comparison_file = "/root/projects/tencent-doc-manager/csv_security_results/test_comparison_comparison.json"
    if os.path.exists(comparison_file):
        with open(comparison_file, 'r', encoding='utf-8') as f:
            comparison_data = json.load(f)

        if 'differences' in comparison_data:
            print(f"   âœ… CSVå¯¹æ¯”æ–‡ä»¶å­˜åœ¨ï¼ŒåŒ…å«{len(comparison_data['differences'])}ä¸ªå·®å¼‚")
            test_results["passed"].append("CSVå¯¹æ¯”æ–‡ä»¶å­˜åœ¨ä¸”æœ‰çœŸå®æ•°æ®")

            # æ˜¾ç¤ºéƒ¨åˆ†å·®å¼‚ç¤ºä¾‹
            print("\n   ğŸ“‹ å·®å¼‚ç¤ºä¾‹:")
            for i, diff in enumerate(comparison_data['differences'][:3]):
                print(f"      - è¡Œ{diff['è¡Œå·']} {diff['åˆ—å']}: '{diff['åŸå€¼']}' â†’ '{diff['æ–°å€¼']}' (é£é™©ç­‰çº§: {diff['risk_level']})")
        else:
            print("   âŒ CSVå¯¹æ¯”æ–‡ä»¶æ ¼å¼é”™è¯¯")
            test_results["failed"].append("CSVå¯¹æ¯”æ–‡ä»¶æ ¼å¼é”™è¯¯")
    else:
        print("   âŒ CSVå¯¹æ¯”æ–‡ä»¶ä¸å­˜åœ¨")
        test_results["failed"].append("CSVå¯¹æ¯”æ–‡ä»¶ä¸å­˜åœ¨")

    # 2. æµ‹è¯•comparison_to_scoring_adapter
    print("\n2ï¸âƒ£ æµ‹è¯•comparison_to_scoring_adapter...")
    try:
        from production.core_modules.comparison_to_scoring_adapter import ComparisonToScoringAdapter

        adapter = ComparisonToScoringAdapter()

        # ä½¿ç”¨çœŸå®çš„CSVå¯¹æ¯”æ•°æ®
        if os.path.exists(comparison_file):
            with open(comparison_file, 'r', encoding='utf-8') as f:
                comparison_data = json.load(f)

            # æå–è¡¨æ ¼æ•°æ® - ä¼ å…¥å•ä¸ªå¯¹æ¯”ç»“æœè€Œä¸æ˜¯åˆ—è¡¨
            table_data = adapter.extract_table_data(comparison_data)
            table_data_list = [table_data] if table_data else []

            if table_data_list:
                print(f"   âœ… AdapteræˆåŠŸæå–{len(table_data_list)}ä¸ªè¡¨æ ¼æ•°æ®")
                test_results["passed"].append("Adapterèƒ½å¤Ÿæå–çœŸå®æ•°æ®")

                # æ£€æŸ¥L1/L2/L3åˆ†ç±»
                # column_modificationsæ˜¯è¡Œå·åˆ—è¡¨ï¼Œéœ€è¦è®¡ç®—æ‰“åˆ†
                for table_data in table_data_list:
                    # ç”Ÿæˆæ‰“åˆ†æ•°æ®
                    for col_name, modified_rows in table_data['column_modifications'].items():
                        # è·å–åˆ—çº§åˆ«
                        level = adapter._get_column_level(col_name)

                        # è®¡ç®—åˆ†æ•°
                        modification_count = len(modified_rows)
                        total_rows = table_data.get('total_rows', 270)
                        score = adapter._calculate_column_score(modification_count, total_rows, col_name)

                        # éªŒè¯å¼ºåˆ¶é˜ˆå€¼ï¼ˆåªæœ‰å½“æœ‰ä¿®æ”¹æ—¶æ‰éªŒè¯ï¼‰
                        if modification_count > 0:  # åªæ£€æŸ¥æœ‰ä¿®æ”¹çš„åˆ—
                            if level == 'L1' and score < 0.8:
                                test_results["failed"].append(f"L1åˆ—{col_name}æœ‰{modification_count}ä¸ªä¿®æ”¹ä½†åˆ†æ•°{score}ä½äº0.8")
                            elif level == 'L2' and score < 0.6:
                                test_results["failed"].append(f"L2åˆ—{col_name}æœ‰{modification_count}ä¸ªä¿®æ”¹ä½†åˆ†æ•°{score}ä½äº0.6")
                        else:
                            # æ²¡æœ‰ä¿®æ”¹çš„åˆ—åº”è¯¥æ˜¯0.05ï¼ˆèƒŒæ™¯å€¼ï¼‰
                            if score != 0.05:
                                test_results["warnings"].append(f"åˆ—{col_name}æ— ä¿®æ”¹ä½†åˆ†æ•°ä¸º{score}ï¼ˆåº”è¯¥ä¸º0.05ï¼‰")

                print("   âœ… L1/L2/L3åˆ—åˆ†ç±»å’Œå¼ºåˆ¶é˜ˆå€¼éªŒè¯é€šè¿‡")
                test_results["passed"].append("åˆ—åˆ†ç±»å’Œå¼ºåˆ¶é˜ˆå€¼æ­£ç¡®")
            else:
                print("   âŒ Adapteræ— æ³•æå–æ•°æ®")
                test_results["failed"].append("Adapteræ— æ³•æå–æ•°æ®")

    except Exception as e:
        print(f"   âŒ Adapteræµ‹è¯•å¤±è´¥: {e}")
        test_results["failed"].append(f"Adapteræµ‹è¯•å¤±è´¥: {e}")

    # 3. æµ‹è¯•comprehensive_score_generator_v2
    print("\n3ï¸âƒ£ æµ‹è¯•comprehensive_score_generator_v2...")
    try:
        from production.core_modules.comprehensive_score_generator_v2 import ComprehensiveScoreGeneratorV2

        generator = ComprehensiveScoreGeneratorV2()

        # ç”Ÿæˆç»¼åˆæ‰“åˆ†æ–‡ä»¶
        result = generator.generate(
            week_number="38",
            comparison_files=[comparison_file] if os.path.exists(comparison_file) else []
        )

        if result and os.path.exists(result):
            print(f"   âœ… ç”Ÿæˆç»¼åˆæ‰“åˆ†æ–‡ä»¶: {result}")
            test_results["passed"].append("ç»¼åˆæ‰“åˆ†æ–‡ä»¶ç”ŸæˆæˆåŠŸ")

            # è¯»å–å¹¶éªŒè¯å†…å®¹
            with open(result, 'r', encoding='utf-8') as f:
                comprehensive_data = json.load(f)

            # æ£€æŸ¥æ•°æ®æºæ ‡è®°ï¼ˆåœ¨summaryä¸­ï¼‰
            data_source = comprehensive_data.get('summary', {}).get('data_source')
            if data_source == 'real_csv_comparison':
                print("   âœ… æ•°æ®æºæ ‡è®°ä¸º'real_csv_comparison'")
                test_results["passed"].append("æ•°æ®æºæ ‡è®°æ­£ç¡®")
            else:
                print(f"   âš ï¸  æ•°æ®æºæ ‡è®°ä¸º'{data_source}'")
                test_results["warnings"].append("æ•°æ®æºæ ‡è®°ä¸æ­£ç¡®")

            # æ£€æŸ¥æ˜¯å¦æœ‰éšæœºæ•°æ®ç‰¹å¾
            import re
            content_str = json.dumps(comprehensive_data)

            # æ£€æŸ¥æ˜¯å¦æœ‰æ˜æ˜¾çš„éšæœºæ¨¡å¼ï¼ˆå¦‚è¿ç»­çš„0.7x, 0.8xç­‰ï¼‰
            random_pattern = re.findall(r'0\.\d{2,}', content_str)
            if len(set(random_pattern)) > 20:  # å¦‚æœæœ‰å¤ªå¤šä¸åŒçš„å°æ•°ï¼Œå¯èƒ½æ˜¯éšæœºçš„
                test_results["warnings"].append("å‘ç°å¤§é‡ä¸åŒçš„å°æ•°å€¼ï¼Œå¯èƒ½åŒ…å«éšæœºæ•°æ®")

        else:
            print("   âŒ æ— æ³•ç”Ÿæˆç»¼åˆæ‰“åˆ†æ–‡ä»¶")
            test_results["failed"].append("æ— æ³•ç”Ÿæˆç»¼åˆæ‰“åˆ†æ–‡ä»¶")

    except Exception as e:
        print(f"   âŒ ç»¼åˆæ‰“åˆ†ç”Ÿæˆæµ‹è¯•å¤±è´¥: {e}")
        test_results["failed"].append(f"ç»¼åˆæ‰“åˆ†ç”Ÿæˆæµ‹è¯•å¤±è´¥: {e}")

    # 4. æ£€æŸ¥æ˜¯å¦è¿˜æœ‰DetailedScoreGeneratorçš„ä½¿ç”¨
    print("\n4ï¸âƒ£ æ£€æŸ¥DetailedScoreGeneratorä½¿ç”¨æƒ…å†µ...")

    # æ£€æŸ¥production/scoring_engine/comprehensive_score_generator_v2.py
    scorer_file = "/root/projects/tencent-doc-manager/production/scoring_engine/comprehensive_score_generator_v2.py"
    if os.path.exists(scorer_file):
        with open(scorer_file, 'r', encoding='utf-8') as f:
            content = f.read()

        if "from integrated_scorer import IntegratedScorer" in content:
            print("   âœ… scoring_engineä½¿ç”¨IntegratedScorer")
            test_results["passed"].append("scoring_engineä½¿ç”¨æ­£ç¡®çš„æ‰“åˆ†å™¨")
        else:
            print("   âŒ scoring_engineæœªä½¿ç”¨IntegratedScorer")
            test_results["failed"].append("scoring_engineæœªä½¿ç”¨æ­£ç¡®çš„æ‰“åˆ†å™¨")

        if "DetailedScoreGenerator" in content and "from detailed_score_generator" in content:
            print("   âŒ scoring_engineä»åœ¨å¯¼å…¥DetailedScoreGenerator")
            test_results["failed"].append("ä»åœ¨ä½¿ç”¨éšæœºæ•°æ®ç”Ÿæˆå™¨")

    # 5. æ£€æŸ¥IntegratedScoreræ˜¯å¦å­˜åœ¨
    print("\n5ï¸âƒ£ æ£€æŸ¥IntegratedScorerå®ç°...")
    integrated_scorer_file = "/root/projects/tencent-doc-manager/production/scoring_engine/integrated_scorer.py"
    if os.path.exists(integrated_scorer_file):
        with open(integrated_scorer_file, 'r', encoding='utf-8') as f:
            content = f.read()

        if "import random" not in content:
            print("   âœ… IntegratedScorerä¸ä½¿ç”¨randomæ¨¡å—")
            test_results["passed"].append("IntegratedScoreræ— éšæœºæ•°æ®")
        else:
            print("   âŒ IntegratedScoreråŒ…å«randomå¯¼å…¥")
            test_results["failed"].append("IntegratedScorerå¯èƒ½ä½¿ç”¨éšæœºæ•°æ®")

    # æ€»ç»“æµ‹è¯•ç»“æœ
    print("\n" + "=" * 80)
    print("ğŸ“Š æµ‹è¯•ç»“æœæ€»ç»“")
    print("=" * 80)

    print(f"\nâœ… é€šè¿‡æµ‹è¯•: {len(test_results['passed'])}é¡¹")
    for item in test_results['passed']:
        print(f"   - {item}")

    if test_results['failed']:
        print(f"\nâŒ å¤±è´¥æµ‹è¯•: {len(test_results['failed'])}é¡¹")
        for item in test_results['failed']:
            print(f"   - {item}")

    if test_results['warnings']:
        print(f"\nâš ï¸  è­¦å‘Š: {len(test_results['warnings'])}é¡¹")
        for item in test_results['warnings']:
            print(f"   - {item}")

    # æœ€ç»ˆåˆ¤å®š
    print("\n" + "=" * 80)
    if not test_results['failed']:
        print("ğŸ‰ æ•°æ®é“¾è·¯éªŒè¯é€šè¿‡ï¼ç³»ç»Ÿä½¿ç”¨çœŸå®æ•°æ®ï¼Œæ— è™šæ‹Ÿæˆåˆ†")
        return True
    else:
        print("âš ï¸  æ•°æ®é“¾è·¯å­˜åœ¨é—®é¢˜ï¼Œè¯·ä¿®å¤å¤±è´¥é¡¹")
        return False

if __name__ == "__main__":
    success = test_real_data_pipeline()
    sys.exit(0 if success else 1)