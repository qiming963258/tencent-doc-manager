#!/usr/bin/env python3
"""
Claudeå°è£…æœåŠ¡å¿«é€Ÿæ€§èƒ½æµ‹è¯•
é’ˆå¯¹å®é™…å“åº”æ—¶é—´ä¼˜åŒ–çš„è½»é‡çº§å‹åŠ›æµ‹è¯•
"""

import asyncio
import aiohttp
import time
import statistics
import json
from datetime import datetime
import psutil

class QuickPerformanceBenchmark:
    """å¿«é€Ÿæ€§èƒ½åŸºå‡†æµ‹è¯•"""
    
    def __init__(self, base_url: str = "http://localhost:8081"):
        self.base_url = base_url
        
    async def test_response_times(self, concurrent_count: int = 3) -> dict:
        """æµ‹è¯•å“åº”æ—¶é—´ï¼ˆè½»é‡çº§ï¼‰"""
        print(f"ğŸ“Š æµ‹è¯• {concurrent_count} å¹¶å‘å“åº”æ—¶é—´...")
        
        async def single_request(session, request_id):
            try:
                start_time = time.time()
                async with session.post(f"{self.base_url}/analyze", json={
                    "content": f"æµ‹è¯•è¯·æ±‚ #{request_id}: è´Ÿè´£äººä»å¼ ä¸‰æ”¹ä¸ºæå››",
                    "analysis_type": "risk_assessment",
                    "context": {"test_id": request_id}
                }) as response:
                    result = await response.json()
                    response_time = time.time() - start_time
                    return {
                        "success": response.status == 200,
                        "response_time": response_time,
                        "request_id": request_id
                    }
            except Exception as e:
                return {
                    "success": False,
                    "error": str(e),
                    "request_id": request_id
                }
        
        # æ‰§è¡Œå¹¶å‘æµ‹è¯•
        async with aiohttp.ClientSession() as session:
            start_time = time.time()
            tasks = [single_request(session, i) for i in range(concurrent_count)]
            results = await asyncio.gather(*tasks)
            total_time = time.time() - start_time
        
        # ç»Ÿè®¡ç»“æœ
        successful = [r for r in results if r["success"]]
        failed = len(results) - len(successful)
        
        if successful:
            response_times = [r["response_time"] for r in successful]
            return {
                "concurrent_level": concurrent_count,
                "total_requests": concurrent_count,
                "successful_requests": len(successful),
                "failed_requests": failed,
                "success_rate": len(successful) / concurrent_count * 100,
                "total_time": total_time,
                "avg_response_time": statistics.mean(response_times),
                "min_response_time": min(response_times),
                "max_response_time": max(response_times),
                "throughput": len(successful) / total_time
            }
        else:
            return {"error": "æ‰€æœ‰è¯·æ±‚å¤±è´¥", "failed_requests": concurrent_count}
    
    async def test_sustained_load(self, duration_seconds: int = 30, request_interval: int = 15) -> dict:
        """æŒç»­è´Ÿè½½æµ‹è¯•ï¼ˆè½»é‡çº§ï¼‰"""
        print(f"â±ï¸ æŒç»­è´Ÿè½½æµ‹è¯• ({duration_seconds}ç§’, æ¯{request_interval}ç§’1ä¸ªè¯·æ±‚)...")
        
        results = []
        start_time = time.time()
        request_id = 0
        
        async with aiohttp.ClientSession() as session:
            while time.time() - start_time < duration_seconds:
                try:
                    req_start = time.time()
                    async with session.post(f"{self.base_url}/analyze", json={
                        "content": f"æŒç»­æµ‹è¯• #{request_id}: è¿›åº¦ä»60%æ›´æ–°ä¸º85%",
                        "analysis_type": "content_analysis",
                        "context": {"sustained_test": True}
                    }) as response:
                        result = await response.json()
                        response_time = time.time() - req_start
                        
                        results.append({
                            "timestamp": time.time(),
                            "success": response.status == 200,
                            "response_time": response_time,
                            "request_id": request_id
                        })
                        
                        request_id += 1
                        
                        # ç­‰å¾…é—´éš”
                        elapsed = time.time() - req_start
                        if elapsed < request_interval:
                            await asyncio.sleep(request_interval - elapsed)
                            
                except Exception as e:
                    results.append({
                        "timestamp": time.time(),
                        "success": False,
                        "error": str(e),
                        "request_id": request_id
                    })
                    request_id += 1
                    await asyncio.sleep(request_interval)
        
        # åˆ†æç»“æœ
        successful = [r for r in results if r["success"]]
        if successful:
            response_times = [r["response_time"] for r in successful]
            return {
                "duration_seconds": duration_seconds,
                "total_requests": len(results),
                "successful_requests": len(successful),
                "failed_requests": len(results) - len(successful),
                "success_rate": len(successful) / len(results) * 100,
                "avg_response_time": statistics.mean(response_times),
                "max_response_time": max(response_times),
                "min_response_time": min(response_times)
            }
        else:
            return {"error": "æ‰€æœ‰æŒç»­è¯·æ±‚å¤±è´¥"}
    
    def get_system_stats(self) -> dict:
        """è·å–ç³»ç»Ÿæ€§èƒ½ç»Ÿè®¡"""
        return {
            "cpu_percent": psutil.cpu_percent(interval=1),
            "memory_percent": psutil.virtual_memory().percent,
            "memory_used_mb": psutil.virtual_memory().used / 1024 / 1024,
            "memory_available_mb": psutil.virtual_memory().available / 1024 / 1024
        }
    
    def calculate_performance_score(self, results: dict) -> dict:
        """è®¡ç®—æ€§èƒ½è¯„åˆ†"""
        score = {"response_score": 0, "stability_score": 0, "overall_score": 0}
        
        # å“åº”æ—¶é—´è¯„åˆ†
        if "response_times" in results and results["response_times"].get("avg_response_time"):
            avg_time = results["response_times"]["avg_response_time"]
            if avg_time < 10:
                score["response_score"] = 100
            elif avg_time < 20:
                score["response_score"] = 80
            elif avg_time < 30:
                score["response_score"] = 60
            else:
                score["response_score"] = 40
        
        # ç¨³å®šæ€§è¯„åˆ†
        if "sustained_load" in results and results["sustained_load"].get("success_rate"):
            success_rate = results["sustained_load"]["success_rate"]
            if success_rate >= 95:
                score["stability_score"] = 100
            elif success_rate >= 90:
                score["stability_score"] = 80
            elif success_rate >= 80:
                score["stability_score"] = 60
            else:
                score["stability_score"] = 40
        
        # ç»¼åˆè¯„åˆ†
        score["overall_score"] = int((score["response_score"] + score["stability_score"]) / 2)
        return score
    
    async def run_quick_benchmark(self) -> dict:
        """è¿è¡Œå¿«é€ŸåŸºå‡†æµ‹è¯•"""
        print("ğŸš€ å¼€å§‹å¿«é€Ÿæ€§èƒ½åŸºå‡†æµ‹è¯•")
        print("=" * 50)
        
        test_results = {
            "timestamp": datetime.now().isoformat(),
            "test_type": "Quick Performance Benchmark",
            "service": "Claude Mini Wrapper"
        }
        
        # 1. åŸºç¡€å“åº”æ—¶é—´æµ‹è¯•
        print("\n1ï¸âƒ£ åŸºç¡€å“åº”æ—¶é—´æµ‹è¯•...")
        test_results["response_times"] = await self.test_response_times(concurrent_count=3)
        if "avg_response_time" in test_results["response_times"]:
            print(f"   å¹³å‡å“åº”æ—¶é—´: {test_results['response_times']['avg_response_time']:.2f}ç§’")
            print(f"   æˆåŠŸç‡: {test_results['response_times']['success_rate']:.1f}%")
        
        # 2. è½»é‡çº§å¹¶å‘æµ‹è¯•
        print("\n2ï¸âƒ£ è½»é‡çº§å¹¶å‘æµ‹è¯•...")
        test_results["concurrent_test"] = await self.test_response_times(concurrent_count=5)
        if "avg_response_time" in test_results["concurrent_test"]:
            print(f"   5å¹¶å‘å¹³å‡å“åº”æ—¶é—´: {test_results['concurrent_test']['avg_response_time']:.2f}ç§’")
            print(f"   ååé‡: {test_results['concurrent_test']['throughput']:.2f} req/s")
        
        # 3. æŒç»­è´Ÿè½½æµ‹è¯•
        print("\n3ï¸âƒ£ æŒç»­è´Ÿè½½æµ‹è¯•...")
        test_results["sustained_load"] = await self.test_sustained_load(duration_seconds=60, request_interval=20)
        if "success_rate" in test_results["sustained_load"]:
            print(f"   æŒç»­æµ‹è¯•æˆåŠŸç‡: {test_results['sustained_load']['success_rate']:.1f}%")
            print(f"   æŒç»­æµ‹è¯•å¹³å‡å“åº”æ—¶é—´: {test_results['sustained_load']['avg_response_time']:.2f}ç§’")
        
        # 4. ç³»ç»Ÿèµ„æºç»Ÿè®¡
        print("\n4ï¸âƒ£ ç³»ç»Ÿèµ„æºç»Ÿè®¡...")
        test_results["system_stats"] = self.get_system_stats()
        print(f"   CPUä½¿ç”¨ç‡: {test_results['system_stats']['cpu_percent']:.1f}%")
        print(f"   å†…å­˜ä½¿ç”¨ç‡: {test_results['system_stats']['memory_percent']:.1f}%")
        print(f"   å¯ç”¨å†…å­˜: {test_results['system_stats']['memory_available_mb']:.0f}MB")
        
        # 5. æ€§èƒ½è¯„åˆ†
        test_results["performance_score"] = self.calculate_performance_score(test_results)
        
        print("\n" + "=" * 50)
        print("ğŸ“Š å¿«é€ŸåŸºå‡†æµ‹è¯•å®Œæˆ!")
        print(f"   å“åº”æ—¶é—´è¯„åˆ†: {test_results['performance_score']['response_score']}/100")
        print(f"   ç¨³å®šæ€§è¯„åˆ†: {test_results['performance_score']['stability_score']}/100")
        print(f"   ç»¼åˆæ€§èƒ½è¯„åˆ†: {test_results['performance_score']['overall_score']}/100")
        
        return test_results

async def main():
    """ä¸»å‡½æ•°"""
    benchmark = QuickPerformanceBenchmark()
    results = await benchmark.run_quick_benchmark()
    
    # ä¿å­˜æµ‹è¯•ç»“æœ
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"quick_benchmark_results_{timestamp}.json"
    
    with open(filename, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2, default=str)
    
    print(f"\nğŸ“„ åŸºå‡†æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {filename}")
    return results

if __name__ == "__main__":
    asyncio.run(main())