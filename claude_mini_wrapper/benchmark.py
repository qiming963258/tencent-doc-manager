import asyncio
import aiohttp
import time
import statistics
from typing import List, Dict, Any
import json
from datetime import datetime
import psutil
import threading

class PerformanceBenchmark:
    """Claudeå°è£…æœåŠ¡æ€§èƒ½åŸºå‡†æµ‹è¯•"""
    
    def __init__(self, base_url: str = "http://localhost:8081"):
        self.base_url = base_url
        self.results = {}
        
    async def benchmark_response_time(self, concurrent_levels: List[int] = [1, 5, 10]) -> Dict[str, Any]:
        """å“åº”æ—¶é—´åŸºå‡†æµ‹è¯•"""
        print("ğŸ“Š è¿è¡Œå“åº”æ—¶é—´åŸºå‡†æµ‹è¯•...")
        
        results = {}
        
        for concurrency in concurrent_levels:
            print(f"   æµ‹è¯•å¹¶å‘çº§åˆ«: {concurrency}")
            
            async with aiohttp.ClientSession() as session:
                # åˆ›å»ºæµ‹è¯•ä»»åŠ¡
                tasks = []
                for i in range(concurrency):
                    task = self._single_analyze_request(session, i)
                    tasks.append(task)
                
                # æ‰§è¡Œå¹¶å‘è¯·æ±‚
                start_time = time.time()
                responses = await asyncio.gather(*tasks, return_exceptions=True)
                total_time = time.time() - start_time
                
                # åˆ†æç»“æœ
                successful_responses = [r for r in responses if not isinstance(r, Exception)]
                failed_responses = [r for r in responses if isinstance(r, Exception)]
                
                if successful_responses:
                    response_times = [r["response_time"] for r in successful_responses]
                    
                    results[f"concurrent_{concurrency}"] = {
                        "total_requests": concurrency,
                        "successful_requests": len(successful_responses),
                        "failed_requests": len(failed_responses),
                        "total_time": total_time,
                        "avg_response_time": statistics.mean(response_times),
                        "median_response_time": statistics.median(response_times),
                        "min_response_time": min(response_times),
                        "max_response_time": max(response_times),
                        "p95_response_time": self._percentile(response_times, 95),
                        "p99_response_time": self._percentile(response_times, 99),
                        "throughput": len(successful_responses) / total_time,
                        "success_rate": len(successful_responses) / concurrency * 100
                    }
                    
                    print(f"     æˆåŠŸ: {len(successful_responses)}/{concurrency}")
                    print(f"     å¹³å‡å“åº”æ—¶é—´: {results[f'concurrent_{concurrency}']['avg_response_time']:.2f}s")
                    print(f"     ååé‡: {results[f'concurrent_{concurrency}']['throughput']:.2f} req/s")
                else:
                    print(f"     âŒ æ‰€æœ‰è¯·æ±‚å¤±è´¥")
        
        return results
    
    async def _single_analyze_request(self, session: aiohttp.ClientSession, request_id: int) -> Dict[str, Any]:
        """å•ä¸ªåˆ†æè¯·æ±‚"""
        try:
            start_time = time.time()
            async with session.post(f"{self.base_url}/analyze", json={
                "content": f"æµ‹è¯•åˆ†æè¯·æ±‚ #{request_id}: è´Ÿè´£äººä»å¼ ä¸‰æ”¹ä¸ºæå››",
                "analysis_type": "risk_assessment",
                "context": {"request_id": request_id}
            }) as response:
                result = await response.json()
                response_time = time.time() - start_time
                
                return {
                    "request_id": request_id,
                    "success": response.status == 200,
                    "response_time": response_time,
                    "status_code": response.status,
                    "result": result
                }
        except Exception as e:
            return {
                "request_id": request_id,
                "success": False,
                "response_time": time.time() - start_time,
                "error": str(e)
            }
    
    def _percentile(self, data: List[float], percentile: float) -> float:
        """è®¡ç®—ç™¾åˆ†ä½æ•°"""
        sorted_data = sorted(data)
        index = int(len(sorted_data) * percentile / 100)
        return sorted_data[min(index, len(sorted_data) - 1)]
    
    async def benchmark_sustained_load(self, duration_minutes: int = 5, requests_per_second: int = 2) -> Dict[str, Any]:
        """æŒç»­è´Ÿè½½æµ‹è¯•"""
        print(f"â±ï¸ è¿è¡ŒæŒç»­è´Ÿè½½æµ‹è¯• ({duration_minutes}åˆ†é’Ÿ, {requests_per_second} req/s)...")
        
        duration_seconds = duration_minutes * 60
        interval = 1.0 / requests_per_second
        
        results = {
            "duration_seconds": duration_seconds,
            "target_rps": requests_per_second,
            "requests": [],
            "system_metrics": []
        }
        
        # å¯åŠ¨ç³»ç»Ÿç›‘æ§
        system_monitor = SystemMonitor()
        monitor_thread = threading.Thread(target=system_monitor.start_monitoring, args=(duration_seconds,))
        monitor_thread.start()
        
        async with aiohttp.ClientSession() as session:
            start_time = time.time()
            request_id = 0
            
            while time.time() - start_time < duration_seconds:
                # å‘é€è¯·æ±‚
                task_start = time.time()
                result = await self._single_analyze_request(session, request_id)
                results["requests"].append({
                    "timestamp": time.time(),
                    "request_id": request_id,
                    "success": result["success"],
                    "response_time": result["response_time"]
                })
                
                request_id += 1
                
                # æ§åˆ¶è¯·æ±‚é¢‘ç‡
                elapsed = time.time() - task_start
                if elapsed < interval:
                    await asyncio.sleep(interval - elapsed)
        
        # ç­‰å¾…ç³»ç»Ÿç›‘æ§ç»“æŸ
        monitor_thread.join()
        results["system_metrics"] = system_monitor.get_metrics()
        
        # åˆ†æç»“æœ
        successful_requests = [r for r in results["requests"] if r["success"]]
        failed_requests = [r for r in results["requests"] if not r["success"]]
        
        if successful_requests:
            response_times = [r["response_time"] for r in successful_requests]
            
            results["summary"] = {
                "total_requests": len(results["requests"]),
                "successful_requests": len(successful_requests),
                "failed_requests": len(failed_requests),
                "success_rate": len(successful_requests) / len(results["requests"]) * 100,
                "actual_rps": len(results["requests"]) / duration_seconds,
                "avg_response_time": statistics.mean(response_times),
                "max_response_time": max(response_times),
                "min_response_time": min(response_times),
                "p95_response_time": self._percentile(response_times, 95)
            }
            
            print(f"   æ€»è¯·æ±‚æ•°: {results['summary']['total_requests']}")
            print(f"   æˆåŠŸç‡: {results['summary']['success_rate']:.1f}%")
            print(f"   å®é™…RPS: {results['summary']['actual_rps']:.2f}")
            print(f"   å¹³å‡å“åº”æ—¶é—´: {results['summary']['avg_response_time']:.2f}s")
        
        return results
    
    async def benchmark_memory_usage(self, request_count: int = 100) -> Dict[str, Any]:
        """å†…å­˜ä½¿ç”¨åŸºå‡†æµ‹è¯•"""
        print(f"ğŸ’¾ è¿è¡Œå†…å­˜ä½¿ç”¨æµ‹è¯• ({request_count}ä¸ªè¯·æ±‚)...")
        
        # è·å–åˆå§‹å†…å­˜ä½¿ç”¨
        process = psutil.Process()
        initial_memory = process.memory_info().rss / 1024 / 1024  # MB
        
        memory_snapshots = [initial_memory]
        
        async with aiohttp.ClientSession() as session:
            for i in range(request_count):
                await self._single_analyze_request(session, i)
                
                # æ¯10ä¸ªè¯·æ±‚è®°å½•ä¸€æ¬¡å†…å­˜ä½¿ç”¨
                if i % 10 == 0:
                    current_memory = process.memory_info().rss / 1024 / 1024
                    memory_snapshots.append(current_memory)
        
        final_memory = process.memory_info().rss / 1024 / 1024
        
        results = {
            "initial_memory_mb": initial_memory,
            "final_memory_mb": final_memory,
            "memory_increase_mb": final_memory - initial_memory,
            "max_memory_mb": max(memory_snapshots),
            "avg_memory_mb": statistics.mean(memory_snapshots),
            "memory_snapshots": memory_snapshots
        }
        
        print(f"   åˆå§‹å†…å­˜: {initial_memory:.2f} MB")
        print(f"   æœ€ç»ˆå†…å­˜: {final_memory:.2f} MB")
        print(f"   å†…å­˜å¢é•¿: {results['memory_increase_mb']:.2f} MB")
        print(f"   å³°å€¼å†…å­˜: {results['max_memory_mb']:.2f} MB")
        
        return results
    
    async def run_full_benchmark(self) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´çš„æ€§èƒ½åŸºå‡†æµ‹è¯•"""
        print("ğŸš€ å¼€å§‹è¿è¡ŒClaudeå°è£…æœåŠ¡å®Œæ•´æ€§èƒ½åŸºå‡†æµ‹è¯•")
        print("=" * 70)
        
        benchmark_results = {
            "timestamp": datetime.now().isoformat(),
            "test_suite": "Claude Mini Wrapper Performance Benchmark",
            "version": "1.0.0"
        }
        
        # å“åº”æ—¶é—´æµ‹è¯•
        benchmark_results["response_time"] = await self.benchmark_response_time()
        
        # æŒç»­è´Ÿè½½æµ‹è¯•
        benchmark_results["sustained_load"] = await self.benchmark_sustained_load(duration_minutes=1, requests_per_second=2)
        
        # å†…å­˜ä½¿ç”¨æµ‹è¯•
        benchmark_results["memory_usage"] = await self.benchmark_memory_usage(request_count=20)
        
        # ç”Ÿæˆç»¼åˆè¯„åˆ†
        benchmark_results["performance_score"] = self._calculate_performance_score(benchmark_results)
        
        print("\n" + "=" * 70)
        print("ğŸ“Š æ€§èƒ½åŸºå‡†æµ‹è¯•å®Œæˆ!")
        print(f"   ç»¼åˆæ€§èƒ½è¯„åˆ†: {benchmark_results['performance_score']['overall_score']}/100")
        print(f"   å“åº”æ—¶é—´è¯„åˆ†: {benchmark_results['performance_score']['response_time_score']}/100")
        print(f"   ç¨³å®šæ€§è¯„åˆ†: {benchmark_results['performance_score']['stability_score']}/100")
        print(f"   èµ„æºä½¿ç”¨è¯„åˆ†: {benchmark_results['performance_score']['resource_score']}/100")
        
        return benchmark_results
    
    def _calculate_performance_score(self, results: Dict[str, Any]) -> Dict[str, int]:
        """è®¡ç®—æ€§èƒ½è¯„åˆ†"""
        scores = {}
        
        # å“åº”æ—¶é—´è¯„åˆ† (è¶Šä½è¶Šå¥½)
        if "concurrent_10" in results["response_time"]:
            avg_response = results["response_time"]["concurrent_10"]["avg_response_time"]
            if avg_response < 1.0:
                scores["response_time_score"] = 100
            elif avg_response < 2.0:
                scores["response_time_score"] = 80
            elif avg_response < 3.0:
                scores["response_time_score"] = 60
            else:
                scores["response_time_score"] = 40
        else:
            scores["response_time_score"] = 0
        
        # ç¨³å®šæ€§è¯„åˆ† (æˆåŠŸç‡)
        if "summary" in results["sustained_load"]:
            success_rate = results["sustained_load"]["summary"]["success_rate"]
            if success_rate >= 99:
                scores["stability_score"] = 100
            elif success_rate >= 95:
                scores["stability_score"] = 80
            elif success_rate >= 90:
                scores["stability_score"] = 60
            else:
                scores["stability_score"] = 40
        else:
            scores["stability_score"] = 0
        
        # èµ„æºä½¿ç”¨è¯„åˆ† (å†…å­˜å¢é•¿è¶Šå°‘è¶Šå¥½)
        memory_increase = results["memory_usage"]["memory_increase_mb"]
        if memory_increase < 10:
            scores["resource_score"] = 100
        elif memory_increase < 50:
            scores["resource_score"] = 80
        elif memory_increase < 100:
            scores["resource_score"] = 60
        else:
            scores["resource_score"] = 40
        
        # ç»¼åˆè¯„åˆ†
        scores["overall_score"] = int(
            (scores["response_time_score"] + scores["stability_score"] + scores["resource_score"]) / 3
        )
        
        return scores

class SystemMonitor:
    """ç³»ç»Ÿç›‘æ§å™¨"""
    
    def __init__(self):
        self.metrics = []
        self.monitoring = False
    
    def start_monitoring(self, duration_seconds: int):
        """å¼€å§‹ç³»ç»Ÿç›‘æ§"""
        self.monitoring = True
        start_time = time.time()
        
        while self.monitoring and (time.time() - start_time) < duration_seconds:
            self.metrics.append({
                "timestamp": time.time(),
                "cpu_percent": psutil.cpu_percent(),
                "memory_percent": psutil.virtual_memory().percent,
                "memory_used_mb": psutil.virtual_memory().used / 1024 / 1024
            })
            time.sleep(1)
    
    def get_metrics(self) -> List[Dict[str, Any]]:
        """è·å–ç›‘æ§æŒ‡æ ‡"""
        self.monitoring = False
        return self.metrics

async def main():
    """ä¸»å‡½æ•°"""
    benchmark = PerformanceBenchmark()
    results = await benchmark.run_full_benchmark()
    
    # ä¿å­˜åŸºå‡†æµ‹è¯•ç»“æœ
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"benchmark_results_{timestamp}.json"
    
    with open(filename, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2, default=str)
    
    print(f"ğŸ“„ åŸºå‡†æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {filename}")

if __name__ == "__main__":
    asyncio.run(main())