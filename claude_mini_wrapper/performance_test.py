#!/usr/bin/env python3
"""
Claudeå°è£…ç¨‹åºè½»é‡åŒ–æ€§èƒ½æµ‹è¯•
ä¼˜åŒ–ç‰ˆæœ¬ï¼šå¿«é€ŸéªŒè¯æ ¸å¿ƒåŠŸèƒ½å’Œæ€§èƒ½æŒ‡æ ‡
"""

import asyncio
import aiohttp
import time
import json
import statistics
from datetime import datetime

class LightweightPerformanceTest:
    """è½»é‡çº§æ€§èƒ½æµ‹è¯•"""
    
    def __init__(self, base_url: str = "http://localhost:8081"):
        self.base_url = base_url
        self.results = {}
        
    async def test_basic_functionality(self):
        """åŸºç¡€åŠŸèƒ½æµ‹è¯•"""
        print("ğŸ§ª åŸºç¡€åŠŸèƒ½æµ‹è¯•...")
        
        async with aiohttp.ClientSession() as session:
            # 1. å¥åº·æ£€æŸ¥
            start_time = time.time()
            async with session.get(f"{self.base_url}/health") as resp:
                health_data = await resp.json()
                health_time = time.time() - start_time
            
            # 2. å•æ¬¡åˆ†ææµ‹è¯•
            start_time = time.time()
            async with session.post(
                f"{self.base_url}/analyze",
                json={
                    "content": "é¡¹ç›®è´Ÿè´£äººä»ç‹äº”æ”¹ä¸ºèµµå…­",
                    "analysis_type": "risk_assessment"
                }
            ) as resp:
                analysis_data = await resp.json()
                analysis_time = time.time() - start_time
        
        return {
            "health_check": {
                "status": health_data.get("status"),
                "response_time": health_time,
                "success_rate": health_data.get("api_stats", {}).get("success_rate", 0),
                "models_available": len(health_data.get("models_available", []))
            },
            "single_analysis": {
                "success": "result" in analysis_data,
                "response_time": analysis_time,
                "result_quality": len(analysis_data.get("result", "")),
                "confidence": analysis_data.get("confidence", 0),
                "risk_level": analysis_data.get("risk_level")
            }
        }
    
    async def test_concurrent_requests(self, concurrent_count: int = 3):
        """å¹¶å‘è¯·æ±‚æµ‹è¯•"""
        print(f"âš¡ {concurrent_count}å¹¶å‘è¯·æ±‚æµ‹è¯•...")
        
        test_cases = [
            {"content": "å…·ä½“è®¡åˆ’å†…å®¹ä»Aé¡¹ç›®ä¿®æ”¹ä¸ºBé¡¹ç›®", "analysis_type": "risk_assessment"},
            {"content": "ååŠ©äººä»æå››æ”¹ä¸ºç‹äº”", "analysis_type": "risk_assessment"},
            {"content": "é‚“æ€»æŒ‡å¯¼ç™»è®°å¢åŠ æ–°çš„å¤‡æ³¨ä¿¡æ¯", "analysis_type": "risk_assessment"}
        ]
        
        async def single_request(session, case):
            start_time = time.time()
            try:
                async with session.post(
                    f"{self.base_url}/analyze",
                    json=case
                ) as resp:
                    result = await resp.json()
                    response_time = time.time() - start_time
                    return {
                        "success": True,
                        "response_time": response_time,
                        "confidence": result.get("confidence", 0),
                        "risk_level": result.get("risk_level")
                    }
            except Exception as e:
                return {
                    "success": False,
                    "response_time": time.time() - start_time,
                    "error": str(e)
                }
        
        async with aiohttp.ClientSession() as session:
            start_time = time.time()
            tasks = [single_request(session, case) for case in test_cases[:concurrent_count]]
            results = await asyncio.gather(*tasks)
            total_time = time.time() - start_time
        
        successful_requests = [r for r in results if r["success"]]
        response_times = [r["response_time"] for r in successful_requests]
        
        return {
            "concurrent_level": concurrent_count,
            "total_time": total_time,
            "successful_requests": len(successful_requests),
            "failed_requests": len(results) - len(successful_requests),
            "success_rate": len(successful_requests) / len(results) if results else 0,
            "avg_response_time": statistics.mean(response_times) if response_times else 0,
            "max_response_time": max(response_times) if response_times else 0,
            "min_response_time": min(response_times) if response_times else 0
        }
    
    async def test_memory_and_stability(self):
        """å†…å­˜å’Œç¨³å®šæ€§æµ‹è¯•"""
        print("ğŸ’¾ å†…å­˜å’Œç¨³å®šæ€§æµ‹è¯•...")
        
        # è¿ç»­5ä¸ªè¯·æ±‚æµ‹è¯•ç¨³å®šæ€§
        test_case = {
            "content": "ç›‘ç£äººèŒä½å˜æ›´éœ€è¦é‡æ–°åˆ†é…è´£ä»»",
            "analysis_type": "risk_assessment"
        }
        
        results = []
        async with aiohttp.ClientSession() as session:
            for i in range(5):
                start_time = time.time()
                try:
                    async with session.post(
                        f"{self.base_url}/analyze",
                        json=test_case
                    ) as resp:
                        result = await resp.json()
                        response_time = time.time() - start_time
                        results.append({
                            "success": True,
                            "response_time": response_time,
                            "result_length": len(result.get("result", "")),
                            "confidence": result.get("confidence", 0)
                        })
                except Exception as e:
                    results.append({
                        "success": False,
                        "error": str(e),
                        "response_time": time.time() - start_time
                    })
                
                # é—´éš”1ç§’
                await asyncio.sleep(1)
        
        successful_results = [r for r in results if r["success"]]
        response_times = [r["response_time"] for r in successful_results]
        
        return {
            "total_requests": len(results),
            "successful_requests": len(successful_results),
            "stability_rate": len(successful_results) / len(results),
            "avg_response_time": statistics.mean(response_times) if response_times else 0,
            "response_time_std": statistics.stdev(response_times) if len(response_times) > 1 else 0,
            "avg_result_quality": statistics.mean([r["result_length"] for r in successful_results]) if successful_results else 0
        }
    
    def generate_performance_summary(self, results: dict) -> dict:
        """ç”Ÿæˆæ€§èƒ½æ€»ç»“"""
        
        basic = results.get("basic_functionality", {})
        concurrent = results.get("concurrent_requests", {})
        stability = results.get("memory_stability", {})
        
        # è®¡ç®—ç»¼åˆè¯„åˆ†
        health_score = 100 if basic.get("health_check", {}).get("status") == "healthy" else 0
        
        response_time_score = max(0, 100 - (basic.get("single_analysis", {}).get("response_time", 0) - 5) * 10)
        response_time_score = min(100, response_time_score)
        
        concurrent_score = concurrent.get("success_rate", 0) * 100
        
        stability_score = stability.get("stability_rate", 0) * 100
        
        overall_score = (health_score + response_time_score + concurrent_score + stability_score) / 4
        
        return {
            "overall_performance_score": round(overall_score, 1),
            "health_status": "excellent" if health_score >= 90 else "good" if health_score >= 70 else "poor",
            "response_performance": "excellent" if response_time_score >= 90 else "good" if response_time_score >= 70 else "slow",
            "concurrent_performance": "excellent" if concurrent_score >= 90 else "good" if concurrent_score >= 70 else "poor",
            "stability": "excellent" if stability_score >= 90 else "good" if stability_score >= 70 else "unstable",
            "key_metrics": {
                "avg_response_time": basic.get("single_analysis", {}).get("response_time", 0),
                "concurrent_success_rate": concurrent.get("success_rate", 0),
                "stability_rate": stability.get("stability_rate", 0),
                "api_success_rate": basic.get("health_check", {}).get("success_rate", 0)
            },
            "recommendations": self._generate_recommendations(results)
        }
    
    def _generate_recommendations(self, results: dict) -> list:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        recommendations = []
        
        basic = results.get("basic_functionality", {})
        concurrent = results.get("concurrent_requests", {})
        stability = results.get("memory_stability", {})
        
        # å“åº”æ—¶é—´å»ºè®®
        avg_time = basic.get("single_analysis", {}).get("response_time", 0)
        if avg_time > 15:
            recommendations.append("å“åº”æ—¶é—´è¾ƒé•¿ï¼Œå»ºè®®ä¼˜åŒ–Claude APIè°ƒç”¨æˆ–å¢åŠ ç¼“å­˜")
        elif avg_time > 10:
            recommendations.append("å“åº”æ—¶é—´é€‚ä¸­ï¼Œå¯è€ƒè™‘åœ¨é«˜å¹¶å‘åœºæ™¯ä¸‹å¢åŠ ä¼˜åŒ–")
        
        # å¹¶å‘æ€§èƒ½å»ºè®®
        concurrent_rate = concurrent.get("success_rate", 0)
        if concurrent_rate < 0.8:
            recommendations.append("å¹¶å‘å¤„ç†æˆåŠŸç‡è¾ƒä½ï¼Œå»ºè®®æ£€æŸ¥è¿æ¥æ± é…ç½®å’Œé™æµè®¾ç½®")
        
        # ç¨³å®šæ€§å»ºè®®
        stability_rate = stability.get("stability_rate", 0)
        if stability_rate < 0.9:
            recommendations.append("ç³»ç»Ÿç¨³å®šæ€§æœ‰å¾…æå‡ï¼Œå»ºè®®åŠ å¼ºé”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶")
        
        # é€šç”¨å»ºè®®
        if not recommendations:
            recommendations.append("ç³»ç»Ÿè¿è¡Œè‰¯å¥½ï¼Œå¯ä»¥æŠ•å…¥ç”Ÿäº§ç¯å¢ƒä½¿ç”¨")
        
        return recommendations

async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ å¯åŠ¨Claudeå°è£…ç¨‹åºè½»é‡åŒ–æ€§èƒ½æµ‹è¯•")
    print("=" * 60)
    
    tester = LightweightPerformanceTest()
    all_results = {}
    
    try:
        # åŸºç¡€åŠŸèƒ½æµ‹è¯•
        all_results["basic_functionality"] = await tester.test_basic_functionality()
        print("âœ… åŸºç¡€åŠŸèƒ½æµ‹è¯•å®Œæˆ")
        
        # å¹¶å‘è¯·æ±‚æµ‹è¯•
        all_results["concurrent_requests"] = await tester.test_concurrent_requests(3)
        print("âœ… å¹¶å‘è¯·æ±‚æµ‹è¯•å®Œæˆ")
        
        # ç¨³å®šæ€§æµ‹è¯•
        all_results["memory_stability"] = await tester.test_memory_and_stability()
        print("âœ… ç¨³å®šæ€§æµ‹è¯•å®Œæˆ")
        
        # ç”Ÿæˆæ€§èƒ½æ€»ç»“
        summary = tester.generate_performance_summary(all_results)
        all_results["performance_summary"] = summary
        
        # è¾“å‡ºè¯¦ç»†ç»“æœ
        print("\n" + "=" * 60)
        print("ğŸ“Š æ€§èƒ½æµ‹è¯•ç»“æœè¯¦æƒ…")
        print("=" * 60)
        
        # åŸºç¡€åŠŸèƒ½
        basic = all_results["basic_functionality"]
        print(f"ğŸ¥ å¥åº·æ£€æŸ¥: {basic['health_check']['status']}")
        print(f"â±ï¸  å¥åº·æ£€æŸ¥å“åº”æ—¶é—´: {basic['health_check']['response_time']:.2f}ç§’")
        print(f"ğŸ“ˆ APIå†å²æˆåŠŸç‡: {basic['health_check']['success_rate']:.1%}")
        print(f"ğŸ¤– å¯ç”¨æ¨¡å‹æ•°é‡: {basic['health_check']['models_available']}")
        
        print(f"\nğŸ” å•æ¬¡åˆ†ææµ‹è¯•:")
        print(f"âœ… åˆ†ææˆåŠŸ: {basic['single_analysis']['success']}")
        print(f"â±ï¸  å“åº”æ—¶é—´: {basic['single_analysis']['response_time']:.2f}ç§’")
        print(f"ğŸ¯ ç½®ä¿¡åº¦: {basic['single_analysis']['confidence']:.2f}")
        print(f"âš ï¸  é£é™©ç­‰çº§: {basic['single_analysis']['risk_level']}")
        
        # å¹¶å‘æµ‹è¯•
        concurrent = all_results["concurrent_requests"]
        print(f"\nâš¡ å¹¶å‘æµ‹è¯• (3ä¸ªè¯·æ±‚):")
        print(f"âœ… æˆåŠŸè¯·æ±‚: {concurrent['successful_requests']}/{concurrent['concurrent_level']}")
        print(f"ğŸ“Š æˆåŠŸç‡: {concurrent['success_rate']:.1%}")
        print(f"â±ï¸  å¹³å‡å“åº”æ—¶é—´: {concurrent['avg_response_time']:.2f}ç§’")
        print(f"â° æœ€å¤§å“åº”æ—¶é—´: {concurrent['max_response_time']:.2f}ç§’")
        
        # ç¨³å®šæ€§æµ‹è¯•
        stability = all_results["memory_stability"]
        print(f"\nğŸ’¾ ç¨³å®šæ€§æµ‹è¯• (5ä¸ªè¿ç»­è¯·æ±‚):")
        print(f"âœ… æˆåŠŸç‡: {stability['stability_rate']:.1%}")
        print(f"â±ï¸  å¹³å‡å“åº”æ—¶é—´: {stability['avg_response_time']:.2f}ç§’")
        print(f"ğŸ“ å“åº”æ—¶é—´æ ‡å‡†å·®: {stability['response_time_std']:.2f}")
        
        # æ€§èƒ½æ€»ç»“
        print(f"\n" + "=" * 60)
        print("ğŸ¯ æ€§èƒ½æ€»ç»“")
        print("=" * 60)
        print(f"ğŸ† ç»¼åˆæ€§èƒ½è¯„åˆ†: {summary['overall_performance_score']}/100")
        print(f"ğŸ¥ å¥åº·çŠ¶æ€: {summary['health_status']}")
        print(f"âš¡ å“åº”æ€§èƒ½: {summary['response_performance']}")
        print(f"ğŸ”„ å¹¶å‘æ€§èƒ½: {summary['concurrent_performance']}")
        print(f"ğŸ’ª ç³»ç»Ÿç¨³å®šæ€§: {summary['stability']}")
        
        print(f"\nğŸ“‹ ä¼˜åŒ–å»ºè®®:")
        for i, rec in enumerate(summary['recommendations'], 1):
            print(f"{i}. {rec}")
        
        # ä¿å­˜ç»“æœ
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        result_file = f"performance_test_result_{timestamp}.json"
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(all_results, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {result_file}")
        print("\nğŸ‰ è½»é‡åŒ–æ€§èƒ½æµ‹è¯•å®Œæˆ!")
        
        return all_results
        
    except Exception as e:
        print(f"\nâŒ æ€§èƒ½æµ‹è¯•å¤±è´¥: {e}")
        return {"error": str(e)}

if __name__ == "__main__":
    asyncio.run(main())