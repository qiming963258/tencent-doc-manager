#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
CSVåˆ—åæ™ºèƒ½æ ‡å‡†åŒ–å¤„ç†å™¨
æå–æœ‰ä¿®æ”¹çš„åˆ—ï¼Œæ·»åŠ åºå·ï¼Œè°ƒç”¨AIæ ‡å‡†åŒ–ï¼Œå›å†™ç»“æœ
"""

import json
import csv
import os
import asyncio
import logging
from typing import Dict, List, Tuple, Any, Optional
from datetime import datetime
import string
from pathlib import Path

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# å¯¼å…¥DeepSeekå®¢æˆ·ç«¯
from deepseek_client import DeepSeekClient

class ColumnStandardizationProcessor:
    """åˆ—åæ ‡å‡†åŒ–å¤„ç†å™¨"""
    
    def __init__(self, api_key: str):
        """åˆå§‹åŒ–å¤„ç†å™¨"""
        self.api_key = api_key
        self.deepseek_client = DeepSeekClient(api_key)
        
        # 19ä¸ªæ ‡å‡†åˆ—å
        self.standard_columns = [
            "åºå·", "é¡¹ç›®ç±»å‹", "æ¥æº", "ä»»åŠ¡å‘èµ·æ—¶é—´", "ç›®æ ‡å¯¹é½",
            "å…³é”®KRå¯¹é½", "å…·ä½“è®¡åˆ’å†…å®¹", "é‚“æ€»æŒ‡å¯¼ç™»è®°", "è´Ÿè´£äºº",
            "ååŠ©äºº", "ç›‘ç£äºº", "é‡è¦ç¨‹åº¦", "é¢„è®¡å®Œæˆæ—¶é—´", "å®Œæˆè¿›åº¦",
            "å½¢æˆè®¡åˆ’æ¸…å•", "å¤ç›˜æ—¶é—´", "å¯¹ä¸Šæ±‡æŠ¥", "åº”ç”¨æƒ…å†µ", "è¿›åº¦åˆ†ææ€»ç»“"
        ]
        
        logger.info("åˆ—åæ ‡å‡†åŒ–å¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def generate_column_labels(self, count: int) -> List[str]:
        """
        ç”Ÿæˆè‹±æ–‡å­—æ¯åºå·æ ‡ç­¾
        1-26: A-Z
        27-52: AA-AZ
        ä»¥æ­¤ç±»æ¨
        """
        labels = []
        chars = string.ascii_uppercase
        
        for i in range(count):
            if i < 26:
                labels.append(chars[i])
            else:
                # å¤„ç†è¶…è¿‡26åˆ—çš„æƒ…å†µ
                first = (i - 26) // 26
                second = (i - 26) % 26
                if first < 26:
                    labels.append(chars[first] + chars[second])
                else:
                    # å¤„ç†è¶…è¿‡52åˆ—çš„æç«¯æƒ…å†µ
                    labels.append(f"COL{i+1}")
        
        return labels
    
    def extract_modified_columns(self, comparison_result: Dict) -> Tuple[List[str], Dict[str, List[Any]]]:
        """
        ä»CSVå¯¹æ¯”ç»“æœä¸­æå–æœ‰ä¿®æ”¹çš„åˆ—
        
        Args:
            comparison_result: CSVå¯¹æ¯”ç»“æœå­—å…¸
            
        Returns:
            (å»é‡çš„åˆ—ååˆ—è¡¨, åˆ—åå¯¹åº”çš„æ‰€æœ‰ä¿®æ”¹æ•°æ®)
        """
        modified_columns = set()
        column_data = {}
        
        # åˆ†æå·®å¼‚æ•°æ®
        if 'differences' in comparison_result:
            for diff in comparison_result['differences']:
                for column, values in diff.items():
                    if column == 'row_number':
                        continue
                        
                    # æ£€æŸ¥æ˜¯å¦æœ‰å®é™…ä¿®æ”¹
                    if isinstance(values, dict):
                        baseline = values.get('baseline_value', '')
                        target = values.get('target_value', '')
                        changed = values.get('changed', False)
                        
                        # åªæ”¶é›†æœ‰å˜åŒ–çš„åˆ—
                        if changed and baseline != target:
                            modified_columns.add(column)
                            if column not in column_data:
                                column_data[column] = []
                            column_data[column].append(values)
        
        # å»é‡å¹¶æ’åº
        unique_columns = sorted(list(modified_columns))
        
        logger.info(f"æå–åˆ° {len(unique_columns)} ä¸ªæœ‰ä¿®æ”¹çš„åˆ—")
        return unique_columns, column_data
    
    def build_numbered_prompt(self, columns_with_labels: Dict[str, str]) -> str:
        """
        æ„å»ºå¸¦åºå·çš„AIæç¤ºè¯
        
        Args:
            columns_with_labels: {åºå·: åˆ—å} çš„å­—å…¸
        """
        prompt = f"""ä½ æ˜¯ä¸“ä¸šçš„CSVåˆ—åæ ‡å‡†åŒ–ä¸“å®¶ã€‚è¯·å°†å¸¦åºå·çš„åˆ—åæ˜ å°„åˆ°19ä¸ªæ ‡å‡†åˆ—åã€‚

## ğŸ¯ æ ¸å¿ƒä»»åŠ¡
åˆ†æå¸¦è‹±æ–‡åºå·çš„åˆ—åï¼Œå°†å…¶æ˜ å°„åˆ°æ ‡å‡†åˆ—åã€‚ä¿æŒåºå·ä¸å˜ï¼Œåªæ”¹å˜åˆ—åéƒ¨åˆ†ã€‚

## ğŸ“‹ 19ä¸ªæ ‡å‡†åˆ—å
{json.dumps(self.standard_columns, ensure_ascii=False, indent=2)}

## ğŸ“ éœ€è¦æ ‡å‡†åŒ–çš„åˆ—åï¼ˆå…±{len(columns_with_labels)}ä¸ªï¼‰
"""
        for label, column in columns_with_labels.items():
            prompt += f"{label}: {column}\n"
        
        prompt += """
## ğŸ”„ å¤„ç†è§„åˆ™
1. ä¿æŒè‹±æ–‡åºå·ä¸å˜ï¼ˆAã€Bã€Cç­‰ï¼‰
2. å°†æ¯ä¸ªåˆ—åæ˜ å°„åˆ°æœ€åŒ¹é…çš„æ ‡å‡†åˆ—å
3. å¦‚æœåˆ—æ•°è¶…è¿‡19ä¸ªï¼Œé€‰æ‹©æœ€é‡è¦çš„19ä¸ªï¼Œå…¶ä½™æ ‡è®°ä¸ºä¸¢å¼ƒ
4. å¦‚æœæŸäº›æ ‡å‡†åˆ—æ²¡æœ‰å¯¹åº”ï¼Œæ ‡è®°ä¸ºç¼ºå¤±

## ğŸ“Š è¾“å‡ºæ ¼å¼ï¼ˆä¸¥æ ¼JSONï¼‰
{
    "success": true,
    "numbered_mapping": {
        "A": "æ˜ å°„åçš„æ ‡å‡†åˆ—å",
        "B": "æ˜ å°„åçš„æ ‡å‡†åˆ—å",
        // ... æ‰€æœ‰åºå·çš„æ˜ å°„
    },
    "discarded_labels": ["X", "Y"],  // è¢«ä¸¢å¼ƒçš„åºå·åˆ—è¡¨
    "missing_standard_columns": ["æ ‡å‡†åˆ—1", "æ ‡å‡†åˆ—2"],  // ç¼ºå¤±çš„æ ‡å‡†åˆ—
    "statistics": {
        "total_input": æ•°å­—,
        "mapped_count": æ•°å­—,
        "discarded_count": æ•°å­—,
        "missing_standard_count": æ•°å­—
    }
}

è¯·ç«‹å³åˆ†æå¹¶è¿”å›æ ‡å‡†åŒ–æ˜ å°„ç»“æœã€‚"""
        
        return prompt
    
    async def standardize_columns(self, columns: List[str]) -> Dict[str, Any]:
        """
        è°ƒç”¨AIè¿›è¡Œåˆ—åæ ‡å‡†åŒ–
        
        Args:
            columns: éœ€è¦æ ‡å‡†åŒ–çš„åˆ—ååˆ—è¡¨
            
        Returns:
            æ ‡å‡†åŒ–ç»“æœå­—å…¸
        """
        # ç”Ÿæˆåºå·æ ‡ç­¾
        labels = self.generate_column_labels(len(columns))
        columns_with_labels = {labels[i]: columns[i] for i in range(len(columns))}
        
        logger.info(f"ç”Ÿæˆåºå·æ˜ å°„: {columns_with_labels}")
        
        # æ„å»ºæç¤ºè¯
        prompt = self.build_numbered_prompt(columns_with_labels)
        
        # è°ƒç”¨AI
        messages = [
            {
                "role": "system",
                "content": "ä½ æ˜¯ä¸“ä¸šçš„æ•°æ®åˆ†æä¸“å®¶ï¼Œç²¾é€šCSVåˆ—åæ ‡å‡†åŒ–ã€‚æ€»æ˜¯è¿”å›æœ‰æ•ˆçš„JSONæ ¼å¼ã€‚"
            },
            {
                "role": "user",
                "content": prompt
            }
        ]
        
        result = await self.deepseek_client.chat_completion(messages, temperature=0.1)
        
        if result["success"]:
            try:
                content = result["content"]
                # æå–JSONéƒ¨åˆ†
                json_start = content.find('{')
                json_end = content.rfind('}') + 1
                
                if json_start != -1 and json_end > 0:
                    json_str = content[json_start:json_end]
                    parsed = json.loads(json_str)
                    
                    # æ·»åŠ åŸå§‹æ˜ å°„ä¿¡æ¯
                    parsed['original_columns'] = columns_with_labels
                    return {
                        "success": True,
                        "result": parsed
                    }
            except Exception as e:
                logger.error(f"è§£æAIå“åº”å¤±è´¥: {e}")
                return {"success": False, "error": str(e)}
        
        return result
    
    def apply_standardization(self, comparison_result: Dict, standardization_result: Dict) -> Dict:
        """
        å°†æ ‡å‡†åŒ–ç»“æœåº”ç”¨åˆ°CSVå¯¹æ¯”ç»“æœ
        
        Args:
            comparison_result: åŸå§‹CSVå¯¹æ¯”ç»“æœ
            standardization_result: AIæ ‡å‡†åŒ–ç»“æœ
            
        Returns:
            æ ‡å‡†åŒ–åçš„CSVå¯¹æ¯”ç»“æœ
        """
        if not standardization_result.get("success"):
            logger.error("æ ‡å‡†åŒ–å¤±è´¥ï¼Œè¿”å›åŸå§‹ç»“æœ")
            return comparison_result
        
        mapping = standardization_result["result"].get("numbered_mapping", {})
        original_columns = standardization_result["result"].get("original_columns", {})
        discarded_labels = standardization_result["result"].get("discarded_labels", [])
        
        # åˆ›å»ºåŸå§‹åˆ—ååˆ°æ ‡å‡†åˆ—åçš„æ˜ å°„
        column_mapping = {}
        for label, standard_name in mapping.items():
            if label not in discarded_labels and label in original_columns:
                original_name = original_columns[label]
                column_mapping[original_name] = standard_name
        
        logger.info(f"åˆ—åæ˜ å°„å…³ç³»: {column_mapping}")
        
        # åˆ›å»ºæ–°çš„æ ‡å‡†åŒ–ç»“æœ
        standardized_result = {
            "metadata": {
                "original_file": comparison_result.get("metadata", {}).get("original_file", ""),
                "standardization_time": datetime.now().isoformat(),
                "column_mapping": column_mapping,
                "discarded_columns": [original_columns.get(label, label) for label in discarded_labels],
                "standard_columns_count": len(self.standard_columns)
            },
            "differences": []
        }
        
        # å¤„ç†æ¯æ¡å·®å¼‚è®°å½•
        for diff in comparison_result.get("differences", []):
            standardized_diff = {"row_number": diff.get("row_number", 0)}
            
            # å…ˆæ·»åŠ æ‰€æœ‰æ ‡å‡†åˆ—ï¼ˆåˆå§‹åŒ–ä¸ºnullï¼‰
            for std_col in self.standard_columns:
                standardized_diff[std_col] = None
            
            # å¡«å……æœ‰æ˜ å°„çš„åˆ—æ•°æ®
            for original_col, values in diff.items():
                if original_col == "row_number":
                    continue
                
                if original_col in column_mapping:
                    standard_col = column_mapping[original_col]
                    standardized_diff[standard_col] = values
                # ä¸¢å¼ƒæœªæ˜ å°„çš„åˆ—
            
            standardized_result["differences"].append(standardized_diff)
        
        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        standardized_result["statistics"] = {
            "total_differences": len(standardized_result["differences"]),
            "mapped_columns": len(column_mapping),
            "discarded_columns": len(discarded_labels),
            "standard_columns": len(self.standard_columns)
        }
        
        return standardized_result
    
    async def process_file(self, input_file: str, output_file: str = None) -> Dict:
        """
        å¤„ç†CSVå¯¹æ¯”æ–‡ä»¶
        
        Args:
            input_file: è¾“å…¥çš„CSVå¯¹æ¯”ç»“æœæ–‡ä»¶è·¯å¾„
            output_file: è¾“å‡ºçš„æ ‡å‡†åŒ–æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            å¤„ç†ç»“æœ
        """
        logger.info(f"å¼€å§‹å¤„ç†æ–‡ä»¶: {input_file}")
        
        try:
            # è¯»å–è¾“å…¥æ–‡ä»¶
            with open(input_file, 'r', encoding='utf-8') as f:
                if input_file.endswith('.json'):
                    comparison_result = json.load(f)
                else:
                    # å¤„ç†CSVæ ¼å¼
                    comparison_result = self.parse_csv_to_json(input_file)
            
            # æå–æœ‰ä¿®æ”¹çš„åˆ—
            modified_columns, column_data = self.extract_modified_columns(comparison_result)
            
            if not modified_columns:
                logger.warning("æ²¡æœ‰å‘ç°æœ‰ä¿®æ”¹çš„åˆ—")
                return {"success": False, "message": "æ²¡æœ‰å‘ç°æœ‰ä¿®æ”¹çš„åˆ—"}
            
            logger.info(f"å‘ç° {len(modified_columns)} ä¸ªæœ‰ä¿®æ”¹çš„åˆ—: {modified_columns}")
            
            # è°ƒç”¨AIæ ‡å‡†åŒ–
            standardization_result = await self.standardize_columns(modified_columns)
            
            if not standardization_result.get("success"):
                return standardization_result
            
            # åº”ç”¨æ ‡å‡†åŒ–
            standardized_result = self.apply_standardization(
                comparison_result, 
                standardization_result
            )
            
            # ä¿å­˜ç»“æœ
            if output_file is None:
                # è‡ªåŠ¨ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
                base_name = os.path.splitext(input_file)[0]
                output_file = f"{base_name}_standardized.json"
            
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(standardized_result, f, ensure_ascii=False, indent=2)
            
            logger.info(f"æ ‡å‡†åŒ–ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
            
            return {
                "success": True,
                "input_file": input_file,
                "output_file": output_file,
                "statistics": standardized_result["statistics"],
                "column_mapping": standardized_result["metadata"]["column_mapping"]
            }
            
        except Exception as e:
            logger.error(f"å¤„ç†æ–‡ä»¶å¤±è´¥: {e}")
            return {"success": False, "error": str(e)}
    
    def parse_csv_to_json(self, csv_file: str) -> Dict:
        """
        å°†CSVæ–‡ä»¶è§£æä¸ºJSONæ ¼å¼çš„å¯¹æ¯”ç»“æœ
        """
        differences = []
        
        with open(csv_file, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row_num, row in enumerate(reader, 1):
                diff = {"row_number": row_num}
                for column, value in row.items():
                    # ç®€å•è§£æï¼Œå®é™…éœ€è¦æ ¹æ®CSVæ ¼å¼è°ƒæ•´
                    diff[column] = {
                        "baseline_value": value,
                        "target_value": value,
                        "changed": False  # éœ€è¦æ ¹æ®å®é™…æƒ…å†µåˆ¤æ–­
                    }
                differences.append(diff)
        
        return {"differences": differences}
    
    def sync_process_file(self, input_file: str, output_file: str = None) -> Dict:
        """åŒæ­¥ç‰ˆæœ¬çš„æ–‡ä»¶å¤„ç†ï¼ˆä¾›éå¼‚æ­¥ç¯å¢ƒä½¿ç”¨ï¼‰"""
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            return loop.run_until_complete(self.process_file(input_file, output_file))
        finally:
            loop.close()


# å‘½ä»¤è¡Œæ¥å£
if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="CSVåˆ—åæ ‡å‡†åŒ–å¤„ç†å™¨")
    parser.add_argument("input_file", help="è¾“å…¥çš„CSVå¯¹æ¯”ç»“æœæ–‡ä»¶")
    parser.add_argument("-o", "--output", help="è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰")
    parser.add_argument("--api-key", default="sk-onzowexyblsrgltveejlnajoutrjqwbrqkwzcccskwmzonvb",
                       help="DeepSeek APIå¯†é’¥")
    
    args = parser.parse_args()
    
    # åˆ›å»ºå¤„ç†å™¨
    processor = ColumnStandardizationProcessor(args.api_key)
    
    # å¤„ç†æ–‡ä»¶
    result = processor.sync_process_file(args.input_file, args.output)
    
    # æ˜¾ç¤ºç»“æœ
    if result.get("success"):
        print("\nâœ… å¤„ç†æˆåŠŸï¼")
        print(f"è¾“å…¥æ–‡ä»¶: {result['input_file']}")
        print(f"è¾“å‡ºæ–‡ä»¶: {result['output_file']}")
        print(f"\nç»Ÿè®¡ä¿¡æ¯:")
        for key, value in result['statistics'].items():
            print(f"  {key}: {value}")
        print(f"\nåˆ—åæ˜ å°„:")
        for original, standard in result['column_mapping'].items():
            print(f"  {original} â†’ {standard}")
    else:
        print(f"\nâŒ å¤„ç†å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")